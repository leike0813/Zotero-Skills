# SPARSE DETR: EFFICIENT END-TO-END OBJECT DETECTION WITH LEARNABLE SPARSITY

Byungseok $\mathbf { R o h } ^ { 1 * \dagger }$ , JaeWoong $\mathbf { S h i n ^ { 2 * \ddagger } }$ , Wuhyun $\mathbf { S h i n ^ { 1 * } }$ , Saehoon Kim1   
1KakaoBrain   
2Lunit

{peter.roh,aiden.hsin,sam.kim}@kakaobrain.com jwoong.shin@lunit.io

# ABSTRACT

DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency on high resolution feature maps. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves $1 0 \times$ faster convergence and improved performance. Deformable DETR uses the multiscale feature to ameliorate performance, however, the number of encoder tokens increases by $2 0 \times$ compared to DETR, and the computation cost of the encoder attention remains a bottleneck. In our preliminary experiment, we observe that the detection performance hardly deteriorates even if only a part of the encoder token is updated. Inspired by this observation, we propose Sparse DETR that selectively updates only the tokens expected to be referenced by the decoder, thus help the model effectively detect objects. In addition, we show that applying an auxiliary detection loss on the selected tokens in the encoder improves the performance while minimizing computational overhead. We validate that Sparse DETR achieves better performance than Deformable DETR even with only $10 \%$ encoder tokens on the COCO dataset. Albeit only the encoder tokens are sparsified, the total computation cost decreases by $38 \%$ and the frames per second (FPS) increases by $42 \%$ compared to Deformable DETR. Code is available at https://github.com/kakaobrain/sparse-detr.

# 1 INTRODUCTION

In recent years, we have witnessed the dramatic advancement and the success of object detection in deep learning. Diverse object detection methods have been proposed, but the existing algorithms that perform positive matching with the ground truth as a heuristic way require non-maximum suppression (NMS) post-processing of near-duplicate predictions. Recently, Carion et al. (2020) has introduced a fully end-to-end detector DETR by eliminating the need for NMS post-processing through a set-based objective. The training objective is designed by employing the Hungarian algorithm that considers both classification and regression costs, and achieves highly competitive performance. However, DETR is unable to use multi-scale features such as feature pyramid networks (Lin et al., 2017), which are commonly used in object detection to improve the detection of small objects. The main reason is increased memory usage and computation by adding Transformer (Vaswani et al., 2017) architecture. As a result, its ability to detect small objects is relatively poor.

To address this problem, Zhu et al. (2021) has proposed a deformable-attention inspired by the deformable convolution (Dai et al., 2017) and reduced the quadratic complexity to linear complexity through key sparsification in the attention module. By using deformable attention, deformable DETR addresses the slow convergence and high complexity issue of DETR, which enables the encoder to use multi-scale features as an input and significantly improves performance on detecting small objects. However, using the multi-scale features as an encoder input increases the number of tokens to be processed by about 20 times. Eventually, despite efficient computation for the same token length, the overall complexity increases back again, making the model inference slower even than vanilla DETR.

In general, natural images often contain large background regions irrelevant to the objects of interest, and accordingly, in end-to-end detectors, the tokens corresponding to the background also occupy a significant portion. In addition, the importance of each regional feature is not identical, which has been proven by the two-stage detectors successfully do their job by focusing only on the foreground. It suggests that there exists considerable regional redundancy that can be reduced in the detection tasks and seeking to devise an efficient detector focusing on the salient regions is a necessary and natural direction. In our preliminary experiments, we observe the following: (a) during inference of a fully-converged Deformable DETR model on the COCO validation dataset, the encoder tokens referenced by the decoder account for only about $45 \%$ of the total, and (b) retraining a new detector while updating only the encoder tokens preferred by the decoder from another fully-trained detector, barely suffers performance loss(0.1 AP degradation). See Appendix A.9 for the details.

Inspired by this observation, we propose a learnable decoder cross-attention map predictor to sparsify encoder tokens. In the existing methods (Carion et al., 2020; Zhu et al., 2021), the encoder takes all the tokens, i.e. the backbone features combined with corresponding positional embeddings, as input without discrimination. Meanwhile, our approach distinguishes encoder tokens to be referenced later in the decoder and considers only those tokens in self-attention. Therefore, this can significantly reduce the number of encoder tokens involved in the computation and reduce the total computational cost. We further propose the encoder auxiliary loss for selected encoder tokens to improve detection performance while minimizing computational overhead. The proposed auxiliary loss not only improves performance, but also allows training a larger number of encoder layers.

Extensive experiments on the COCO 2017 benchmark (Lin et al., 2014) demonstrate that Sparse DETR effectively reduces computational cost while achieving better detection performance. Without bells and whistles, Sparse DETR using Swin-T (Liu et al., 2021) backbone achieves $4 8 . 2 \mathrm { \ A P }$ with $38 \%$ reduction of the entire computational cost compared to the 48.0 AP baseline and $4 9 . 2 \mathrm { \ A P }$ with $23 \%$ reduction. In the case of the experiment that achieves 48.2 AP using only $10 \%$ of encoder tokens, the computational cost of the transformer encoder block is reduced by approximately $82 \%$ .

We summarize our contributions as follows:

• We propose encoder token sparsification method for an efficient end-to-end object detector, by which we lighten the attention complexity in the encoder. This efficiency enables stacking more encoder layers than Deformable DETR, leading to performance improvement within the same computational budget.   
• We propose two novel sparsification criteria to sample the informative subset from the entire token set: Objectness Score $( O S )$ and Decoder cross-Attention Map (DAM). Based on the decoder cross-attention map criterion, the sparsified model preserves detection performance even when using only $10 \%$ of the whole tokens.   
• We adopt an encoder auxiliary loss only for the selected tokens. This additional loss not only stabilizes the learning process, but also greatly improves performance, with only marginally increased training time.

# 2 RELATED WORK

Efficient computation in vision transformers. It is a well-known problem that the attention computation in Transformers incurs the high time and memory complexity. The vision transformers need to digest even bigger token sets as input so that a large body of works (Parmar et al., 2018; Child et al., 2019a; Ho et al., 2019; Wang et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2021; Kitaev et al., 2020) has been proposed lightweight attention mechanisms for them. Most of those works shed light on the complexity that resides only in a single-scale attention module, which hinders direct extension to the multi-scale features generally required in object detection.

One of the promising approaches for the lighter transformer attention is input-dependent token sparsification. DynamicViT (Rao et al., 2021) and IA-RED $^ 2$ (Pan et al., 2021), similar to our work, both propose jointly-learned token selectors generating the sparsity patterns to be overlaid on the input tokens. Those approaches mainly focus on sparsifying a backbone network evaluated on the classification tasks, while our interest lies in a sparse encoder of the end-to-end object detectors.

On the other hand, there has been a line of works sharing the spirit with ours in that they aim at sparse transformers in the DETR-based framework. Deformable DETR (Zhu et al., 2021) conducts sparse attention computation by sampling only a fraction of the entire key set with learnable 2-d offsets, which enables to use multi-scale feature maps with a reasonable computational cost. It can be viewed as a key sparsification method but with dense queries, while our approach further reduces the query set pursuing even more sparsity. PnP-DETR (Wang et al., 2021) shortens the token length of the transformer encoder by introducing the Polling and Pull (PnP) module to sample the foreground tokens and condense the background tokens into a smaller set. However, their method cannot naively be integrated with Deformable DETR, since their sparsification breaks the 2d spatial structure of the token set assumed in the deformable attention. On the contrary, Sparse DETR preserves the 2d sample space of the set and can be seamlessly combined with the deformable attention, which facilitates handling the multi-scale features. Thus, our approach gets benefits from both the deformable key sampling and the proposed query sparsification. Most of all, we propose explicit objectives for the token selection network, whereas the aforementioned works have no explicit objective implying their beliefs in a good selection strategy, merely relying on the final detection objective.

Auxiliary Loss. Auxiliary loss (Lee et al., 2015; Szegedy et al., 2015) is widely adopted to deliver gradients to the early layers of deep networks. DETR variants employ auxiliary Hungarian matching objectives at the end of every decoder layer with extra FFN heads so that each decoder layer directly learns to detect the correct number of objects out of the decoder’s outputs. Unlike the decoder’s object queries whose number is relatively small(e.g. 300), the number of encoder’s tokens has much larger scales when using multi-scale features. Thus, extending the layerwise auxiliary loss to the multi-scale encoder increases the training time cost by feeding too many tokens to the attached FFN heads. In Sparse DETR, thanks to the sparsity already induced in the encoder, we can instantly economize that cost while enjoying the auxiliary gradients in a wider range of intermediate layers.

# 3 APPROACH

In this section, we present our main contributions: (a) formulating a generalized saliency-based token sparsification scheme for the encoder, (b) proposing the effective saliency criteria with which that scheme can practically work, and (c) employing the encoder auxiliary losses and the top- $k$ decoder query selection to improve the performance. Before describing the details, we revisit the key components of DETR (Carion et al., 2020) and Deformable DETR (Zhu et al., 2021).

# 3.1 PRELIMINARY

DETR. DETR takes the flattened spatial feature map ${ \bf x } _ { \mathrm { f e a t } } \in \mathbb { R } ^ { N \times D }$ from a backbone network into the transformer encoder, where $N$ denotes the number of tokens (i.e. features) and $D$ denotes token dimension. The encoder iteratively updates $\mathbf { x } _ { \mathrm { f e a t } }$ by several vanilla self-attention modules. Then, the transformer decoder takes both the refined encoder tokens (i.e. encoder output) and $M$ learnable object queries $\{ q _ { i } \} _ { i = 1 \cdots M }$ as inputs and predicts a tuple of a class score $\mathbf { c } \in [ \bar { 0 } , 1 ] ^ { C }$ and a bounding box $\mathbf { b } \in [ 0 , 1 ] ^ { 4 }$ for each object query $q _ { i }$ , denoted as $\{ \hat { \bf y } _ { i } \} = \{ ( { \bf c } _ { i } , { \bf b } _ { i } ) \}$ , where $C$ denotes the number of classes. All components including the backbone network are jointly trained by performing the bipartite matching between the ground truth $\left\{ \mathbf { y } _ { i } \right\}$ and predictions $\left\{ \hat { \mathbf { y } } _ { i } \right\}$ .

Deformable DETR. Deformable DETR replaces the vanilla dense attention, which is the main computational bottleneck in DETR, with a deformable attention module. This significantly reduces the computational cost and improves the convergence. Suppose that we have the same size of a set of queries (denoted as $\Omega _ { q } )$ ) and a set of keys (denoted as $\Omega _ { k }$ ), which means $| \Omega _ { q } | = | \Omega _ { k } | =$ $N$ . The conventional dense attention computes the attention weight $A _ { q k }$ for every pair $\{ ( q , k ) :$ $q \in \Omega _ { q } , k \in \Omega _ { k } \}$ , resulting in quadratic complexity with respect to $N$ . Deformable attention reduces this quadratic complexity into the linear one by only considering relevant keys for each query. Specifically, deformable attention computes attention weight $A _ { q k }$ for all queries and a small set of keys: $\{ ( q , \dot { k } ) : q \in \Omega _ { q } , k \in \Omega _ { q k } \}$ , where $\Omega _ { q k } \subset \Omega _ { k }$ and $\left| \Omega _ { q k } \right| = \mathbf { \bar { \Psi } } K \ll N$ .

Due to this key sparsification, Deformable DETR is able to use the multi-scale features of the backbone network, improving the detection performance of small objects significantly. Paradoxically, using the multi-scale feature increases the number of tokens in the transformer encoder by about $2 0 \times$ compared to DETR, making that the encoder becomes the computational bottleneck of deformable DETR. This motivates us to develop a sparsification method to reduce the number of tokens in the encoder aggressively, which is described in the next sections.

![](Images_J728TKKV/4030d9c2e793d71e0987b7873c52010f22e71dbd1f48a4eb48f44a85c1db8de1.jpg)  
Figure 1: Attention complexity. The circles in the square matrix represent the attention between keys and queries. The gray/white circles correspond to preserved/removed connection respectively, and darker gray on the diagonal positions means where the token attends to itself. (a) Dense attention in DETR takes quadratic complexity. (b) Deformable DETR uses key sparsification, thus takes linear complexity. (c) Sparse DETR further uses query sparsification. Attention in Sparse DETR also takes linear complexity, but is much lighter than Deformable DETR’s.

# 3.2 ENCODER TOKEN SPARSIFICATION

In this section, we introduce our token sparsification scheme that the encoder module selectively refines a small number of encoder tokens. This encoder token subset is obtained from the backbone feature map $\mathbf { X } _ { \mathrm { f e a t } }$ with a certain criterion, which is described in the subsequent section. For features that are not updated in this process, the values of $\mathbf { X } _ { \mathrm { f e a t } }$ are passed through the encoder layers without being changed.

Formally, suppose that we have a scoring network $g : \mathbb { R } ^ { d }  \mathbb { R }$ that measures saliency of each token in $\mathbf { x } _ { \mathrm { f e a t } }$ . We then define $\rho$ -salient regions $\Omega _ { s } ^ { \rho }$ as the top- $\cdot \rho \%$ tokens with the highest scores, for a given keeping ratio $\rho$ , i.e. $S = | \Omega _ { s } ^ { \rho } | = \rho \cdot | \Omega _ { q } | \ll | \Omega _ { q } | = N$ . Then, the $i$ -th encoder layer updates the features $\mathbf { x } _ { i - 1 }$ by:

$$
\mathbf { x } _ { i } ^ { j } = \left\{ \begin{array} { l l } { \mathbf { x } _ { i - 1 } ^ { j } } & { j \notin \Omega _ { s } ^ { \rho } } \\ { \operatorname { L N } ( \operatorname { F F N } ( \mathbf { z } _ { i } ^ { j } ) + \mathbf { z } _ { i } ^ { j } ) } & { j \in \Omega _ { s } ^ { \rho } , \mathrm { ~ w h e r e ~ } \mathbf { z } _ { i } ^ { j } = \operatorname { L N } ( \operatorname { D e f A t t n } ( \mathbf { x } _ { i - 1 } ^ { j } , \mathbf { x } _ { i - 1 } ) + \mathbf { x } _ { i - 1 } ^ { j } ) , } \end{array} \right.
$$

where DefAttn refers to deformable attention, LN to layer normalization (Ba et al., 2016), and FFN to a feed-forward network. Even in the case of unselected tokens, the values are still passed through the encoder layer, so they can be referenced as keys when updating the selected tokens. This means that the unselected tokens can hand over information to the selected tokens without losing the value of themselves while minimizing the computational cost. Here, we use deformable attention for refining tokens in $\Omega _ { s } ^ { \rho }$ , but the proposed encoder token sparsification is applicable regardless of which attention method the encoder uses.

Complexity of Attention Modules in Encoder. Deformable DETR reduces the attention complexity through key sparsification, and we further reduce the attention complexity through query sparsification, as shown in Fig. 1. Conventional dense attention in DETR requires quadratic complexity $O ( N ^ { 2 } )$ , where $N$ is the query length. Deformable attention requires linear complexity $O ( N K )$ , where $K \ll N$ is the number of keys for each query. Sparse attention requires only $O ( S K )$ , where $S \ll N$ is the number of salient encoder queries.

# 3.3 FINDING SALIENT ENCODER TOKENS

In this section, we introduce how to find a salient token set $\Omega _ { s } ^ { \rho }$ from a backbone feature $\mathbf { x } _ { \mathrm { f e a t } }$ . We propose a method for determining saliency using a cross attention map from the transformer decoder. Before presenting our approach, we first discuss a simple yet effective method based on the objectness scores obtained from a separate detection head. The limitation of this simple approach motivates us to develop an advanced one, which is described in the following paragraph.

![](Images_J728TKKV/876bdfec7bc4beb1a28f069055a4988bb371f9cc6d1bd38937f4c2409ac7d653.jpg)  
Figure 2: Illustration on how to learn a scoring network by predicting binarized Decoder crossAttention Map (DAM), where a dashed orange arrow means a backpropagation path. The bottom box shows the forward/backward passes in Sparse DETR, and the top boxes present how to construct DAM for learning the scoring network. See Appendix A.1 for implementation details of scoring net.

Objectness Score. Measuring objectness per each input token (i.e. feature $\mathbf { x } _ { \mathrm { f e a t . } }$ ) of encoder is very natural to determine which ones from a backbone feature should be further updated in the transformer encoder. It is widely known that feature map from a pretrained backbone network is able to find the saliency of objects, which is why the region proposal network (RPN) has been successfully adopted in many object detectors (Ren et al., 2015; Dai et al., 2016; He et al., 2017). Inspired by this observation, we introduce an additional detection head and Hungarian loss to the backbone feature map, where the structure of the newly added head is the same as the one of the final detection head in the decoder. Then, we can select the top- $\rho \%$ encoder tokens with the highest class scores as a salient token set $\Omega _ { s } ^ { \rho }$ . This approach is effective to sparsify encoder tokens, but we believe that it is sub-optimal to the transformer decoder, because the selected encoder tokens from the separate detection head are not explicitly considered for the decoder.

Decoder Cross-Attention Map. We consider another approach to select a subset of encoder tokens that are highly relevant to the decoder in a more explicit manner. We observe that the crossattention maps from the transformer decoder could be used for measuring the saliency, because the decoder gradually attends to a subset of encoder output tokens that are favorable to detect objects as training continues. Motivated by this, we introduce a scoring network that predicts a pseudo groundtruth of the saliency defined by decoder cross-attention maps, and use it to determine which encoder tokens should be further refined on the fly. Fig. 2 summarizes how to train a scoring network, and details are presented below.

To determine the saliency of each input token of encoder $\mathbf { x } _ { \mathrm { f e a t } }$ , we have to aggregate the decoder cross-attentions between all object queries and the encoder output. This procedure produces a single map of the same size as the feature map from the backbone, which is defined as Decoder crossAttention Map (DAM). In the case of the dense attention, DAM can be easily obtained by summing up attention maps from every decoder layer. In case of deformable attention, for each encoder token, the corresponding value of DAM can be obtained by accumulating the attention weights of decoder object queries whose attention offsets are directed toward the encoder output tokens. Refer to the Appendix A.2 for the details in the DAM creation.

To train the scoring network, we binarize DAM so that the top- $\cdot \rho \%$ (by attention weights) of encoder tokens is only retained. This is because our goal is to find a small subset of encoder tokens that the decoder references the most, rather than precisely predicting how much each encoder token will be referenced by the decoder. This binarized DAM implies the one-hot target that indicates whether each encoder token is included in the top- $\cdot \rho \%$ most referenced encoder tokens. Then, we consider a 4-layer scoring network $g$ to predict how likely a given encoder token is included in the top- $\rho \%$ most referenced tokens, and the network is trained by minimizing the binary cross entropy (BCE) loss between the binarized DAM and prediction:

$$
\mathcal { L } _ { d a m } = - \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \mathrm { { B C E } } ( g ( \mathbf { x } _ { \mathrm { { f e a t } } } ) _ { i } , \mathbf { D A M } _ { i } ^ { \mathrm { { b i n } } } ) ,
$$

![](Images_J728TKKV/4e6fb1688ec6f223548e793a65dabdbfa2bb9b932ca5412b9c5025e54f74d2be.jpg)  
Figure 3: Sparse DETR architecture. Sparse DETR introduces three additional components: (a) the scoring network, (b) auxiliary heads in the encoder, and (c) the auxiliary head to select the top- $k$ tokens for the decoder. Sparse DETR measures the saliency of encoder tokens by using the scoring network, and selects the top- $\cdot \rho \%$ tokens, which is referred to as (1) in the diagram. After refining only the selected tokens in the encoder blocks, the auxiliary head selects the top- $k$ tokens from the encoder output, which is served as the decoder object queries. This process is referred to as (2) in the diagram. In addition, we remark that additional auxiliary heads in each encoder block play a key role in achieving improved performance. Only sparsified encoder tokens are passed to the encoder auxiliary heads for efficiency. All auxiliary heads in the encoder and decoder are trained with a Hungarian loss as described in Deformable DETR (Zhu et al., 2021).

where $\mathrm { D A M } _ { i } ^ { \mathrm { b i n } }$ means the binarized DAM value of the ith encoder token.

One may say that since DAM in the early phase of training is not accurate, pruning out the encoder tokens based on the result in the decoder degrades the final performance or hurts the convergence. However, we empirically observe that the optimization is very stable even in the early phase of training, and achieves better performance compared to the method based on objectness score. We describe detailed comparisons in the experiments section.

# 3.4 ADDITIONAL COMPONENTS

In this section, we introduce two additional components: (a) auxiliary losses on the encoder tokens and (b) top- $k$ decoder queries selection. We empirically observe that these greatly help improve the final performance and stabilize the optimization. The overall architecture of Sparse DETR including these components is depicted in Fig. 3.

Encoder Auxiliary Loss. In DETR variants, auxiliary detection heads are attached to decoder tilayers, but not to encoder layers. Due to a significantly larger number of encoder tokens (about 18k tokens) compared to decoder tokens (about 300), encoder auxiliary heads will heavily increase the computational cost. In Sparse DETR, however, only part of encoder tokens are refined by the encoder, and adding auxiliary heads only for sparsified encoder tokens is not a big burden.

We empirically observe that applying an auxiliary detection head along with Hungarian loss on the selected tokens stabilizes the convergence of deeper encoders by alleviating the vanishing gradient issue and even improves the detection performance. We conjecture that, following the analysis in Sun et al. (2021), applying Hungarian loss at the intermediate layers helps distinguish the confusing features in the encoder, which contributes to the detection performance in the final head.

Top- $k$ Decoder Queries. In DETR and Deformable DETR, decoder queries are given by only learnable object queries or with predicted reference points via another head after the encoder. In Efficient DETR (Yao et al., 2021), decoder takes a part of encoder output as input, similar to RoI Pooling (Ren et al., 2015). Here, an auxiliary detection head is attached to the encoder output $\mathbf { x } _ { \mathrm { e n c } }$ and the head calculates the objectness (class) score of each encoder output. Based on the score, the top- $k$ encoder outputs are passed as decoder queries, similar to objectness score-based encoder token sparsification. Since this outperforms the methods based on learnable object queries or the two-stage scheme, we include this top- $k$ decoder query selection in our final architecture.

Table 1: Detection results of Sparse DETR on COCO 2017 val set. Top- $k$ & BBR denotes that we sample the top- $k$ object queries instead of using the learned object queries (Yao et al., 2021), and perform bounding box refinement in the decoder block (Zhu et al., 2021), respectively. Note that the proposed encoder auxiliary loss is only applied to Sparse DETR. FLOPs and FPS are measured in the same way as used in Zhu et al. (2021). The results marked by $\dag , \ddag$ are the reported ones from Zhu et al. (2021) and Wang et al. (2021), respectively.   

<table><tr><td rowspan="2">Method</td><td rowspan="2">Epochs</td><td rowspan="2">Keeping ratio (p)</td><td rowspan="2">Top-k &amp;BBR</td><td colspan="6">AP50 AP75 APs APM APL</td><td rowspan="2"></td><td colspan="2"></td></tr><tr><td>AP</td><td></td><td></td><td></td><td></td><td></td><td>params FLOPs FPS</td><td></td></tr><tr><td colspan="10">ResNet-50 backbone:</td><td></td><td></td><td></td><td></td></tr><tr><td>F-RCNN-FPNt</td><td>109</td><td>N/A</td><td></td><td>42.0</td><td>62.1</td><td>45.5</td><td>26.6</td><td>45.4</td><td>53.4</td><td>42M</td><td>180G</td><td></td><td>26</td></tr><tr><td>DETRt</td><td>500</td><td>100%</td><td></td><td>42.0</td><td>62.4</td><td>44.2</td><td>20.5</td><td>45.8</td><td>61.1</td><td></td><td>41M</td><td>86G</td><td>28</td></tr><tr><td>DETR-DC5†</td><td>500</td><td>100%</td><td></td><td>43.3</td><td>63.1</td><td>45.9</td><td>22.5</td><td>47.3</td><td></td><td>61.1</td><td>41M</td><td>187G</td><td>12</td></tr><tr><td rowspan="2">PnP-DETR$</td><td>500</td><td>33%</td><td></td><td>41.1</td><td>61.5</td><td>43.7</td><td>20.8</td><td>44.6</td><td>60.0</td><td></td><td>-</td><td>1</td><td>-</td></tr><tr><td>500</td><td>50%</td><td></td><td>41.8</td><td>62.1</td><td>44.4</td><td>21.2</td><td>45.3</td><td>60.8</td><td>-</td><td></td><td>1</td><td>-</td></tr><tr><td rowspan="2">PnP-DETR-DC5‡</td><td>500</td><td>33%</td><td></td><td>42.7</td><td>62.8</td><td></td><td>45.1</td><td>22.4</td><td>46.2</td><td>60</td><td>1</td><td>-</td><td>-</td></tr><tr><td>500</td><td>50%</td><td></td><td>43.1</td><td>63.4</td><td>45.3</td><td>22.7</td><td>46.5</td><td></td><td>61.1</td><td>1</td><td>-</td><td>-</td></tr><tr><td rowspan="2">Deformable-DETR</td><td>50</td><td>100%</td><td></td><td>43.9</td><td>62.8</td><td>47.8</td><td>26.1</td><td></td><td>47.4</td><td>58.0</td><td>40M</td><td>173G</td><td>19.1</td></tr><tr><td>50</td><td>100% 10%</td><td>√</td><td>46.0</td><td>65.2</td><td>49.8</td><td>28.2</td><td>49.1</td><td></td><td>61.0</td><td>41M</td><td>177G</td><td>18.2</td></tr><tr><td rowspan="5">Sparse-DETR</td><td>50 50</td><td></td><td>√</td><td></td><td>45.3 65.8</td><td>49.3</td><td>28.4</td><td></td><td>48.3</td><td>60.1</td><td>41M</td><td>105G</td><td>25.3</td></tr><tr><td></td><td>20%</td><td>√</td><td></td><td>45.6 65.8</td><td></td><td>49.6</td><td>28.5</td><td>48.6</td><td>60.4</td><td>41M</td><td>113G</td><td>24.8</td></tr><tr><td>50</td><td>30%</td><td>√</td><td>46.0</td><td>65.9</td><td>49.7</td><td></td><td>29.1</td><td>49.1</td><td>60.6</td><td>41M</td><td>121G</td><td>23.2</td></tr><tr><td>50</td><td>40%</td><td>√</td><td>46.2</td><td>66.0</td><td>50.3</td><td>28.7</td><td></td><td>49.0</td><td>61.4</td><td>41M</td><td>128G</td><td>21.8</td></tr><tr><td>50</td><td>50%</td><td>√</td><td>46.3</td><td>66.0</td><td>50.1</td><td>29.0</td><td>49.5</td><td></td><td>60.8</td><td>41M</td><td>136G</td><td>20.5</td></tr><tr><td colspan="2">Swin-T backbone:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="2">DETR</td><td>500</td><td>100%</td><td></td><td>45.4</td><td>66.2</td><td>48.1</td><td>22.9</td><td>49.5</td><td>65.9</td><td>45M</td><td>92G</td><td>26.8</td></tr><tr><td colspan="2">Deformable-DETR</td><td>50</td><td>100%</td><td></td><td>45.7</td><td>65.3</td><td>49.9</td><td>26.9</td><td>49.4</td><td>61.2</td><td>40M</td><td>180G</td><td>15.9</td></tr><tr><td colspan="2"></td><td>50</td><td>100%</td><td>√</td><td>48.0</td><td>68.0</td><td>52.0</td><td>30.3</td><td>51.4</td><td>63.7</td><td>41M</td><td>185G</td><td>15.4</td></tr><tr><td colspan="2" rowspan="6">Sparse-DETR</td><td>50</td><td>10%</td><td>√</td><td>48.2</td><td>69.2</td><td>52.3</td><td>29.8</td><td>51.2</td><td>64.5</td><td>41M</td><td>113G</td><td>21.2</td></tr><tr><td>50</td><td></td><td>20%</td><td>√</td><td>48.8</td><td>69.4</td><td>53.0</td><td></td><td></td><td></td><td>41M</td><td>121G</td><td>20.0</td></tr><tr><td>50</td><td>30%</td><td></td><td>√</td><td>49.1</td><td>69.5</td><td></td><td>30.4 31.4</td><td>51.9 52.5</td><td>64.8 65.1</td><td>41M</td><td>129G</td><td>18.9</td></tr><tr><td>50</td><td></td><td></td><td>√</td><td>49.2</td><td>69.5</td><td>53.5 53.5</td><td>31.4</td><td>52.9</td><td>64.8</td><td>41M</td><td>136G</td><td>18.0</td></tr><tr><td>50</td><td>40% 50%</td><td>√</td><td></td><td>49.3</td><td>69.5</td><td>53.3</td><td>32.0</td><td>52.7</td><td>64.9</td><td>41M</td><td>144G</td><td>17.2</td></tr></table>

# 4 EXPERIMENTS

We compare Sparse DETR with the conventional object detectors, including the recently proposed ones in the DETR family. In addition, we conduct an ablation study to support our claims in Section 3, presenting the performance comparison between token selection criteria (OS vs. DAM), the effectiveness of the encoder auxiliary loss, and the dynamic sparsification during inference.

Implementation Details. We use ResNet-50 (He et al., 2016) and Swin Transformer (Liu et al., 2021) as pre-trained backbone networks, where Swin Transformer is one of the state-of-the-art architecture in the ViT family. We stack 6 encoder and 6 decoder layers, each with an auxiliary head at the end. We train the model on a $4 \times \mathsf { V } 1 0 0$ GPU machine with a total batch size of 16, for 50 epochs, where the initial learning rate is 0.0002 and decayed by 1/10 at the 40 epoch. Unless otherwise specified, we use the same hyperparameters as in Deformable DETR.

# 4.1 COMPARISON WITH OBJECT DETECTION BASELINES

Baselines. We compare Sparse DETR with Faster-RCNN with FPN (Lin et al., 2017), DETR (Carion et al., 2020), Deformable DETR (Zhu et al., 2021), and $\mathrm { P n P }$ DETR (Wang et al., 2021). We also compare with DETR and Deformable DETR that uses Swin-Tiny (Liu et al., 2021) as a backbone. Here, for brevity, we denote Deformable DETR with the top- $k$ object query selection and bounding box refinement, as Deformable ${ \mathrm { D E T R } } +$ . In Sparse DETR, encoder tokens are sparsified with keeping ratios of $10 \%$ , $20 \%$ , $30 \%$ , $40 \%$ , and $50 \%$ , using DAM criterion. We demonstrate detection performance and inference costs on COCO val2017 dataset.

Result. Table 1 shows the results of Sparse DETR and the other baselines on COCO val2017 set. Remarkably, on the ResNet-50 backbone, Sparse DETR with a keeping ratio over $30 \%$ outperforms all the baselines while minimizing the computational cost. Even with the keeping ratio reduced down to $10 \%$ , Sparse DETR still performs better than most baselines except for Deformable ${ \mathrm { D E T R } } +$ . More surprisingly, on the Swin-T backbone, Sparse DETR only with the keeping ratio $10 \%$ outperforms all the baselines with no exception, while improving FPS by $3 8 \%$ compared to Deformable DETR $^ +$ .

![](Images_J728TKKV/2cb7d08c907957310b4fa251fcc029885f1b5491bf85d816ae087a40096b2203.jpg)  
Figure 4: Selection criteria. Comparison of the performance with respect to encoder token selection criteria for different backbones.

![](Images_J728TKKV/4b990c5070d658f80240b99afdf39dad2f8461cbd160456fa66c93fffcc5abf1.jpg)  
Figure 5: Correlation graph. Correlation graphs of OS and DAM during training.

Remark that, compared to the most competitive baseline, Deformable $\mathrm { D E T R + }$ , the improvement in $\mathsf { A P } _ { L }$ is relatively noticeable on the Swin-T backbone even under the extreme sparsity of $10 \%$ , while the performance gap on the ResNet-50 backbone comes evenly from different sizes of objects. We conjecture that it is because a single token in Swin-T can hold a wider region of information than the one in ResNet-50, so even if we aggressively sparsify the encoder token, the network seems to have enough information to detect objects.

# 4.2 COMPARISON BETWEEN TOKEN SELECTION CRITERIA

Baselines. To verify the benefits of the proposed saliency criteria, we compare three token sparsification criteria: random, Objectness Score (OS), and Decoder cross-Attention Map (DAM). The random baseline samples a fixed ratio of arbitrary tokens. Note that the proposed encoder auxiliary loss is applied for all the methods.

Result. As illustrated in Fig. 4, the random strategy suffers noticeable performance degradation. On the other hand, the DAM-based model outperforms the OS-based model at every ratio and almost catches up with the non-sparse baseline when using $50 \%$ of encoder tokens. See the Appendix A.4 for detailed results of this experiment.

To analyze the reason that DAM-based model outperforms its counterpart, we measure the overlap between the encoder tokens referred by the decoder and the tokens refined by the encoder. As a metric, we compute a scalar correlation Corr as:

$$
\mathrm { \Gamma } _ { C o r r } : = \frac { \sum _ { x \in \Omega _ { D } \cap \Omega _ { s } ^ { \rho } } \mathrm { D A M } _ { x } } { \sum _ { x \in \Omega _ { D } } \mathrm { D A M } _ { x } } ,
$$

where $\Omega _ { D }$ is the encoder token set referred by the decoder and $\mathrm { D A M } _ { x }$ is the DAM-value corresponding to token $x$ . This Corr metric indicates the ratio of tokens polished by the encoder among the tokens referred by the decoder.

Fig. 5 demonstrates that Corr of DAM-based model rises higher than that of OS-based model. This result implies that DAM-based model is a more suitable sparsification method for the decoder, because the tokens referenced by the decoder are explicitly refined in the encoder, which achieves better detection performance. See the Appendix A.4 for detailed results of this experiment.

![](Images_J728TKKV/0fdd80e091e7247c07f4b0c74beb0bbe5bed882a749d4573a18caacbe82eac01.jpg)  
Figure 6: Ablation of # encoder layers.   
Figure 7: Dynamic sparsification.

# 4.3 EFFECTIVENESS OF THE ENCODER AUXILIARY LOSS

Owing to the sparsified token set in our model, we can apply the auxiliary loss to the encoder layers without sacrificing too much computational cost. Apart from improved efficiency and performance, we find another benefit of the encoder auxiliary loss that allows us to safely stack more encoder layers without failing to converge.

As shown in Fig. 6, the encoder auxiliary loss not only enhances detection performance, but also consistently increases detection performance as the encoder layers doubled to 12. However, we observe that the training without its assistance utterly fails when using 12 encoder layers. We argue that gradient propagated through decoder cross-attention vanishes as we stack more encoder layers, thus intermediate gradients from the auxiliary loss are required. The observations reported in Appendix A.5 supports this assertion and Appendix A.6 details the results of Fig. 6.

# 4.4 DYNAMIC SPARSIFICATION FOR INFERENCE STAGE

To deploy the models in various hardware conditions of real-world applications, one often should retrain them at different scales according to the performance-computation trade-off required. We evaluate if our model trained with a fixed sparsity can adapt well to dynamic sparsity at inference time to check out Sparse DETR can avoid that hassle. Figure 7 shows the performance under the varied keeping ratio $( \rho )$ during inference when the model trained using the Swin-T backbone and $30 \%$ of encoder tokens with the DAM-based method. When the inference keeping ratio is small, the performance of dynamic sparsification is slightly degraded, but the overall performance is satisfactory at various keeping ratios given that only a single model is used.

PnP DETR introduces dynamic ratio training to achieve similar performance to the fixed keeping ratio counterpart. However, without the additional trick, it suffers significant performance degradation, for instance, $5 . 0 \ \mathrm { A P }$ drop when training/inference keeping ratio is 0.33/0.5, despite the increased number of encoder tokens. On the contrary, Sparse DETR achieves 0.2 AP improvement in a similar condition where the training/inference keeping ratio is 0.3/0.5. To conclude, our method shows better robustness compared to $\mathrm { P n P }$ DETR without further treatment, showing a greater potential of dynamic adaptation to different hardware environments. Note that any technique such as dynamic ratio training is orthogonal to our method and introducing it may bring even more robustness.

# 5 CONCLUSION

In this paper, we have presented encoder token sparsification algorithm that lowers the computational cost of the encoder, which is a computational bottleneck in the DETR and Deformable DETR. By doing so, the proposed Sparse DETR architecture outperforms the Deformable DETR even when using only $10 \%$ of the encoder token, and decreases the overall computation by $38 \%$ , and increases the FPS by $42 \%$ compared to the Deformable DETR. We hope that our proposed method will provide insights to effectively detect objects in the transformer structure in the future.

# REFERENCES

Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In ICLR (Poster). OpenReview.net, 2019.

Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019a.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019b.

Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarl ´ os, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, ´ David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In ICLR. OpenReview.net, 2021.

Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN: object detection via region-based fully convolutional networks. In NIPS, pp. 379–387, 2016.

Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, 2017.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.

Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross B. Girshick. Mask R-CNN. In ´ ICCV, 2017.

Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016.

Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. CoRR, abs/1912.12180, 2019.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 5156–5165. PMLR, 2020.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR. OpenReview.net, 2020.

Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. DeeplySupervised Nets. In Guy Lebanon and S. V. N. Vishwanathan (eds.), Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pp. 562–570, San Diego, California, USA, 09–12 May 2015. PMLR.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ´ ECCV, 2014.

Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. ´ Feature pyramid networks for object detection. In CVPR, 2017.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.

Bowen Pan, Yifan Jiang, Rameswar Panda, Zhangyang Wang, Rogerio Feris, and Aude Oliva. ´ IA-RED2: Interpretability-aware redundancy reduction for vision transformers. arXiv preprint arXiv:2106.12620, 2021.

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, volume 80 of Proceedings of Machine Learning Research, pp. 4052–4061. PMLR, 2018.

Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. DynamicViT: efficient vision transformers with dynamic token sparsification. arXiv preprint arXiv:2106.02034, 2021.

Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. In NIPS, pp. 91–99, 2015.

Byungseok Roh, Wuhyun Shin, Ildoo Kim, and Sungwoong Kim. Spatilly consistent representation learning. In CVPR. IEEE, 2021.

Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, and Ping Luo. What makes for end-to-end object detection? In ICML, volume 139, pp. 9934–9944, 2021.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pp. 1–9. IEEE Computer Society, 2015.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.

Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan L. Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV (4), volume 12349 of Lecture Notes in Computer Science, pp. 108–126. Springer, 2020.

Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning deep transformer models for machine translation. In ACL (1), pp. 1810–1822. Association for Computational Linguistics, 2019.

Tao Wang, Li Yuan, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. PnP-DETR: towards efficient visual analysis with transformers. In ICCV, 2021.

Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient DETR: improving end-to-end object detector with dense prior. arXiv preprint arXiv:2104.01318, 2021.

Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: deformable transformers for end-to-end object detection. In ICLR, 2021.

# A APPENDIX

# A.1 IMPLEMENTATION DETAILS OF THE SCORING NETWORK

The scoring network is consists of 4 linear layers where Layer Normalization (Ba et al., 2016) comes before the first layer and every layer except for the last one is followed by GELU (Hendrycks & Gimpel, 2016) activation. The output dimension of the 1st layer is 256 and halved to 128 and 64 at the 2nd and 3rd layers. The last layer outputs 1-d logit for the BCE loss. Since the network locally processes the input tokens in a token-wise manner, the final decisions may overlook global statistics without additional treatment. To remedy this issue, we set aside half of the output dimension of the first layer as a global feature, and average them across the whole token set, then concatenate it with each of the remained local features to maintain the original dimension. We also exclude the tokens that correspond to the zero-padded area when selecting top- $\cdot \rho \%$ scores, thereby we can prevent those tokens from participating in Hungarian matching process and getting meaningless gradients from the detection objective.

# A.2 DAM CREATION IN DEFORMABLE ATTENTION

As attention offset calculated in deformable attention is a fractional position, deformable attention uses bilinear interpolation to get values. Thus, we also use bilinear interpolation to obtain DAM.

Assume that, one of the attention offsets, weights and the reference point of decoder object query $q$ is calculated as $p , A$ and $r$ , respectively. Then, deformable attention takes values as:

$$
\sum _ { x } A \cdot G ( x , r + p ) \cdot v ( x )
$$

, where $x$ enumerates all integral spatial locations in the feature map, $G ( \cdot , \cdot )$ is the bilinear interpolation kernel defined as $G ( a , b ) = \operatorname* { m a x } ( 0 , 1 - | a _ { x } - b _ { x } | ) \cdot \operatorname* { m a x } ( 0 , 1 - | a _ { y } - b _ { y } | )$ and $v$ is the values. Similarly, we accumulate DAM-value for location $x$ as:

$$
\sum _ { ( p , A , r ) } A \cdot G ( x , r + p )
$$

. Then, we accumulate DAM over every decoder object query.

# A.3 ALTERNATIVE OBJECTIVES FOR DAM-BASED MODEL

As a training objective of the scoring network using DAM, we can consider other alternatives as long as they can encourage the predicted scores to represent the relative saliency of the encoder tokens. One of the naive alternatives is the regression loss by which the scoring network directly predicts the values in DAM. The ranking loss can be another choice with which the network focuses more on learning the relativeness rather than estimating the set of salient tokens.

Figure 8 shows the default BCE loss outperforms the alternatives. First, it is well-known that the regression problem is much harder than classification. Furthermore, since the value of DAM changes during training, the regression loss to predict the accurate value is more difficult. In case of the pairwise ranking loss, ranking the DAM elements may also be unstable as DAM gradually evolves. Meanwhile, the BCE loss may reduce those element-level noises down to the set-level in that its binary (keep or drop) targets retain more consistency compared to the exact values or ranks. Refer to Table 2 to see the exact values of the points represented in Figure 8.

![](Images_J728TKKV/a916061eaf67de7468b2873f911a6fefd0d820c8815b41c27dd685994c99c8eb.jpg)  
Figure 8: DAM loss ablation

Table 2: Comparision between the alternative objectives for DAM-based scoring network.   

<table><tr><td>Loss</td><td>Keeping ratio (p)</td><td>AP AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td rowspan="4">smoothed L1</td><td>10%</td><td>47.8 68.9</td><td>51.7</td><td>29.8</td><td>50.9</td><td>64.0</td></tr><tr><td>20%</td><td>48.4 69.0</td><td>52.5</td><td>31.1</td><td>51.4</td><td>64.7</td></tr><tr><td>30%</td><td>48.6 69.0</td><td>52.6</td><td>31.1</td><td>51.9</td><td>64.7</td></tr><tr><td>40%</td><td>48.6 69.3</td><td>52.9</td><td>33.4</td><td>51.8</td><td>64.5</td></tr><tr><td rowspan="4">ranking</td><td>10%</td><td>48.0 69.1</td><td>52.1</td><td>29.9</td><td>51.4</td><td>64.6</td></tr><tr><td>20%</td><td>48.7 69.5</td><td>53.0</td><td>31.1</td><td>51.8</td><td>65.1</td></tr><tr><td>30%</td><td>48.8 69.2</td><td>52.8</td><td>31.4</td><td>52.0</td><td>64.9</td></tr><tr><td>40%</td><td>48.9 69.3</td><td>53.1</td><td>31.5</td><td>52.2</td><td>64.7</td></tr><tr><td rowspan="4">BCE</td><td>10%</td><td>48.2 69.2</td><td>52.3</td><td>29.8</td><td>51.2</td><td>64.5</td></tr><tr><td>20%</td><td>48.8 69.4</td><td>53.0</td><td>30.4</td><td>51.9</td><td>64.8</td></tr><tr><td>30%</td><td>49.1 69.5</td><td>53.5</td><td>31.4</td><td>52.5</td><td>65.1</td></tr><tr><td>40%</td><td>49.2 69.5</td><td>53.5</td><td>31.4</td><td>52.9</td><td>64.8</td></tr></table>

Table 3: Comparison between token selection criteria.   

<table><tr><td>Scoring method</td><td>Keeping</td><td colspan="6"></td><td colspan="3"></td></tr><tr><td rowspan="2"></td><td rowspan="2">ratio (p) ResNet-50 backbone:</td><td>AP AP50</td><td></td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td><td>params</td><td>FLOPs</td><td>FPS</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">N/A</td><td>100%</td><td>46.3</td><td>65.8</td><td>50.1</td><td>29.0</td><td>49.4</td><td>61.7</td><td>41M</td><td>177G</td><td>18.2</td></tr><tr><td>0%</td><td>42.2</td><td>63.0</td><td>45.6</td><td>25.9</td><td>45.3</td><td>56.5</td><td>36M</td><td>91G</td><td>35.0</td></tr><tr><td rowspan="4">random</td><td>10% 20%</td><td>43.6 44.0</td><td>64.3 64.8</td><td>47.2 47.8</td><td>26.7 27.3</td><td>46.9</td><td>58.4</td><td>41M</td><td>102G</td><td>27.7</td></tr><tr><td>30%</td><td>44.0</td><td></td><td></td><td></td><td>47.0</td><td>58.4</td><td>41M</td><td>110G</td><td>25.6</td></tr><tr><td></td><td></td><td>64.9</td><td>47.5</td><td>27.4</td><td>47.4</td><td>58.2</td><td>41M</td><td>117G</td><td>24.1</td></tr><tr><td>40% 50%</td><td>44.5 44.4</td><td>65.1</td><td>48.0 48.0</td><td>27.3 27.8</td><td>47.8 47.4</td><td>59.8 59.2</td><td>41M 41M</td><td>125G</td><td>22.5 21.1</td></tr><tr><td rowspan="5">OS</td><td>10%</td><td>44.9</td><td>64.8 65.2</td><td>48.7</td><td>27.9</td><td>47.8</td><td>60.4</td><td>41M</td><td>133G 106G</td><td>26.6</td></tr><tr><td>20%</td><td>45.5</td><td>65.5</td><td>49.3</td><td>28.7</td><td>48.3</td><td>60.5</td><td>41M</td><td>114G</td><td>24.7</td></tr><tr><td>30%</td><td>45.7</td><td>65.8</td><td>49.5</td><td>29.7</td><td>48.5</td><td>60.8</td><td>41M</td><td>121G</td><td>23.2</td></tr><tr><td>40%</td><td>45.8</td><td>65.5</td><td>49.8</td><td>29.1</td><td>48.8</td><td></td><td>41M</td><td></td><td>21.8</td></tr><tr><td>50%</td><td>46.0</td><td>65.9</td><td>49.8</td><td>28.8</td><td>48.9</td><td>60.5 60.6</td><td>41M</td><td>129G 136G</td><td>20.6</td></tr><tr><td rowspan="5">DAM</td><td>10%</td><td>45.3</td><td>65.8</td><td>49.3</td><td>28.4</td><td>48.3</td><td>60.1</td><td>41M</td><td>105G</td><td>26.5</td></tr><tr><td>20%</td><td>45.6</td><td>65.8</td><td>49.6</td><td>28.5</td><td>48.6</td><td>60.4</td><td>41M</td><td>113G</td><td>24.8</td></tr><tr><td>30%</td><td>46.0</td><td>65.9</td><td>49.7</td><td>29.1</td><td>49.1</td><td>60.6</td><td>41M</td><td>121G</td><td>23.2</td></tr><tr><td>40%</td><td>46.2</td><td>66.0</td><td>50.3</td><td>28.7</td><td>49.0</td><td>61.4</td><td>41M</td><td>128G</td><td>21.8</td></tr><tr><td>50%</td><td>46.3</td><td>66.0</td><td>50.1</td><td>29.0</td><td>49.5</td><td>60.8</td><td>41M</td><td>136G</td><td>20.5</td></tr><tr><td colspan="2">Swin-T backbone:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">N/A</td><td>100% 0%</td><td>49.4 43.7</td><td>69.4</td><td>53.5</td><td>31.9</td><td>52.6</td><td>65.1</td><td>41M</td><td>185G</td><td>15.4</td></tr><tr><td>10%</td><td>45.5</td><td>65.8</td><td>46.9</td><td>27.0</td><td>46.7</td><td>60.0</td><td>37M</td><td>96G</td><td>26.5</td></tr><tr><td rowspan="5">random</td><td>20%</td><td>45.6</td><td>67.6</td><td>48.8</td><td>28.4</td><td>48.5</td><td>62.2</td><td>41M</td><td>110G</td><td>22.1</td></tr><tr><td>30%</td><td></td><td>67.5</td><td>49.2</td><td>28.6</td><td>49.1</td><td>62.2</td><td>41M</td><td>118G</td><td>20.8</td></tr><tr><td></td><td>46.2</td><td>68.1</td><td>49.7</td><td>29.5</td><td>49.7</td><td>63.0</td><td>41M</td><td>125G</td><td>19.7</td></tr><tr><td>40%</td><td>46.5</td><td>68.2</td><td>50.0</td><td>29.9</td><td>49.8</td><td>63.0</td><td>41M</td><td>133G</td><td>18.7</td></tr><tr><td>50%</td><td>47.2</td><td>68.3</td><td>50.9</td><td>29.1</td><td>50.4</td><td>63.9</td><td>41M</td><td>141G</td><td>17.7</td></tr><tr><td rowspan="5">OS</td><td>10%</td><td>48.0</td><td>69.1</td><td>52.1</td><td>29.9</td><td>51.1</td><td>64.4</td><td>42M</td><td>114G</td><td>21.4</td></tr><tr><td>20%</td><td>48.3</td><td>69.1</td><td>52.5</td><td>30.4</td><td>51.6</td><td>64.2</td><td>42M</td><td>122G</td><td>20.2</td></tr><tr><td>30%</td><td>48.6</td><td>69.2</td><td>53.0</td><td>31.0</td><td>52.0</td><td>64.6</td><td>42M</td><td>129G</td><td>18.6</td></tr><tr><td>40%</td><td>48.9</td><td>69.4</td><td>53.1</td><td>33.0</td><td>51.9</td><td>64.5</td><td>42M</td><td>137G</td><td>18.2</td></tr><tr><td>50%</td><td>49.0</td><td>69.2</td><td>53.5</td><td>31.2</td><td>52.4</td><td>65.0</td><td>42M</td><td>145G</td><td>17.2</td></tr><tr><td rowspan="5">DAM</td><td>10%</td><td>48.2</td><td>69.2</td><td>52.3</td><td>29.8</td><td>51.2</td><td>64.5</td><td>41M</td><td>113G</td><td>21.2</td></tr><tr><td>20%</td><td>48.8</td><td>69.4</td><td>53.0</td><td>30.4</td><td>51.9</td><td>64.8</td><td>41M</td><td>121G</td><td>20.0</td></tr><tr><td>30%</td><td>49.1</td><td>69.5</td><td>53.5</td><td>31.4</td><td>52.5</td><td>65.1</td><td>41M</td><td>129G</td><td>18.9</td></tr><tr><td>40%</td><td>49.2</td><td>69.5</td><td>53.5</td><td>31.4</td><td>52.9</td><td>64.8</td><td>41M</td><td>136G</td><td>18.0</td></tr><tr><td>50%</td><td>49.3</td><td>69.5</td><td>53.3</td><td>32.0</td><td>52.7</td><td>64.9</td><td>41M</td><td>144G</td><td>17.2</td></tr></table>

![](Images_J728TKKV/13d5745ebe68019dffbea99293309ee106ffe06960286b4382b811395b614a6f.jpg)  
Figure 9: Layerwise gradient norm in DETR variants. An observation of the vanishing gradient problem on DETR variants with different backbones by measuring $\ell ^ { 2 }$ -norm of gradients in a layerwise manner. The first letter in $x$ -axis label represents module name, specifically, ‘B’ for the backbone and ‘E’ for the encoder, and the second number represents $i$ -th layer in that module. (a), (b): Layerwise gradient norm of DETR with ResNet-50 backbone. With the default settings(PostLN), the gradient scale generally decreases as more encoder layers are stacked, while the Pre-LN technique preserves gradient magnitude even in deeper early layers. (c) : Layerwise gradient norm of Deformable-DETR with Swin-T backbone. In this case, the Pre-LN fails to prevent vanishing gradient while the encoder auxiliary loss(denoted as Post-LN $^ +$ EncAux) proposed in this paper effectively resolves this issue.

# A.4 EXPERIMENTAL DETAILS FOR DIFFERENT TOKEN SELECTION CRITERIA

Table 3 contains the specific values used to plot (a) ResNet-50 and (b) Swin-T backbone in Figure 4. Additionally, they also include a lower-bound baseline that has no scoring method with keeping ratio $0 \%$ , meaning that the entire encoder block is removed and the backbone features are directly passed to the decoder. Even with the lowest keeping ratio $10 \%$ , all the scoring methods including random criterion outperform this lower-bound baseline. Note that all experiments reported in Table 3 except for the keeping ratio $0 \%$ use the encoder auxiliary loss for training.

# A.5 VANISHING GRADIENT PROBLEM IN THE DEEP END-TO-END DETECTORS

As shown in Section 4.2 in Carion et al. (2020), they observe that the performance of DETR gradually improves with more encoder layers. To reproduce this result, we used the default settings of the official code, but only changed the number of encoder layers. However, we fail to train the DETR model when using more than 9 encoder layers, which is probably due to different hyperparameters from the ones used in their experiments. Interestingly, we also found that the DETR model converges stably with the Pre-LN architecture(Baevski & Auli, 2019; Child et al., 2019b; Wang et al., 2019) that is known to be a better choice than the canonical Post-LN when the number of layers of transformer increases. We used the pre norm option the authors already have implemented in their code.

Table 4: Effectiveness of the encoder auxiliary loss using Swin-T. When the number of encoder layers is more than 9, the model training fails, but if the encoder auxiliary loss is adopted, the model training is feasible regardless of the number of encoder layers, and accuracy is improved.   

<table><tr><td rowspan="2">#of encoder</td><td rowspan="2">Keeping ratio (p)</td><td rowspan="2">Aux. loss</td><td colspan="6"></td><td rowspan="2"></td><td colspan="2"></td></tr><tr><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td><td>params FLOPs</td><td>FPS</td></tr><tr><td rowspan="9">6</td><td>100%</td><td></td><td>48.0</td><td>68.0</td><td>52</td><td>30.3</td><td>51.4</td><td>63.7</td><td>41M</td><td>185G</td><td>15.4</td></tr><tr><td>100%</td><td>√</td><td>49.4</td><td>69.4</td><td>53.5</td><td>31.9</td><td>52.6</td><td>65.1</td><td>41M</td><td>185G</td><td>15.4</td></tr><tr><td>10%</td><td></td><td>46.8</td><td>68.0</td><td>50.6</td><td>29.7</td><td>49.7</td><td>63.3</td><td>41M</td><td>113G</td><td>21.2</td></tr><tr><td>20%</td><td></td><td>47.5</td><td>68.3</td><td>51.4</td><td>31.4</td><td>50.4</td><td>64.4</td><td>41M</td><td>121G</td><td>20.0</td></tr><tr><td>30%</td><td></td><td>47.6</td><td>67.9</td><td>51.4</td><td>29.9</td><td>51.1</td><td>63.9</td><td>41M</td><td>129G</td><td>18.9</td></tr><tr><td>40%</td><td></td><td>47.6</td><td>68.2</td><td>51.5</td><td>30.3</td><td>50.8</td><td>64.0</td><td>41M</td><td>136G</td><td>18.0</td></tr><tr><td>10%</td><td></td><td>48.2</td><td>69.2</td><td>52.3</td><td>29.8</td><td>51.2</td><td>64.5</td><td>41M</td><td>113G</td><td>21.2</td></tr><tr><td>20%</td><td></td><td>48.8</td><td>69.4</td><td>53.0</td><td>30.4</td><td>51.9</td><td>64.8</td><td>41M</td><td>121G</td><td>20.0</td></tr><tr><td>30%</td><td></td><td>49.1</td><td>69.5</td><td>53.5</td><td>31.4</td><td>52.5</td><td>65.1</td><td>41M</td><td>129G</td><td>18.9</td></tr><tr><td>9</td><td>40%</td><td></td><td>49.2</td><td>69.5</td><td>53.5</td><td>31.4</td><td>52.9</td><td>64.8</td><td>41M</td><td>136G</td><td>18.0</td></tr><tr><td rowspan="5">12</td><td>100%</td><td>√</td><td>49.7</td><td>69.4</td><td>54.1</td><td>32.4</td><td>52.9</td><td>65.4</td><td>44M</td><td>220G</td><td>12.8</td></tr><tr><td>100%</td><td>√</td><td>50.1</td><td>69.6</td><td>54.6</td><td>32.2</td><td>53.4</td><td>65.8</td><td>46M</td><td>261G</td><td>11.0</td></tr><tr><td>10%</td><td></td><td>49.0</td><td>69.5 69.6</td><td>53.5 53.5</td><td>31.6</td><td>52.2</td><td>65.2</td><td>46M</td><td>128G</td><td>19.2</td></tr><tr><td>20% 30%</td><td>公</td><td>49.4 49.3</td><td>69.4</td><td>53.6</td><td>31.9 31.7</td><td>52.8 52.5</td><td>65.4 65.6</td><td>46M 46M</td><td>143G 158G</td><td>17.5</td></tr><tr><td>40%</td><td></td><td>49.8</td><td>69.8</td><td>54.3</td><td>33.1</td><td>53.4</td><td>65.4</td><td>46M</td><td>173G</td><td>15.6 14.6</td></tr></table>

Figure 9(a) and 9(b) illustrate that gradient norm of each layer from bottom to top in 6 and 12 encoder layers when using Post-LN and Pre-LN, respectively. We compute the $\ell ^ { 2 }$ -norm of the gradients for all parameters in a particular layer, as if they are concatenated into a single vector. To see training dynamics in the early stage of training, we track the gradients computed on a fixed set of training data and average them over the first 150 steps with a batch size of 2. Note that we applied Pre-LN only to the encoder module for a fair comparison between only the early modules although it could be used in any other transformer modules, e.g. backbone or decoder.

We found that the vanishing gradient issue is generally observed regardless of the encoder size. When we double the size of the encoder, as one can expect, the gradient in the early layers ends up with an even smaller scale, which may have caused the convergence failure. Meanwhile, the PreLN technique seems to significantly alleviate this issue even for the deeper encoder by maintaining the gradient scale evenly through the encoder layers and conveying a strong training signal to the backbone layers.

On the other hand, as shown in Figure 9(c), Deformable-DETR suffers from the same problem of vanishing gradient and even the Pre-LN technique does not help in this case. Meanwhile, the encoder auxiliary loss proposed in our paper drastically amplifies the gradient magnitude in the early layers by providing aggressive intermediate objectives for each encoder layer. Note that it also creates good synergy with our main sparsification strategy owing to reduced training cost. We claim that this observation supports our motivation of introducing the encoder auxiliary loss.

# A.6 EXPERIMENTAL DETAILS FOR EFFECTIVENESS OF THE ENCODER AUXILIARY LOSS

Table 4 presents the detailed values of Figure 4. As shown in the Table 4 and discussed in Section A.5, training of a deeper encoder of more than 9 layers fails without the auxiliary loss, but if it is adopted, the convergence becomes feasible as the intermediate gradients provided to the early encoder layers augment the vanishing gradient back-propagated from the decoder module.

Table 5: Detection results of Sparse DETR with SCRL initialization using ResNet-50. The same environment and hyperparameters as Experiments section are used, except for initializing the backbone with SCRL (Roh et al., 2021) model. The results marked by $\ S$ mean that the backbone network is initialized by SCRL instead of the ImageNet (Deng et al., 2009) pre-trained one.   

<table><tr><td rowspan="2">Method</td><td rowspan="2">Keeping ratio (p)</td><td colspan="6"></td><td rowspan="2"></td><td colspan="2"></td></tr><tr><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td><td>params FLOPs</td><td>FPS</td></tr><tr><td rowspan="5">Sparse-DETR</td><td>10%</td><td>45.3</td><td>65.8</td><td>49.3</td><td>28.4</td><td>48.3</td><td>60.1</td><td>41M</td><td>105G</td><td>25.3</td></tr><tr><td>20%</td><td>45.6</td><td>65.8</td><td>49.6</td><td>28.5</td><td>48.6</td><td>60.4</td><td>41M</td><td>113G</td><td>24.8</td></tr><tr><td>30%</td><td>46.0</td><td>65.9</td><td>49.7</td><td>29.1</td><td>49.1</td><td>60.6</td><td>41M</td><td>121G</td><td>23.2</td></tr><tr><td>40%</td><td>46.2</td><td>66.0</td><td>50.3</td><td>28.7</td><td>49.0</td><td>61.4</td><td>41M</td><td>128G</td><td>21.8</td></tr><tr><td>50%</td><td>46.3</td><td>66.0</td><td>50.1</td><td>29.0</td><td>49.5</td><td>60.8</td><td>41M</td><td>136G</td><td>20.5</td></tr><tr><td rowspan="5">Sparse-DETR$</td><td>10%</td><td>46.9</td><td>67.2</td><td>51.0</td><td>30.2</td><td>49.7</td><td>62.3</td><td>41M</td><td>105G</td><td>25.3</td></tr><tr><td>20%</td><td>47.3</td><td>67.1</td><td>51.4</td><td>29.7</td><td>50.3</td><td>62.7</td><td>41M</td><td>113G</td><td>24.8</td></tr><tr><td>30%</td><td>47.4</td><td>67.3</td><td>51.4</td><td>30.1</td><td>50.5</td><td>62.4</td><td>41M</td><td>121G</td><td>23.2</td></tr><tr><td>40%</td><td>47.7</td><td>67.4</td><td>51.6</td><td>30.0</td><td>50.8</td><td>62.9</td><td>41M</td><td>128G</td><td>21.8</td></tr><tr><td>50%</td><td>47.9</td><td>67.5</td><td>52.1</td><td>30.5</td><td>51.2</td><td>63.2</td><td>41M</td><td>136G</td><td>20.5</td></tr></table>

Table 6: Performance of Sparse DETR with Swin-B. The same environment and hyperparameters as Experiments section are used, except for changing the backbone to a larger scale. Note that Aux. loss means only the ones applied to the encoder layers.   
A.7 EFFECTIVENESS OF USING A DENSE REPRESENTATION AS BACKBONE INITIALIZATION   

<table><tr><td rowspan="2">Backbone</td><td rowspan="2">Keeping ratio (p)</td><td rowspan="2">Aux. loss</td><td colspan="5"></td><td rowspan="2"></td><td colspan="2">FLOPs</td></tr><tr><td>AP</td><td>AP50</td><td>AP75 APs</td><td>APM</td><td>APL</td><td>params</td><td>FPS</td></tr><tr><td rowspan="5">Swin-T</td><td>100%</td><td></td><td>48.0</td><td>68.0</td><td>52.0</td><td>30.3</td><td>51.4</td><td>63.7</td><td>41M</td><td>185G</td><td>15.4</td></tr><tr><td>10%</td><td></td><td>48.2</td><td>69.2</td><td>52.3</td><td>29.8</td><td>51.2</td><td>64.5</td><td>41M</td><td>113G</td><td>21.2</td></tr><tr><td>20%</td><td></td><td>48.8</td><td>69.4</td><td>53.0</td><td>30.4</td><td>51.9</td><td>64.8</td><td>41M</td><td>121G</td><td>20.0</td></tr><tr><td>30%</td><td></td><td>49.1</td><td>69.5</td><td>53.5</td><td>31.4</td><td>52.5</td><td>65.1</td><td>41M</td><td>129G</td><td>18.9</td></tr><tr><td>40%</td><td></td><td>49.2</td><td>69.5</td><td>53.5</td><td>31.4</td><td>52.9</td><td>64.8</td><td>41M</td><td>136G</td><td>18.0</td></tr><tr><td rowspan="5">Swin-B</td><td>100%</td><td></td><td>52.5</td><td>72.9</td><td>56.9</td><td>34.7</td><td>56.5</td><td>69.6</td><td>101M</td><td>400G</td><td>7.6</td></tr><tr><td>10%</td><td></td><td>52.2</td><td>73.5</td><td>57.0</td><td>34.0</td><td>56.3</td><td>70.3</td><td>101M</td><td>335G</td><td>8.8</td></tr><tr><td>20%</td><td></td><td>53.1</td><td>73.8</td><td>57.9</td><td>34.6</td><td>56.9</td><td>70.6</td><td>101M</td><td>343G</td><td>8.6</td></tr><tr><td>30%</td><td></td><td>53.2</td><td>73.7</td><td>57.7</td><td>35.3</td><td>56.8</td><td>70.8</td><td>101M</td><td>350G</td><td>8.4</td></tr><tr><td>40%</td><td></td><td>53.3</td><td>73.4</td><td>58.0</td><td>36.3</td><td>57.2</td><td>70.9</td><td>101M</td><td>358G</td><td>8.2</td></tr></table>

Recently, many self-supervised learning methods through contrastive learning have been studied, and in particular, methods for obtaining dense representations with better performance for localization downstream tasks such as object detection are in the spotlight. In order to check whether our proposed method is effective even when such dense representation is used, the backbone network is initialized with the SCRL (Roh et al., 2021) model that aims to learn dense representations in a selfsupervised way instead of initializing with the ImageNet (Deng et al., 2009) pre-trained one. Just as the SCRL model outperformed the ImageNet pre-trained model in various localization downstream tasks, our proposed method, Sparse DETR, also shows better performance in all keeping ratios $( \rho )$ without the influence of encoder token sparsification as shown in Table 5.

# A.8 USING A LARGER TRANSFORMER-BASED BACKBONE(SWIN-B)

We perform experiments on Sparse DETR with Swin-Base(Liu et al., 2021) backbone to see if our method shows similar efficiency and performance gain even when using a heavier transformer-based backbone. Table 6 illustrates a comparison of COCO detection performance between Swin-T and Swin-B backbone under the varied sparsity. Due to the increased capacity, using Swin-B backbone significantly boosts up the baseline AP up to 52.5 $( + 4 . 5 ) $ but with $2 . 4 \times$ parameters and $2 . 1 \times$ com

![](Images_J728TKKV/50d21d233c5c397cd03c8dec750c35d8a8c1394d4585babe06ae4a870f271084.jpg)  
Figure 10: Distribution of the ratio of non-zero values of DAM on COCO 2017 val set.

Table 7: Two-stage encoder token sparsification with a varied keeping ratio. COCO detection performance when the encoder tokens are sparsified at the later stage with the top- ${ \cdot \rho \% }$ binarized DAMs pre-computed from the former stage. All models are Deformable-DETR $^ +$ with Swin-T backbone and the encoder auxiliary loss is not applied. Note that the performance of the $50 \%$ model(47.9 AP) hardly degenerates compared to the baseline(48.0 AP).

<table><tr><td>Keeping ratio (p)</td><td>AP AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td>100%</td><td>48.0 68.0</td><td>52.0</td><td>30.3</td><td>51.4</td><td>63.7</td></tr><tr><td>10%</td><td>44.0 66.0</td><td>47.2</td><td>26.9</td><td>46.8</td><td>61.0</td></tr><tr><td>20%</td><td>44.9 66.3</td><td>48.3</td><td>28.2</td><td>48.2</td><td>61.4</td></tr><tr><td>30%</td><td>46.5 67.3</td><td>50.2</td><td>30.9</td><td>49.7</td><td>62.3</td></tr><tr><td>40%</td><td>47.3 67.9</td><td>51.3</td><td>30.7</td><td>50.7</td><td>63.4</td></tr><tr><td>50%</td><td>47.9 67.8</td><td>52.0</td><td>29.8</td><td>51.4</td><td>63.7</td></tr></table>

putational cost. With the keeping ratio of $40 \%$ and the encoder auxiliary loss, the performance gap remains at a similar leve $( + 4 . 1 )$ . We can also observe consistent performance gains as the keeping ratio gets higher while the increasing gap converges more quickly than Swin-T. It may be because a single visual token of Swin-B can incorporate a wider range of information due to the deeper attention hierarchies and a smaller number of tokens is required to fully represent all the objects in an image. Note that the efficiency of a backbone network is behind the scope of this paper. Our work is orthogonal to the backbone sparsification approaches, e.g. DynamicViT (Rao et al., 2021), and we leave the integration with those works as future work.

A.9 THE PRELIMINARY EXPERIMENTS: WHY PURSUE A SPARSE ENCODER?

Using a model trained with Deformable-DETR, we have analyzed the number of encoder output tokens referenced by the decoder’s object query. Unlike using the bilinear interpolation described in the appendix A.2 to generate DAMs for training with pseudo-labels, in this analysis, we do not use bilinear interpolation to calculate how many encoder tokens are directly referenced by the decoder object query. To analyze the non-zero values of DAM, we use a Deforamble DETR model trained with Top- $k$ sampling strategy (Yao et al., 2021) and bounding box refinement (Zhu et al., 2021) using ResNet-50 backbone. Fig. 10 illustrates the distribution of the ratio of non-zero values of DAM on COCO val2017 dataset. As shown in Fig. 10, on average, only $45 \%$ of encoder tokens were referenced by object queries.

This observation naturally raises a question: Can we preserve the detection performance even if we focus, in the first place, only on the encoder tokens that the decoder might have preferred? As a preliminary experiment to answer this question, we trained the detector restricting token updates to the subset to which the decoder could have referred if there had been no such restriction. To this end, we performed the two-stage learning as follows: (i) We first obtained the DAMs of the entire training data by feeding them to a fully-trained Deformable-DETR model. (ii) Then, we retrained another model from the scratch by updating only a subset of tokens determined by the binarized DAM preserving top- $\rho \%$ of the elements(refer to Section3.3 for more details). Table 7 shows the performance on COCO detection for different keeping ratio $\rho$ . We found that the two-stage model almost catches up with the baseline $\rho = 1 0 0 \%$ as the keeping ratio is raised close to $45 \%$ , namely the percentage of non-zero values in DAM computed on the validation dataset earlier.

These observations have strongly motivated us to develop the encoder token sparsification method presented in the main text. Note that our main algorithm differs from this preliminary experiment in some aspects: (a) A DAM is obtained from the jointly learning decoder, not from the separately trained decoder, and (b) a binarized DAM is utilized as a prediction target of the scoring network rather than used directly as a sparsification mask.

# A.10 VISUALIZATIONS OF SELECTED ENCODER TOKENS

We visualize selected encoder tokens and top- $k$ decoder queries for each criterion, OS, and DAM. In the first row, selected encoder tokens from the backbone feature map are visualized as yellow regions, whereas unselected tokens are visualized as purple regions. In the second row, selected top$k$ decoder queries from encoder output are visualized in the same manner. In the final row, DAM values and Corr metrics are visualized. Corr is measured as in Section 4.2.

Interestingly, the DAM-based selection seems to better capture the objects than OS-based selection. The OS-based selection also captures objects well, but it typically focuses on the high-frequency edges that are not only in the foreground but also in the background. On the other hand, the DAMbased selection captures the boundary of the objects and also their inner areas and is less distracted from the background edges. We analyze that DAM focuses on the boundary of objects to lower the regression loss, and attends to the inside of objects to lower the classification loss. Finally, the scoring network predicts such a DAM well, and refining the encoder tokens according to it finally helps achieve better detection performance.

![](Images_J728TKKV/db3fcb083a6db7fa55e575f3a6008d1eee51e7ce13c52eaada6c78de054e68d5.jpg)  
Figure 11: Visualization of selected tokens and DAM for COCO validation image #289960

![](Images_J728TKKV/74212157348619d917dc1803253b5fa5e8344e6ea8b6369fff21f3a7afaaf2e3.jpg)  
Figure 12: Visualization of selected tokens and DAM for COCO validation image #22396

![](Images_J728TKKV/61855b25d5b49f547f146ad6aafa6b746f66abf0f62830202ddccd8738b26e9b.jpg)  
Figure 13: Visualization of selected tokens and DAM for COCO validation image #46252

![](Images_J728TKKV/2c815809cb5bb6a802985f4f6667578f1f4dbe9282e6d9e0b1e2cdfb7704160e.jpg)  
Figure 14: Visualization of selected tokens and DAM for COCO validation image #6040

![](Images_J728TKKV/fbad7fa1312c01c90ebb55120ad992f755d97538c6d69f9c1980cc4dcad5503b.jpg)  
Figure 15: Visualization of selected tokens and DAM for COCO validation image #17379