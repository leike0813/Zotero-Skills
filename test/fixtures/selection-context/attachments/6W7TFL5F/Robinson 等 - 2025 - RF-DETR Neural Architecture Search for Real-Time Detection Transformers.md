# RF-DETR: NEURAL ARCHITECTURE SEARCH FOR REAL-TIME DETECTION TRANSFORMERS

Isaac Robinson1, Peter Robicheaux1, Matvei Popov1, Deva Ramanan2, Neehar Peri2 1Roboflow, 2Carnegie Mellon University

# ABSTRACT

Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavyweight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the “tunable knobs” for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by $5 . 3 \ \mathrm { A P }$ at similar latency, and RF-DETR $2 \mathbf { x }$ -large) outperforms GroundingDINO (tiny) by $1 . 2 ~ \mathrm { A P }$ on Roboflow100-VL while running $2 0 \times$ as fast. To the best of our knowledge, RF-DETR $2 \mathbf { x } .$ -large) is the first real-time detector to surpass 60 AP on COCO. Our code is available on GitHub.

# 1 INTRODUCTION

Object detection is a fundamental problem in computer vision that has matured in recent years (Felzenszwalb et al., 2009; Lin et al., 2014; Ren et al., 2015). Open-vocabulary detectors like GroundingDINO (Liu et al., 2023) and YOLO-World (Cheng et al., 2024) achieve remarkable zero-shot performance on common categories like car, truck, and pedestrian. However, state-of-the-art vision-language models (VLMs) still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training (Robicheaux et al., 2025). Fine-tuning VLMs on a target dataset significantly improves in-domain performance at the cost of runtime efficiency (due to heavy-weight text encoders) and open-vocabulary generalization. In contrast, specialist (i.e., closed-vocabulary) object detectors like D-FINE (Peng et al., 2024) and RT-DETR (Zhao et al., 2024) achieve real-time inference, but underperform fined-tuned VLMs like GroundingDINO. In this paper, we modernize specialist detectors by combining internet-scale pretraining with real-time architectures to achieve state-of-the-art performance and fast inference.

Are Specialist Detectors Over-Optimized for COCO? Sustained progress in object detection can be largely attributed to standardized benchmarks like PASCAL VOC (Everingham et al., 2015) and COCO (Lin et al., 2014). However, we find that recent specialist detectors implicitly overfit to COCO at the cost of real-world performance using bespoke model architectures, learning rate schedulers, and augmentation schedulers. Notably, state-of-the-art object detectors like YOLOv8 (Jocher et al., 2023) generalize poorly to real-world datasets with significantly different data distributions from COCO (e.g., number of objects per image, number of classes, and dataset size). To address these limitations, we present RF-DETR, a scheduler-free approach that leverages internetscale pre-training to generalize to real-world data distributions. To better specialize our model for diverse hardware platforms and dataset characteristics, we revisit neural architecture search (NAS) in the context of end-to-end object detection and segmentation.

Rethinking Neural Architecture Search (NAS) for DETRs. NAS discovers accuracy-latency tradeoffs by exploring architectural variants within a pre-defined search space. NAS has been previously studied in the context of image classification (Tan & Le, 2019; Cai et al., 2019) and for model sub-components like detector backbones Tan et al. (2020) and FPNs Ghiasi et al. (2019). Unlike prior work, we explore end-to-end weight-sharing NAS for object detection and segmentation. Our key insight, inspired by OFA (Cai et al., 2019), is that we can vary model inputs like image resolution, and architectural components like patch size during training. Further, weight-sharing NAS allows us to modify inference configurations like the number of decoder layers and query tokens to specialize our strong base model without fine-tuning. We evaluate all model configurations with grid search on a validation set. Importantly, our approach does not evaluate the search space until the base model has been fully-trained on the target dataset. As a result, all possible sub-nets (i.e., model configurations within the search space) achieve strong performance without further finetuning, significantly reducing the computational cost of optimizing for new hardware. Interestingly, we find that sub-nets not explicitly seen during training still achieve high performance, suggesting that RF-DETR can generalize to unseen architectures (cf. H). Extending RF-DETR for segmentation is also relatively straightforward and only requires adding a lightweight instance segmentation head. We denote this model as RF-DETR-Seg. Notably, this allows us to also leverage end-to-end weight-sharing NAS to discover Pareto optimal architectures for real-time instance segmentation.

Standardizing Latency Evaluation. We evaluate our approach on COCO (Lin et al., 2014) and Roboflow100-VL (RF100-VL) (Robicheaux et al., 2025) and achieve state-of-the-art performance among real-time detectors. RF-DETR (nano) outperforms D-FINE (nano) by $5 \%$ AP on COCO at comparable run-times, and RF-DETR (2x-lage) beats GroundingDINO (tiny) on RF100-VL at a fraction of the runtime. RF-DETR-Seg (nano) outperforms YOLOv11-Seg ( $\mathbf { \dot { X } }$ -large) on COCO while running $4 \times$ as fast. However, comparing RF-DETR’s latency with prior work remains challenging because reported latency evaluation varies significantly between papers. Notably, each new model re-benchmarks the latency of prior work for fair comparison on their hardware. For example, DFINE’s reported latency evaluation of LW-DETR (Chen et al., 2024a) is $2 5 \%$ faster than originally reported. We identify that this lack of reproducibility can be primarily attributed to GPU power throttling during inference. We find that buffering between forward passes limits power over-draw and standardizes latency evaluation (cf. Table 1).

Contributions. We present three major contributions. First, we introduce RF-DETR, a family of scheduler-free NAS-based detection and segmentation models that outperform prior state-of-the-art on RF100-VL (Robicheaux et al., 2025) and real-time methods with latencies $\leq 4 0$ ms on COCO (Lin et al., 2014)(cf. Fig. 1). To the best of our knowledge, RF-DETR is the first real-time detector to exceed $6 0 \ \mathrm { m A P }$ on COCO. Next, we explore the “tunable-knobs” for weight-sharing NAS to improve accuracy-latency tradeoffs for end-to-end object detection (cf. Fig. 3). Notably, our use of a weight-sharing NAS allows us to leverage large-scale pre-training and effectively transfer to small datasets (cf. Tab. 4). Lastly, we revisit current benchmarking protocols for measuring latency and propose a simple standardized procedure to improve reproducibility.

# 2 RELATED WORKS

Neural Architecture Search (NAS) automatically identifies families of model architectures with different accuracy-latency tradeoffs (Zoph & Le, 2016; Zoph et al., 2018; Real et al., 2019; Cai et al., 2018a). Early NAS approaches (Zoph & Le, 2016; Real et al., 2019) focused primarily on maximizing accuracy, with little consideration for efficiency. As a result, discovered architectures (e.g., NASNet and AmoebaNet) were often computationally expensive. More recent hardware-aware NAS methods (Cai et al., 2018b; Tan et al., 2019; Wu et al., 2019) address this limitation by incorporating hardware feedback directly into the search process. However, these methods must repeat the search and training process for each new hardware platform. In contrast, OFA (Cai et al., 2019) proposes a weight-sharing NAS that decouples training and search by simultaneously optimizing thousands of sub-nets with different accuracy-latency tradeoffs. Contemporary methods typically evaluate NAS for object detection by simply replacing standard backbones with NAS backbones in existing detection frameworks. Unlike prior work, we directly optimize end-to-end object detection accuracy to find Pareto optimal accuracy-latency tradeoffs for any target dataset.

Real-Time Object Detectors are of significant interest for safety-critical and interactive applications. Historically, two-stage detectors like Mask-RCNN (He et al., 2017) and Hybrid Task Cascade (Chen et al., 2019) achieved state-of-the-art performance at the cost of latency, while single-stage detectors like YOLO (Redmon et al., 2016) and SSD (Liu et al., 2016) traded accuracy for stateof-the-art runtime. However, modern detectors (Zhao et al., 2024) reexamine this accuracy-latency tradeoff, simultaneously improving on both axes. Recent YOLO variants innovate on architecture, data augmentation, and training techniques (Redmon et al., 2016; Wang et al., 2023; 2024; Jocher et al., 2023; 2024) to improve performance while maintaining fast inference. Despite their efficiency, most YOLO models rely on non-maximum suppression (NMS), which introduces additional latency. In contrast, DETR (Carion et al., 2020) removes hand-crafted components like NMS and anchor boxes. However, early DETR variants (Zhu et al., 2020; Zhang et al., 2022a; Meng et al., 2021; Liu et al., 2022) achieved strong accuracy at the cost of runtime, limiting their use in realtime applications. Recent works such as RT-DETR (Zhao et al., 2024) and LW-DETR (Chen et al., 2024a) have successfully adapted high performance DETRs for real-time applications.

![](Images_FL7EZ6Y8/ae1369740400434828ff754227a3a44a274c010374ac5ff8ccdead3cc7a8e59e.jpg)  
Figure 1: Accuracy-Latency Pareto Curve. We plot the Pareto accuracy-latency frontier for realtime detectors on the COCO detection val-set (top left, bottom left), COCO segmentation val-set (top right), and RF100-VL test-set (bottom right). Since RF100-VL contains 100 distinct datasets, we select target latencies for the N, S, M, L, XL, 2XL configurations, search for RF-DETR models with latencies within $10 \%$ of the target and report their average performance after fine-tuning to convergence. Importantly, all points along RF-DETR’s continuous Pareto curves for COCO are derived from a single training run.

Vision-Language Models are trained on large-scale, weakly supervised image-text pairs from the web. Such internet-scale pre-training is a key enabler for open-vocabulary object detection (Liu et al., 2023; Cheng et al., 2024). GLIP (Li et al., 2022) frames detection as phrase grounding with a single text query, while Detic (Zhou et al., 2022) boosts long-tail detection using ImageNet-level supervision (Russakovsky et al., 2015). MQ-Det (Xu et al., 2024) extends GLIP with a learnable module that enables multi-modal prompting. Recent VLMs demonstrate strong zero-shot performance and are often applied as black-box models in diverse downstream tasks (Ma et al., 2023; Peri et al., 2023; Khurana et al., 2024; Osep et al., 2024; Takmaz et al., 2025). However, Robicheaux et al. (2025) find that such models perform poorly when evaluated on categories not typically found in their pre-training, requiring further fine-tuning. In addition, many vision-language models are prohibitively slow, making them difficult to use for real-time tasks. In contrast, RF-DETR combines the fast inference of real-time detectors with the internet-scale priors of VLMs to achieve state-ofthe-art performance on RF100-VL and at all latencies $\leq 4 0$ ms on COCO.

![](Images_FL7EZ6Y8/c56cf4e863a9fda1d8c0c51260c948f845a9e6ef4395c69c024d3e927e284824.jpg)  
Figure 2: RF-DETR Architecture. RF-DETR uses a pre-trained ViT backbone to extract multiscale features of the input image. We interleave windowed and non-windowed attention blocks to balance accuracy and latency. Notably, the deformable cross-attention layer and segmentation head both bilinearly interpolate the the output of the projector, allowing for consistent spatial organization of features. Lastly, we apply detection and segmentation losses at all decoder layers to facilitate decoder drop out at inference.

# 3 RF-DETR: WEIGHT-SHARING NAS WITH FOUNDATION MODELS

In this section, we describe the architecture of our base model (cf. Fig. 2) and present the “tunable knobs” of our weight-sharing NAS (cf. Fig. 3). Further, we highlight the limitations of handdesigned learning-rate and augmentation schedulers, and advocate for a scheduler-free approach.

Incorporating Internet-Scale Priors. RF-DETR modernizes LW-DETR (Chen et al., 2024a) by simplifying its architecture and training procedure to improve generalization to diverse target domains. First, we replace LW-DETR’s CAEv2 (Zhang et al., 2022b) backbone with DINOv2 (Oquab et al., 2023). We find that initializing our backbone with DINOv2’s pre-trained weights significantly improves detection accuracy on small datasets. Notably, CAEv2’s encoder has 10 layers with a patch size of 16, while DINOv2’s encoder has 12 layers. Our DINOv2 backbone has more layers and is slower than CAEv2, but we make up for this latency using NAS (discussed next). Lastly, we facilitate training on consumer-grade GPUs via gradient accumulation by using layer norm instead of batch norm in the multi-scale projector.

Real-Time Instance Segmentation. Inspired by Li et al. (2023), we add a lightweight instance segmentation head to jointly predict high quality segmentation masks. Our segmentation head bilinearly interpolates the output of the encoder and learns a lightweight projector to generate a pixel embedding map. Specifically, we upsample the same low-resolution feature map for the detection and segmentation heads to ensure that it contains relevant spatial information. Unlike MaskDINO (Li et al., 2023), we do not incorporate multi-scale backbone features in our segmentation head to minimize latency. Lastly, we compute the dot product of all projected query token embeddings (at the output of each decoder layer transformed by a FFN) with the pixel embedding map to generate segmentation masks. Interestingly, we can interpret these pixel embeddings as segmentation prototypes Bolya et al. (2019). Motivated by LW-DETR’s observation that pre-training improves DETRs, we pre-train RF-DETR-Seg on Objects-365 (Shao et al., 2019) psuedo-labeled with SAM2 (Ravi et al., 2024) instance masks.

![](Images_FL7EZ6Y8/de8354f349eee80ffe272e7b8194181f7e5c30013cb66f0fa5a4acc166e2f3af.jpg)  
Figure 3: NAS Search Space. We vary (a) patch size, (b) number of decoder layers, (c) number of queries, (d) image resolution, and (e) number of windows per attention block in our weightsharing NAS. In addition to training thousands of network configurations in parallel, we find that this “architecture augmentation” serves as a regularizer and improves generalization.

End-to-End Neural Architecture Search. Our weight-sharing NAS evaluates thousands of model configurations with different input image resolutions, patch sizes, window attention blocks, decoder layers, and query tokens. At every training iteration, we uniformly sample a random model configuration and perform a gradient update. This allows our model to efficiently train thousands of sub-nets in parallel, similar to ensemble learning with dropout (Srivastava et al., 2014). We find that this weight-sharing NAS approach also serves as a regularizer during training, effectively performing “architecture augmentation”. To the best of our knowledge, RF-DETR is the first end-to-end weight-sharing NAS applied to object detection and segmentation. We describe each component below.

• Patch Size. Smaller patches lead to higher accuracy at greater computational cost. We adopt a FlexiVIT-style (Beyer et al., 2023) transformation to interpolate between patch sizes during training.   
• Number of Decoder Layers. Similar to recent DETRs (Peng et al., 2024; Zhao et al., 2024), we apply a regression loss to the output of all decoder layers during training. Therefore, we can drop any (or all) decoder blocks during inference. Interestingly, removing the entire decoder during inference effectively turns RF-DETR into a single-stage detector. Notably, truncating the decoder also shrinks the size of the segmentation branch, allowing for greater control over segmentation latency.   
• Number of Query Tokens. Query tokens learn spatial priors for bounding box regression and segmentation. We drop query tokens (ordered by the maximum sigmoid of the corresponding class logit per token at the output of the encoder, see appendix B) at test time to vary the maximum number of detections and reduce inference latency. The Pareto optimal number of query tokens implicitly encodes dataset statistics about the average number of objects per image in a target dataset.

• Image Resolution. Higher resolution improves small object detection performance, while lower resolution improves runtime. We pre-allocate $N$ positional embeddings corresponding to the largest image resolution divided by the smallest patch size and interpolate these embeddings for smaller resolutions or larger patch sizes.

• Number of Windows per Windowed Attention Block. Window attention restricts selfattention to only process a fixed number of neighboring tokens. We can add or remove windows per block to balance accuracy, global information mixing, and computational efficiency.

At inference time, we pick a specific model configuration to select an operating point on the accuracy-latency Pareto curve. Importantly, different model configurations may have similar parameter counts but significantly different latencies. Similar to Cai et al. (2019), we see little benefit from fine-tuning the NAS-mined models on COCO (Appendix F), but note modest improvements from fine-tuning NAS-mined models on RF100-VL. We posit that RF-DETR on RF100-VL benefits from additional fine-tuning because the “architecture augmentation” regularization requires more than 100 epochs to converge on small datasets. Notably, prior weight-sharing NAS methods (Cai et al., 2019) train in stages and use a different learning-rate scheduler per-stage. However, such schedulers make strict assumptions about model convergence, which may not hold across diverse datasets.

Training Schedulers and Augmentations Bias Model Performance. State-of-the-art detectors often require careful hyper-parameter tuning to maximize performance on standard benchmarks. However, such bespoke training procedures implicitly bias the model towards certain dataset characteristics (e.g. number of images). Concurrent with DINOv3 (Simeoni et al., 2025), we ob- ´ serve that cosine schedules assume a known (fixed) optimization horizon, which is impractical for diverse target datasets like those in RF100-VL. Data augmentations introduce similar biases by presuming prior knowledge of dataset properties. For example, prior work leverages aggressive data augmentation (e.g., VerticalFlip, RandomFlip, RandomResize, RandomCrop, YOLOXHSVRandomAug, and CachedMixUp) to increase effective dataset size. However, certain augmentations like VerticalFlip may negatively bias model predictions in safety-critical domains. For example, a person detector in a self-driving vehicle should not be trained with VerticalFlip to avoid false positive detections from reflections in puddles. Therefore, we limit augmentations to horizontal flips and random crops. Lastly, LW-DETR applies a per-image random resize augmentation, where each image is padded to match the largest image in the batch. As a result, most images have significant padding, which introduces window artifacts, and wastes computation on padded regions. In contrast, we resize images at the batch level to minimize the number of padded pixels per-batch and to ensure that all positional encoding resolutions are equally likely to be seen at train time.

# 4 EXPERIMENTS

We evaluate RF-DETR on COCO and RF100-VL and demonstrate that our approach achieves stateof-the-art accuracy among all real-time methods. In addition, we identify inconsistencies in standard benchmarking protocols and present a simple standardized procedure to improve reproducibility. Following LW-DETR (Chen et al., 2024a), we group models of similar latency into the same size bucket rather than grouping based on parameter count.

Datasets and Metrics. We evaluate RF-DETR on COCO for fair comparison with prior work and on RF100-VL to evaluate generalization to real-world datasets with significantly different data distributions. Due to the diversity of RF100-VL’s 100 datasets, we posit that overall performance on this benchmark is a proxy for transferability to any target domain. We use pycocotools to report standard metrics like mean average precision (mAP) and provide breakdown analysis for $\mathsf { A P } _ { 5 0 }$ , $\mathsf { A P } _ { 7 5 }$ , $\mathsf { A P } _ { S m a l l }$ , $\mathbf { A P } _ { M e d i u m }$ , and $\mathsf { A P } _ { L a r g e }$ . Further, we evaluate efficiency by measuring GFLOPs, number of parameters, and inference latency on an NVIDIA T4 GPU with Tensor-RT 10.4 and CUDA 12.4.

Standardizing Latency Benchmarking. Despite its maturity, benchmarking object detectors remains inconsistent across prior work. For example, YOLO-based models often omit non-maximal suppression (NMS) when computing latency, leading to unfair comparisons with end-to-end detec

Table 1: Standardizing Latency Evaluation. Variance in latency measurements can be largely attributed to power throttling and GPU overheating. We mitigate this issue by buffering for $2 0 0 \mathrm { m s }$ between forward passes. Notably, this benchmarking approach is not designed to measure sustained throughput, but rather ensures reproducible latency measurements. We are unable to reproduce YOLOv8 and YOLOv11’s mAP results in TensorRT, likely because these models evaluate with multi-class NMS but only use single-class NMS in inference. We use the standard NMS-tuned confidence threshold of 0.01. YOLOv8 and YOLOv11 performance degrades further when quantizied from FP32 to FP16, reaffirming that all models should report latency and accuracy using the same model artifact. Notably, naively quantizing D-FINE to FP16 reduces performance to 0.5 AP. We fix this issue by changing the authors’ export code to use ONNX opset 17. See Appendix A for more details.

<table><tr><td rowspan=1 colspan=1>Method</td><td rowspan=1 colspan=2>ReportedAP50:95     Latency (ms)</td><td rowspan=1 colspan=1>AP50:95</td><td rowspan=1 colspan=1>Buffering (FP-32)Latency (ms)</td><td rowspan=1 colspan=1>AP50:95</td><td rowspan=1 colspan=1>Buffering (FP-16)Latency (ms)</td></tr><tr><td rowspan=1 colspan=1>YOLOv8 (M)</td><td rowspan=1 colspan=1>50.2</td><td rowspan=1 colspan=1>5.86</td><td rowspan=1 colspan=1>49.3</td><td rowspan=1 colspan=1>14.8</td><td rowspan=1 colspan=1>47.3</td><td rowspan=1 colspan=1>5.4</td></tr><tr><td rowspan=1 colspan=1>Y0LOv11(M)</td><td rowspan=1 colspan=1>51.5</td><td rowspan=1 colspan=1>4.7</td><td rowspan=1 colspan=1>49.7</td><td rowspan=1 colspan=1>18.7</td><td rowspan=1 colspan=1>48.3</td><td rowspan=1 colspan=1>5.2</td></tr><tr><td rowspan=1 colspan=1>RT-DETR (R18)</td><td rowspan=1 colspan=1>49.0</td><td rowspan=1 colspan=1>4.61</td><td rowspan=1 colspan=1>49.0</td><td rowspan=1 colspan=1>12.2</td><td rowspan=1 colspan=1>49.0</td><td rowspan=1 colspan=1>4.4</td></tr><tr><td rowspan=1 colspan=1>LW-DETR(M)</td><td rowspan=1 colspan=1>52.5</td><td rowspan=1 colspan=1>5.6</td><td rowspan=1 colspan=1>52.6</td><td rowspan=1 colspan=1>26.8</td><td rowspan=1 colspan=1>52.6</td><td rowspan=1 colspan=1>4.4</td></tr><tr><td rowspan=1 colspan=1>D-FINE(M)</td><td rowspan=1 colspan=1>55.1</td><td rowspan=1 colspan=1>5.62</td><td rowspan=1 colspan=1>55.1</td><td rowspan=1 colspan=1>13.9</td><td rowspan=1 colspan=1>55.0(0.5*)</td><td rowspan=1 colspan=1>5.4</td></tr><tr><td rowspan=1 colspan=1>RF-DETR(M)</td><td rowspan=1 colspan=1>-</td><td rowspan=1 colspan=1>-</td><td rowspan=1 colspan=1>54.8</td><td rowspan=1 colspan=1>20.5</td><td rowspan=1 colspan=1>54.7</td><td rowspan=1 colspan=1>4.4</td></tr></table>

Table 2: COCO Detection Evaluation. We compare RF-DETR with popular real-time and openvocabulary object detectors below. We find that RF-DETR (nano) outperforms D-FINE (nano) and LW-DETR (tiny) by more than 5 AP. RF-DETR significantly outperforms YOLOv8 and YOLOv11, while RF-DETR’s nano size achieves performance parity with YOLOv8 and YOLOv11’s medium size model. We denote models that do not support TensorRT execution with a star, and instead report PyTorch latency results. See Appendix E for L, XL, and Max variants of RF-DETR on COCO.   

<table><tr><td>Model</td><td>Size</td><td>#Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75</td><td> APs</td><td>APM</td><td>APL</td></tr><tr><td colspan="7">Real-TimeObjectDetectionw/NMS</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>YOLOv8(Jocher et al.,2023)</td><td></td><td>N</td><td>3.2M</td><td>8.7</td><td>2.1</td><td>35.2</td><td>49.2</td><td>38.3</td><td>15.8</td><td>38.8</td><td>51.3</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td></td><td>N</td><td>2.6M</td><td>6.5</td><td>2.2</td><td>37.1</td><td>51.6</td><td>40.4</td><td>17.3</td><td>40.7</td><td>55.6</td></tr><tr><td>YOLOv8 (Jocher et al.,2023)</td><td></td><td>S</td><td>11.2M</td><td>28.6</td><td>2.9</td><td>42.4</td><td>57.6</td><td>46.0</td><td>22.2</td><td>47.1</td><td>59.6</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td></td><td>S</td><td>9.4M</td><td>21.5</td><td>3.2</td><td>44.1</td><td>59.3</td><td>47.9</td><td>26.1</td><td>48.5</td><td>62.6</td></tr><tr><td></td><td>YOLOv8 (Jocher etal.,2023)</td><td>M</td><td>25.9M</td><td>78.9</td><td>5.4</td><td>47.3</td><td>62.5</td><td>51.5</td><td>27.5</td><td>52.9</td><td>65.1</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td></td><td>M</td><td>20.1M</td><td>68.0</td><td>5.1</td><td>48.3</td><td>63.6</td><td>52.5</td><td>29.1</td><td>53.8</td><td>66.3</td></tr><tr><td colspan="10">Open-Vocabulary Object Detection (Fully-Supervised Fine-Tuning)</td></tr><tr><td>GroundingDINO (Liu et al.,2023)</td><td></td><td>T</td><td>173.0M</td><td>1008.3</td><td>427.6*</td><td>58.2</td><td>=</td><td>=</td><td>=</td><td>-</td><td>-</td></tr><tr><td></td><td>End-to-End Real-Time Object Detection</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LW-DETR(Chen et al.,2024a)</td><td></td><td>T</td><td>12.1M</td><td>21.4</td><td>1.9</td><td>42.9</td><td>60.7</td><td>45.9</td><td>22.7</td><td>47.3</td><td>60.0</td></tr><tr><td></td><td>D-FINE(Peng etal.,2024)</td><td>N</td><td>3.8M</td><td>7.3</td><td>2.1</td><td>42.7</td><td>60.2</td><td>45.4</td><td>22.9</td><td>46.6</td><td>62.1</td></tr><tr><td>RF-DETR(Ours)</td><td></td><td>N</td><td>30.5M</td><td>31.9</td><td>2.3</td><td>48.0</td><td>67.0</td><td>51.4</td><td>25.2</td><td>53.5</td><td>70.0</td></tr><tr><td>LW-DETR(Chen et al.,2024a)</td><td></td><td>S</td><td>14.6M</td><td>31.8</td><td>2.6</td><td>48.0</td><td>66.8</td><td>51.6</td><td>26.7</td><td>52.5</td><td>65.6</td></tr><tr><td>D-FINE (Peng et al.,2024)</td><td></td><td>S</td><td>10.2M</td><td>25.2</td><td>3.5</td><td>50.6</td><td>67.6</td><td>55.0</td><td>32.6</td><td>54.6</td><td>66.6</td></tr><tr><td>RF-DETR(Ours)</td><td></td><td>S</td><td>32.1M</td><td>59.8</td><td>3.5</td><td>52.9</td><td>71.9</td><td>57.0</td><td>32.0</td><td>58.3</td><td>73.0</td></tr><tr><td>RT-DETR(Zhao et al.,2024)</td><td></td><td>R18</td><td>36.0M</td><td>100.0</td><td>4.4</td><td>49.0</td><td>66.6</td><td>53.3</td><td>32.8</td><td>52.1</td><td>65.0</td></tr><tr><td>LW-DETR(Chen etal.,2024a)</td><td>M</td><td></td><td>28.2M</td><td>83.9</td><td>4.4</td><td>52.6</td><td>72.0</td><td>56.6</td><td>32.5</td><td>57.6</td><td>70.5</td></tr><tr><td>D-FINE (Peng et al.,2024)</td><td></td><td>M</td><td>19.2M</td><td>56.6</td><td>5.4</td><td>55.0</td><td>72.6</td><td>59.7</td><td>37.6</td><td>59.4</td><td>71.7</td></tr><tr><td>RF-DETR(Ours)</td><td></td><td>M</td><td>33.7M</td><td>78.8</td><td>4.4</td><td>54.7</td><td>73.5</td><td>59.2</td><td>36.1</td><td>59.7</td><td>73.8</td></tr><tr><td>RF-DETR(Ours)</td><td></td><td>2XL</td><td>126.9M</td><td>438.4</td><td>17.2</td><td>60.1</td><td>78.5</td><td>65.5</td><td>43.2</td><td>64.9</td><td>76.2</td></tr></table>

tors. Additionally, YOLO-based segmentation models measure the latency of generating prototype predictions instead of directly usable per-object masks (Jocher et al., 2024), leading to biased runtime measurements. Further, D-FINE’s reported latency evaluation of LW-DETR is $2 5 \%$ faster than reported by Chen et al. (2024b). We observe that such differences can be attributed to detectable power throttling events, particularly when the GPU overheats (cf. Table 1). In contrast, simply pausing for 200ms between consecutive forward passes largely mitigates power throttling, yielding more stable latency measurements. Lastly, we find that prior work often reports latency using FP16 quantized models, but evaluates accuracy with FP32 models. However, naive quantization can significantly degrade performance (in some cases dropping performance to near 0 AP). To ensure fair comparison, we advocate reporting accuracy and latency with the same model artifact. We release our stand-alone benchmarking tool on GitHub.

Evaluating RF-DETR and RF-DETR-Seg on COCO. COCO (Lin et al., 2014) is a flagship benchmark for object detection and instance segmentation. In Table 2, we compare RF-DETR with leading real-time and open-vocabulary detectors. RF-DETR (nano) beats both D-FINE (nano) and

Table 3: COCO Instance Segmentation Evaluation. We compare RF-DETR with popular realtime instance segmentation methods on COCO. Notably, RF-DETR (nano) outperforms all reported YOLOv8 and YOLOv11 model sizes. Further RF-DETR (nano) outperforms FastInst by $4 . 4 \%$ , while running nearly ten times faster. RF-DETR(medium) approaches the performance on MaskDINO at a fraction of the runtime. We denote models that do not support TensorRT execution with a star, and instead report PyTorch latency results. Our latencies for YOLOs also include the conversion of protos into masks, which are not typically included in prior benchmarks but nonetheless contribute meaningfully to practical latency. See Appendix E for L, XL, and Max variants of RF-DETR-Seg on COCO.

<table><tr><td>Model</td><td>Size</td><td>#Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td colspan="9">Real-Time Instance Segmentation w/ NMS</td></tr><tr><td>YOLOv8 (Jocher et al.,2023)</td><td>N</td><td>3.4M</td><td>12.6</td><td>3.5</td><td>28.3</td><td>45.6</td><td>29.8</td><td>9.3</td><td>31.3</td><td>44.3</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>N</td><td>2.9M</td><td>10.4</td><td>3.6</td><td>30.0</td><td>47.8</td><td>31.5</td><td>10.0</td><td>33.4</td><td>47.7</td></tr><tr><td>YOLOv8(Jocher etal.,2023)</td><td>S</td><td>11.8M</td><td>42.6</td><td>4.2</td><td>34.0</td><td>53.8</td><td>36.0</td><td>13.6</td><td>38.5</td><td>52.2</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>S</td><td>10.1M</td><td>35.5</td><td>4.6</td><td>35.0</td><td>55.4</td><td>37.1</td><td>15.3</td><td>39.7</td><td>53.9</td></tr><tr><td>YOLOv8 (Jocher etal.,2023)</td><td>M</td><td>27.3M</td><td>110.2</td><td>7.0</td><td>37.3</td><td>58.2</td><td>39.9</td><td>16.7</td><td>43.0</td><td>56.1</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>M</td><td>22.4M</td><td>123.3</td><td>6.9</td><td>38.5</td><td>60.0</td><td>40.9</td><td>18.0</td><td>44.3</td><td>57.6</td></tr><tr><td>End-to-End Instance Segmentation</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RF-DETR-Seg.(Ours)</td><td>N</td><td>33.6M</td><td>50.0</td><td>3.4</td><td>40.3</td><td>63.0</td><td>42.6</td><td>16.3</td><td>45.3</td><td>63.6</td></tr><tr><td>RF-DETR-Seg.(Ours)</td><td>S</td><td>33.7M</td><td>70.6</td><td>4.4</td><td>43.1</td><td>66.2</td><td>45.9</td><td>21.9</td><td>48.5</td><td>64.1</td></tr><tr><td>FastInst (He et al.,2023)</td><td>R50</td><td>29.7M</td><td>99.7</td><td>39.6*</td><td>34.9</td><td>56.0</td><td>36.2</td><td>13.3</td><td>38.0</td><td>56.8</td></tr><tr><td>MaskDINO (Li et al.,2023)</td><td>R50</td><td>52.1M</td><td>586</td><td>242*</td><td>46.3</td><td>69.0</td><td>50.7</td><td>26.1</td><td>49.3</td><td>66.1</td></tr><tr><td>RF-DETR-Seg.(Ours)</td><td>M</td><td>35.7M</td><td>102.0</td><td>5.9</td><td>45.3</td><td>68.4</td><td>48.8</td><td>25.5</td><td>50.4</td><td>65.3</td></tr><tr><td>RF-DETR(Ours)</td><td>2XL</td><td>38.6M</td><td>435.3</td><td>21.8</td><td>49.9</td><td>73.1</td><td>54.5</td><td>33.9</td><td>54.1</td><td>65.7</td></tr></table>

LW-DETR (nano) by more than 5 AP. We see similar trends for small and medium sizes as well. Notably, RF-DETR also significantly outperforms YOLOv8 and YOLOv11. RF-DETR (nano) matches the performance of YOLOv8 and YOLOv11 (medium). We use mmdetection’s implementation of GroundingDINO and include their reported AP since they do not release a model artifact for GroundingDINO fine-tuned on COCO. We benchmark mmGroundingDINO’s parameter count, GFLOPS, and latency using the released open-vocabulary model. In Table 3, we compare RF-DETR-Seg with real-time instance segmentation models. RF-DETR-Seg (nano) outperforms YOLOv8 and YOLOv11 at all sizes. Furthermore, RF-DETR-Seg (nano) beats FastInst by $5 . 4 \%$ while running almost ten times faster. Similarly, RF-DETR (x-large) surpasses GroundingDINO (tiny), and RFDETR-Seg (large) outperforms MaskDINO (R50), at a fraction of their runtime.

Evaluating RF-DETR on RF100-VL. RF100-VL is a challenging detection benchmark composed of 100 diverse datasets. We report latencies, FLOPs, and accuracy averaged over all 100 datasets in Table 4. Our results show that RF-DETR (2x-large) outperforms GroundingDINO and LLMDet while requiring only a fraction of their runtime. Interestingly, RT-DETR outperforms D-FINE (which is built on RT-DETR) at mAP50, indicating that D-FINE’s hyperparameters are potentially overoptimized for COCO. We note that RF-DETR benefits from scaling to larger backbone sizes (Appendix E). In contrast, YOLOv8 and YOLOv11 consistently underperform DETR-based detectors, and scaling these model families to larger sizes does not improve their performance on RF100-VL.

Impact of Neural Architecture Search. We ablate the impact of weight-sharing NAS in Table 3. We find that adopting a gentler set of hyperparameters compared to LW-DETR (e.g. larger batch size, lower learning rate, and replacing batch normalization with layer normalization) reduces performance over LW-DETR by $1 . 0 \%$ . Notably, replacing batch normalization with layer normalization hurts performance, but is necessary to train on consumer hardware. However, replacing LW-DETRs CAEv2 backbone with DINOv2 improves performance by $2 \%$ . The lower learning rate, in particular, helps preserve DINOv2’s pre-trained knowledge, while additional epochs of Objects-365 pre-training further compensate for the slower optimization. Our final model with weight-sharing NAS improves over LW-DETR by $2 \%$ without increasing latency.

Impact of Backbone Architecture and Pre-Training. We study the impact of different backbone architectures in RF-DETR. We find that DINOv2 achieves the best performance, outperforming CAEv2 by $2 \%$ . Interestingly, despite having fewer parameters than SigLIPv2, SAM2’s Hiera-S backbone is considerably slower. This is in contrast with the Hiera-S claim that it is meaningfully faster than equivalently performant ViTs. However, Hiera does not explore latency in the context of kernels such as Flash Attention, which are highly optimized in compilers such as TensorRT.

Table 4: RF100-VL Evaluation. We compare RF-DETR with real-time and open-vocabulary object detectors on RF100-VL. Interestingly, RF-DETR ( $2 \mathbf { x }$ -large) outperforms GroundingDINO (tiny), and LLMDet (tiny) at a fraction of their runtime. We report the average latency and FLOPs over all 100 datasets. We note that YOLOv8 and YOLOv11’s latency measurements may be suboptimal because the default tuned NMS threshold of 0.01 may not work well for all datasets in RF100-VL. We denote models that do not support TensorRT execution with a star, and instead report PyTorch latency results. See Appendix E for L, XL, and Max variants of RF-DETR on RF100-VL.   

<table><tr><td>Model</td><td>Size</td><td>#Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs APM</td><td>APL</td><td></td></tr><tr><td colspan="9">Real-Time Object Detectors w/NMS</td><td></td></tr><tr><td>YOLOv8 (Jocher et al.,2023)</td><td>N</td><td>3.2M</td><td>8.7</td><td>2.6</td><td>55.0</td><td>81.1</td><td>59.5</td><td>4.8</td><td>44.1</td><td>48.0</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td></td><td>N 2.6M</td><td>6.5</td><td>3.0</td><td>55.5</td><td>81.3</td><td>60.3</td><td>4.7</td><td>44.4</td><td>49.2</td></tr><tr><td>YOLOv8 (Jocher et al.,2023)</td><td>S</td><td>11.2M</td><td>28.6</td><td>3.1</td><td>56.3</td><td>82.0</td><td>60.9</td><td>6.1</td><td>45.6</td><td>48.6</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>S</td><td>9.4M</td><td>21.5</td><td>3.3</td><td>56.4</td><td>82.5</td><td>61.3</td><td>6.5</td><td>45.5</td><td>48.5</td></tr><tr><td>YOLOv8 (Jocher et al.,2023)</td><td>M</td><td>25.9M</td><td>78.9</td><td>5.4</td><td>56.5</td><td>82.3</td><td>60.9</td><td>6.4</td><td>45.7</td><td></td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>M</td><td>20.1M</td><td>68.0</td><td>5.1</td><td>57.0</td><td>82.5</td><td>61.9</td><td>7.3</td><td>46.1</td><td>48.6 48.6</td></tr><tr><td>Open-Vocabulary Object-Detectors (Fully-Supervised Fine-Tuning)</td><td colspan="8"></td><td></td></tr><tr><td>GroundingDINO (Liu et al.,2023)</td><td>T</td><td>173.0M</td><td>1008.3</td><td>309.9*</td><td>62.3</td><td>88.8</td><td>67.8</td><td>39.2</td><td></td><td>69.5</td></tr><tr><td>LLMDet (Fu et al.,2025)</td><td>T</td><td>173.0M</td><td>1008.3</td><td>308.4*</td><td>62.3</td><td>88.3</td><td>67.8</td><td>39.1</td><td>57.7</td><td>70.3</td></tr><tr><td>End-to-End Real-Time Object Detectors</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>57.6</td><td></td></tr><tr><td>LW-DETR(Chen et al.,2024a)</td><td>N</td><td>12.1M</td><td>21.4</td><td>1.9</td><td>57.1</td><td>84.7</td><td>61.5</td><td></td><td></td><td></td></tr><tr><td>D-FINE(Peng et al.,2024)</td><td></td><td>N</td><td>3.8M 7.3</td><td>2.0</td><td>58.2</td><td>84.4</td><td>62.5</td><td>31.2 32.4</td><td>51.8</td><td>65.8 65.8</td></tr><tr><td>RF-DETR(Ours)</td><td></td><td>N</td><td>31.2M 34.5</td><td>2.5</td><td>57.6</td><td>84.9</td><td>62.1</td><td>30.7</td><td>52.9 52.2</td><td>66.8</td></tr><tr><td></td><td>RF-DETRw/Fine-Tuning(Ours)</td><td>N</td><td>31.2M 34.5</td><td>2.5</td><td>58.7</td><td>85.6</td><td>63.5</td><td>32.4</td><td>52.7</td><td>67.0</td></tr><tr><td></td><td>LW-DETR(Chen et al.,2024a)</td><td>S 14.6M</td><td>31.8</td><td>2.6</td><td>57.4</td><td>85.0</td><td>62.0</td><td>32.1</td><td></td><td>65.8</td></tr><tr><td>D-FINE(Peng et al.,2024)</td><td>S</td><td>10.2M</td><td>25.2</td><td>3.5</td><td>60.3</td><td>85.3</td><td>65.4</td><td>36.6</td><td>52.1 56.0</td><td>68.4</td></tr><tr><td>RF-DETR(Ours)</td><td>S</td><td>33.5M</td><td>62.4</td><td>3.7</td><td>60.7</td><td>87.0</td><td>66.0</td><td>35.4</td><td>55.4</td><td>69.6</td></tr><tr><td>RF-DETRw/Fine-Tuning(Ours)</td><td>S</td><td>33.5M</td><td>62.4</td><td>3.7</td><td>61.0</td><td>87.2</td><td>66.4</td><td>35.3</td><td>55.9</td><td>69.8</td></tr><tr><td></td><td>RT-DETR(Zhao et al.,2024)</td><td>M 36.0M</td><td>100.0</td><td>4.3</td><td>59.6</td><td>85.7</td><td>64.6</td><td>36.4</td><td></td><td></td></tr><tr><td>LW-DETR(Chen et al.,2024a)</td><td></td><td>M 28.2M</td><td>83.9</td><td>4.3</td><td>59.8</td><td>86.8</td><td>64.9</td><td>34.0</td><td>54.6 54.4</td><td>67.3 68.9</td></tr><tr><td>D-FINE (Peng et al.,2024)</td><td>M</td><td>19.2M</td><td>56.6</td><td>5.6</td><td>60.6</td><td>85.5</td><td>65.8</td><td>36.0</td><td>56.6</td><td>67.5</td></tr><tr><td>RF-DETR(Ours)</td><td></td><td>M 33.5M</td><td>86.7</td><td>4.6</td><td>61.5</td><td>87.7</td><td>67.0</td><td>36.44</td><td>56.5</td><td>69.8</td></tr><tr><td></td><td>RF-DETRw/Fine-Tuning(Ours)</td><td>M 33.5M</td><td>86.7</td><td>4.6</td><td>61.9</td><td>87.9</td><td>67.3</td><td>36.4</td><td>56.6</td><td>70.1</td></tr><tr><td>RF-DETR(Ours)</td><td></td><td>2XL 123.5M</td><td>410.2</td><td>15.6</td><td>63.3</td><td>88.9</td><td>69.0</td><td>38.7</td><td>58.2</td><td>71.6</td></tr><tr><td></td><td>RF-DETR(Ours)w/Fine-Tuning</td><td>2XL</td><td>123.5M 410.2</td><td></td><td>15.6</td><td>63.5 89.0</td><td>69.2</td><td>38.9</td><td>58.3</td><td>71.7</td></tr></table>

Table 5: Ablation on Neural Architecture Search. We ablate the impact of each “tunable knob” on accuracy and latency below. Using a gentler set of hyperparameters compared to LW-DETR (e.g. smaller batch size, lower learning rate, replacing batch norm with layer norm) reduces performance by $1 \%$ . However, we regain this lost performance by replacing LW-DETR’s CAEV2 backbone with DINOv2. Importantly, the lower learning rate and layer-norm allow us to better preserve DINOv2’s foundational knowledge and allows us to train with larger batch sizes, making weight-sharing NAS more effective. Counterintuitively, introducing weight sharing NAS to the training scheme improves performance of the base configuration even though patch size 14 isn’t in the NAS search space.

<table><tr><td>Model</td><td># Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75 APs</td><td>APM</td><td>APL</td></tr><tr><td>LW-DETR(M)</td><td>28.2M</td><td>83.7</td><td>4.4</td><td>52.6</td><td>72.0</td><td>56.6 32.5</td><td>57.6</td><td>70.5</td></tr><tr><td>+Gentler Hyperparameters</td><td>28.2M</td><td>83.7</td><td>4.4</td><td>51.6</td><td>71.1</td><td>55.5 31.7</td><td>56.4</td><td>69.4</td></tr><tr><td>+ DINOv2 Backbone</td><td>32.3M</td><td>78.2</td><td>4.7</td><td>53.6</td><td>72.7</td><td>58.0 34.3</td><td>58.3</td><td>72.4</td></tr><tr><td>+ Additional O365Pre-Training</td><td>32.3M</td><td>78.2</td><td>4.7</td><td>54.3</td><td>73.4</td><td>58.8 35.8</td><td>59.2</td><td>72.3</td></tr><tr><td>+Weight SharingNAS</td><td>32.3M</td><td>78.2</td><td>4.7</td><td>54.6</td><td>73.4</td><td>59.3 36.3</td><td>59.3</td><td>72.1</td></tr><tr><td>+ Patch Size 14→16,Res 560→640</td><td>32.3M</td><td>78.5</td><td>4.7</td><td>54.4</td><td>73.2</td><td>59.1</td><td>35.9 59.2</td><td>72.1</td></tr><tr><td>+ Image Resolution 640→576</td><td>32.2M</td><td>64.2</td><td>4.0</td><td>53.6</td><td>72.4</td><td>58.2</td><td>34.8 58.6</td><td>72.0</td></tr><tr><td>+#Windows per Block 4→2</td><td>32.2M</td><td>63.7</td><td>4.3</td><td>54.3</td><td>73.3</td><td>58.8</td><td>35.6 59.4</td><td>73.2</td></tr><tr><td>+#Decoder Layers 3→4</td><td>33.7M</td><td>64.8</td><td>4.4</td><td>54.6</td><td>73.5</td><td>59.1</td><td>36.0 59.8</td><td>73.7</td></tr><tr><td>+#Query Tokens 300→300</td><td>33.7M</td><td>64.8</td><td>4.4</td><td>54.6</td><td>73.5</td><td>59.1</td><td>36.0 59.8</td><td>73.7</td></tr></table>

Additionally, existing foundation model families typically do not release lightweight ViT variants such as ViT-S or ViT-T, making it difficult to repurpose such models for real-time applications.

Rethinking Standard Accuracy Benchmarking Practices. Following prior work, we report all COCO results on the validation set. However, relying solely on the validation for both model selection and evaluation can lead to overfitting. For example, D-FINE (which builds on RT-DETR) conducts an extensive hyperparameter sweep on COCO’s validation set and reports its best model. However, evaluating this configuration on RF100-VL shows that D-FINE underperforms RT-DETR on the test set. In contrast, our method achieves state-of-the-art performance among all real-time detectors on RF100-VL and COCO, demonstrating the robustness of our weight-sharing NAS. In addition to evaluating on COCO, we advocate that future detectors should also evaluate on datasets with public validation and test splits like RF100-VL.

Table 6: Ablation on Backbone. We ablate the impact of using different backbone architectures for RF-DETR below. We find that DINOv2 achieves the highest performance, outperforming CAEv2 by $2 . 4 \%$ . All models are pretrained with 60 epochs of Objects365 and the ’Gentler Hyperparameters’ setting. Note that SAM2 and SigLIPv2 perform poorly when evaluated in FP16. Therefore, we report FP16 TensorRT latency with FP32 ONNX accuracy for these two models as an upper bound on what their performance could be if optimized for FP16.   

<table><tr><td>LW-DETR (M) + Gentler Hyperparameters</td><td>#Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td>w/CAEv2 ViT/S-16-Truncated Backbone</td><td>28.3M</td><td>83.7</td><td>4.4</td><td>52.3</td><td>71.4</td><td>56.3</td><td>32.3</td><td>56.4</td><td>70.0</td></tr><tr><td>w/DINOv2ViT/S-14Backbone</td><td>32.3M</td><td>78.2</td><td>4.7</td><td>54.3</td><td>73.4</td><td>58.8</td><td>35.8</td><td>59.2</td><td>72.3</td></tr><tr><td>w/SigLIPv2ViT/B-32Backbone*</td><td>105.1M</td><td>81.6</td><td>4.8</td><td>50.4</td><td>70.4</td><td>53.7</td><td>28.0</td><td>55.3</td><td>73.0</td></tr><tr><td>w/SAM2Hiera-SBackbone*</td><td>44.0M</td><td>109.1</td><td>11.2</td><td>53.6</td><td>72.4</td><td>57.9</td><td>33.3</td><td>58.3</td><td>71.0</td></tr></table>

Limitations. Despite controlling for power throttling and GPU overheating during inference, our latency measurements still have a variance of up to $0 . 1 \mathrm { { m s } }$ due to the non-deterministic behavior of TensorRT during compilation. Specifically, TensorRT can introduce power throttling, which in turn affects the resulting engine and leads to random fluctuations in latency. Although the measurement of a given TensorRT engine is generally consistent, recompiling the same ONNX artifact can produce different latency results. Therefore, we only report latencies with one digit of precision after the decimal place.

# 5 CONCLUSION

In this paper, we introduce RF-DETR, a state-of-the-art NAS-based method for fine-tuning specialist end-to-end object detectors for target datasets and hardware platforms. Our approach outperforms prior state-of-the-art real-time methods on COCO and RF100-VL, improving upon D-FINE (nano) by $5 \%$ AP on COCO. Moreover, we highlight that current architectures, learning rate schedulers and augmentation schedulers are tailored to maximize performance on COCO, suggesting that the community should benchmark models on diverse, large-scale datasets to prevent implicit overfitting. Lastly, we highlight the high variance in latency benchmarking due to power throttling and propose a standardized protocol to improve reproducibility.

# REFERENCES

Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. Flexivit: One model for all patch sizes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14496–14506, 2023.

Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9157–9166, 2019.

Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by network transformation. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018a.

Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332, 2018b.

Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791, 2019.

Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213–229. Springer, 2020.

Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4974– 4983, 2019.

Qiang Chen, Xiangbo Su, Xinyu Zhang, Jian Wang, Jiahui Chen, Yunpeng Shen, Chuchu Han, Ziliang Chen, Weixiang Xu, Fanrong Li, et al. Lw-detr: A transformer replacement to yolo for real-time detection. arXiv preprint arXiv:2406.03459, 2024a.

Qiang Chen, Xiangbo Su, Xinyu Zhang, Jian Wang, Jiahui Chen, Yunpeng Shen, Chuchu Han, Ziliang Chen, Weixiang Xu, Fanrong Li, et al. Lw-detr: a transformer replacement to yolo for real-time detection. arXiv preprint arXiv:2406.03459, 2024b.

Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024.

M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, 111, 2015.

Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. IEEE transactions on pattern analysis and machine intelligence, 32(9):1627–1645, 2009.

Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, and Wei-Shi Zheng. Llmdet: Learning strong open-vocabulary object detectors under the supervision of large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 14987–14997, 2025.

Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7036–7045, 2019.

Junjie He, Pengyu Li, Yifeng Geng, and Xuansong Xie. Fastinst: A simple query-based model for real-time instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 23663–23672, 2023.

Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In ´ Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017.

Glenn Jocher, Jing Qiu, and Ayush Chaurasia. Ultralytics YOLO, January 2023. URL https: //docs.ultralytics.com/models/yolov8.

Glenn Jocher, Jing Qiu, and Ayush Chaurasia. Ultralytics YOLO, January 2024. URL docs. ultralytics.com/models/yolo11.

Mehar Khurana, Neehar Peri, Deva Ramanan, and James Hays. Shelf-supervised multi-modal pretraining for 3d object detection. arXiv preprint arXiv:2406.10115, 2024.

Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel M Ni, and Heung-Yeung Shum. Mask dino: Towards a unified transformer-based framework for object detection and segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3041–3050, 2023.

Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10965–10975, 2022.

Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ´ ECCV, 2014.

Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.

Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.

Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV, 2016.

Yechi Ma, Neehar Peri, Shuoquan Wei, Wei Hua, Deva Ramanan, Yanan Li, and Shu Kong. Longtailed 3d detection via 2d late fusion. arXiv preprint arXiv:2312.10986, 2023.

Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 3651–3660, 2021.

Maxime Oquab, Timothee Darcet, Th ´ eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, ´ Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.

Aljosa Osep, Tim Meinhardt, Francesco Ferroni, Neehar Peri, Deva Ramanan, and Laura Leal-Taixe. Better call sal: Towards learning to segment anything in lidar. 2024.

Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, and Feng Wu. D-fine: Redefine regression task in detrs as fine-grained distribution refinement. arXiv preprint arXiv:2410.13842, 2024.

Neehar Peri, Achal Dave, Deva Ramanan, and Shu Kong. Towards long-tailed 3d detection. 2023.

Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Va- ¨ sudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Fe- ´ ichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714.

Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, volume 33, pp. 4780–4789, 2019.

Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779–788, 2016.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, 2015.

Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, and Neehar Peri. Roboflow100-vl: A multi-domain object detection benchmark for visionlanguage models. arXiv preprint arXiv:2505.20612, 2025.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211–252, 2015.

Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 8429–8438, 2019. doi: 10.1109/ICCV. 2019.00852.

Oriane Simeoni, Huy V Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, ´ Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. ¨ arXiv preprint arXiv:2508.10104, 2025.

Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1): 1929–1958, 2014.

Ayca Takmaz, Cristiano Saltori, Neehar Peri, Tim Meinhardt, Riccardo de Lutio, Laura Leal-Taixe, and Aljosa Osep. Towards Learning to Complete Anything in Lidar. In International Conference on Machine Learning (ICML), 2025.

Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pp. 6105–6114. PMLR, 2019.

Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2820–2828, 2019.

Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10781–10790, 2020.

Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-offreebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7464–7475, 2023.

Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using programmable gradient information. In European conference on computer vision, pp. 1–21. Springer, 2024.

Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10734–10742, 2019.

Yifan Xu, Mengdan Zhang, Chaoyou Fu, Peixian Chen, Xiaoshan Yang, Ke Li, and Changsheng Xu. Multi-modal queried object detection in the wild. Advances in Neural Information Processing Systems, 36, 2024.

Xiaoju Ye. calflops: a flops and params calculate tool for neural networks in pytorch framework, 2023. URL https://github.com/MrYxJ/calculate-flops.pytorch.

Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022a.

Xinyu Zhang, Jiahui Chen, Junkun Yuan, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi, Kun Yao, et al. Cae v2: Context autoencoder with clip target. arXiv preprint arXiv:2211.09799, 2022b.

Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16965–16974, 2024.

Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenb ¨ uhl, and Ishan Misra. Detecting ¨ twenty-thousand classes using image-level supervision. In European Conference on Computer Vision, pp. 350–368. Springer, 2022.

Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.

Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.

Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8697–8710, 2018.

# A IMPLEMENTATION DETAILS

Training Hyperparamters. RF-DETR extends LW-DETR (Chen et al., 2024a) for Neural Architecture Search. We highlight key differences in our training procedure below. First, we pseudo-label Objects365 (Shao et al., 2019) with SAM2 (Ravi et al., 2024) to allow us to pre-train the segmentation and detection heads on the same data. We use a learning rate of 1e-4 (LW-DETR uses 4e-4), and a batch size of 128 (LW-DETR uses the same). Similar to DINOv3 (Simeoni et al., 2025), we use ´ an EMA scheduler since this is necessary for EMA’s proper function. However, unlike DINOv3, we omit learning-rate warm-up. We clip all gradients greater than 0.1 and apply a per-layer multiplicative decay of 0.8 to preserve information (especially the earlier layers) in the DINOv2 backbone. We place our window attention blocks between layers $\{ 0 , 1 , 3 , 4 , 6 , 7 , 9 , 1 0 \}$ , while LW-DETR places their window attention blocks between layers $\{ 0 , 1 , 3 , 6 , 7 , 9 \}$ . Although we have the same number of windows, contiguous windowed blocks don’t require an additional reshape operation, making our implementation slightly more efficient. Further, we train with more multi-scale resolutions (0.5 to 1.5 scale) than LW-DETR (0.7 to 1.4 scale) to ensure that the augmentation is symmetric around the default scale. Notably, we add resolution as a “tunable knob” in our NAS search space, while LWDETR uses it as a form of data augmentation. Our model training and inference code is available on GitHub.

Latency Evaluation. We ensure fair evaluation between models by measuring detection accuracy and latency using the same artifact. To further standardize inference, we employ CUDA graphs in TensorRT, which pre-queue all kernels rather than requiring the CPU to launch them serially during execution. This optimization can accelerate some networks depending on the number and type of kernels used by the model. We observe that RT-DETR, LW-DETR, and RF-DETR benefit from this optimization. Further, CUDA graphs place LW-DETR on the same latency-accuracy curve as D-FINE, since CUDA graphs speed up LW-DETR but do not benefit D-FINE. We release our stand-alone latency benchmarking tool on GitHub.

Pareto-Optimal Model Configurations on COCO. We present the Pareto-Optimal RF-DETR and RF-DETR-Seg configs in Tables 7 and 8. We highlight notable trends about RF-DETR’s ParetoOptimal architectures in Appendix H.

Table 7: RF-DETR COCO Detection Model Config.   

<table><tr><td rowspan=1 colspan=1>ModelSize</td><td rowspan=1 colspan=1>Resolution</td><td rowspan=1 colspan=1>Patch Size</td><td rowspan=1 colspan=1>Windows</td><td rowspan=1 colspan=1>DecoderLayers</td><td rowspan=1 colspan=1>Queries</td><td rowspan=1 colspan=1>Backbone</td></tr><tr><td rowspan=1 colspan=1>N</td><td rowspan=1 colspan=1>384</td><td rowspan=1 colspan=1>16</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>S</td><td rowspan=1 colspan=1>512</td><td rowspan=1 colspan=1>16</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>3</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>M</td><td rowspan=1 colspan=1>576</td><td rowspan=1 colspan=1>16</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>L</td><td rowspan=1 colspan=1>704</td><td rowspan=1 colspan=1>16</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>XL</td><td rowspan=1 colspan=1>700</td><td rowspan=1 colspan=1>20</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>5</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-B</td></tr><tr><td rowspan=1 colspan=1>2XL</td><td rowspan=1 colspan=1>880</td><td rowspan=1 colspan=1>20</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>5</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-B</td></tr><tr><td rowspan=1 colspan=1>Max</td><td rowspan=1 colspan=1>828</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>6</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-B</td></tr></table>

Table 8: RF-DETR-Seg COCO Segmentation Model Config.   

<table><tr><td rowspan=1 colspan=1>ModelSize</td><td rowspan=1 colspan=1>Resolution</td><td rowspan=1 colspan=1>Patch Size</td><td rowspan=1 colspan=1>Windows</td><td rowspan=1 colspan=1>DecoderLayers</td><td rowspan=1 colspan=1>Queries</td><td rowspan=1 colspan=1>Backbone</td></tr><tr><td rowspan=1 colspan=1>N</td><td rowspan=1 colspan=1>312</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>100</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>S</td><td rowspan=1 colspan=1>384</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>100</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>M</td><td rowspan=1 colspan=1>432</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>5</td><td rowspan=1 colspan=1>200</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>L</td><td rowspan=1 colspan=1>504</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>5</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>XL</td><td rowspan=1 colspan=1>624</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>6</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>2XL</td><td rowspan=1 colspan=1>768</td><td rowspan=1 colspan=1>12</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>6</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-S</td></tr><tr><td rowspan=1 colspan=1>Max</td><td rowspan=1 colspan=1>890</td><td rowspan=1 colspan=1>10</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>6</td><td rowspan=1 colspan=1>300</td><td rowspan=1 colspan=1>DINOv2-S</td></tr></table>

# B ABLATION ON QUERY TOKENS AND DECODER LAYERS

We train RF-DETR (nano) with 300 object queries, following standard practice for real-time DETRbased object detectors. However, many datasets contain fewer than 300 objects per image. Therefore, processing all 300 queries can be computationally wasteful. LW-DETR (tiny) demonstrates that training with fewer queries can improve the latency-accuracy tradeoff. Rather than deciding on the optimal number of queries apriori, we find that we can drop queries at test time without retraining by discarding the lowest-confidence queries ordered by the confidence of the corresponding token at the output of the encoder. As shown in Figure 4, this yields meaningful latency-accuracy tradeoffs. In addition, prior work (Zhao et al., 2024) demonstrates that decoder layers can be pruned at test time, since each layer is supervised independently during training. We find that it is possible to remove all decoder layers, relying solely on the initial query proposals from the two-stage DETR pipeline. In this case, there is no cross-attention to the encoder states or self-attention between queries, leading to a substantial runtime reduction. The resulting model resembles a single-stage YOLO-style architecture without NMS. As shown in Figure 4, eliminating the final decoder layer reduces latency by $10 \%$ with only a 2 mAP drop in performance.

![](Images_FL7EZ6Y8/e7f563111a825199dd3c1aef90b4252c782e5c738b5db19014370b1fd0196786.jpg)  
Figure 4: Impact of Decoder Layers vs. Query Tokens. We evaluate the impact of inference-time query dropping for trading-off accuracy and latency in RF-DETR (nano). Interestingly, we find that dropping the 100 lowest confidence queries does not significantly reduce performance, but modestly improves latency for all decoder layers.

# C BENCHMARKING FLOPS

We benchmark FLOPs for RF-DETR GroundingDINO, and YOLO-E with PyTorch’s FlopCounterMode. We find that FlopCounterMode closely reproduces FLOPs counts obtained with custom benchmarking tools for YOLOv11, D-FINE, and LW-DETR. In practice, we also find that it provides more reliable results than CalFLOPs (Ye, 2023). Notably, LW-DETR’s FLOPs count is roughly twice that of the originally reported result (cf. Table 9). We posit that this discrepancy can be attributed to LW-DETR reporting MACs instead of FLOPs. We rely on the officially reported FLOPs counts from YOLOv11, YOLOv8, D-FINE, and RT-DETR.

Table 9: FLOPs Benchmarking Comparison. We compare FLOPs reported with custom benchmarking tools, CalFLOPs, and PyTorch’s FlopCounterMode. Notably, we find that FlopCounterModel closely matches the results reported with custom benchmarking code, suggesting that it is more reliable than prior generic benchmarking tools.   

<table><tr><td>Model</td><td>Size</td><td>Reported</td><td>CalFLOPs</td><td>FlopCounterMode</td></tr><tr><td>D-FINE</td><td>S</td><td>25.2M</td><td>25.2M</td><td>25.5M</td></tr><tr><td>LW-DETR</td><td>S</td><td>16.6M</td><td>22.9M</td><td>31.8M</td></tr><tr><td>YOLO11</td><td>S</td><td>21.5M</td><td>23.9M</td><td>21.6M</td></tr></table>

# D IMPACT OF CLASS-NAMES ON OPEN-VOCABULARY DETECTORS

We evaluate the impact of fine-tuning open-vocabulary detectors like GroundingDINO with class names on RF100-VL in Table 10. Intuitively, GroundingDINO’s vision-language pre-training is more useful when we prompt with class names (e.g. car, truck, bus) instead of class indices (e.g. 0, 1, 2). Using class names at finetune time therefore provides more information to the VLM about the underlying data than is available to non-VLM detectors, potentially leading to better downstream performance. However, we find that fine-tuning GroundingDINO on RF100-VL yields nearly identical performance in both cases, suggesting that naively fine-tuning the end-to-end model mitigates the benefits of open-vocabulary pre-training. Future should investigate ways of effectively fine-tuning VLMs to preserve foundational pre-training.

Table 10: Evaluating the Impact of Class Names. We evaluate the impact of using class-names when fine-tuning VLMs like GroundingDINO. We find that class-names do not provide significant benefit over prompting with class indices, suggesting that fine-tuning has diminished the impact of internet-scale pre-training.   

<table><tr><td rowspan=1 colspan=4>Model                                          Size# Params. GFLOPS Latency (ms)AP AP5o AP75 APs APM APL</td></tr><tr><td rowspan=1 colspan=4>RF100-VL</td></tr><tr><td rowspan=1 colspan=1>GroundingDINO (Liu etal.,2023) w/ Standard ClassNames</td><td rowspan=1 colspan=1>T</td><td rowspan=1 colspan=1>173.0M  1008.3   309.9*</td><td rowspan=1 colspan=1>62.388.867.839.257.769.5</td></tr><tr><td rowspan=1 colspan=1>GroundingDINO (Liu etal.,2023) w/Class Index Names</td><td rowspan=1 colspan=1>T</td><td rowspan=1 colspan=1>173.0M  1008.3   309.9*</td><td rowspan=1 colspan=1>62.588.268.3 40.0 58.470.3</td></tr></table>

# E BENCHMARKING LARGER MODEL VARIANTS

Detectors like LW-DETR (Chen et al., 2024a) and D-FINE (Peng et al., 2024) hand-design larger variants to scale up a model family. In contrast, NAS-based architectures like RF-DETR automatically discover scaling strategies through grid-based search. We analyze two families of RF-DETR models derived from distinct scaling strategies: one based on a DINOv2-S backbone and another based on a DINOv2-B backbone. To evaluate how well each family scales, we compare their NASgenerated Pareto curves against those of D-FINE. Specifically, at each D-FINE size, we identify the RF-DETR variant with the same backbone that maximizes performance at a comparable latency. For example, when comparing to D-FINE (small), we select the RF-DETR model that offers the best accuracy without exceeding D-FINE (small)’s latency.

As shown in Table 11, the DINOv2-S backbone family initially surpasses D-FINE in mAP@50:95 but fails to maintain this advantage at larger model sizes, suggesting that its scaling strategy is less effective than D-FINEs manual design. In contrast, the DINOv2-B backbone family shows the opposite trend, where the performance gap between D-FINE and RF-DETR narrows as latency increases. This implies that at higher latencies, the DINOv2-B based RF-DETR models could surpass D-FINE (and indeed RF-DETR (2x-large) outperforms D-FINE on mAP 50:95). Importantly, expanding the D-FINE model family would require substantial additional engineering effort, whereas extending the RF-DETR model family is straightforward; higher-latency variants can be sampled directly from the same NAS search without re-training. We present the COCO and RF100-VL of our larger variants in Tables 12, 13, and 14. We also include an RF-DETR Max variant on each dataset to show our method’s maximum performance with latency less than $1 0 0 \mathrm { m s }$ , a scale other model families don’t reach.

Table 11: mAP@50:95 Gap of RF-DETR vs D-FINE at Similar Latencies We compare how different RF-DETR model families scale relative to D-FINE. D-FINE (nano) is excluded since it was not pretrained on Objects-365 and is therefore not expected to follow similar scaling trends. For each RF-DETR backbone, we select the highest accuracy Pareto-optimal NAS-mined model with latency up to that of the corresponding D-FINE variant. Notably, RF-DETR (DINOv2-B) achieves better scalability than RF-DETR (DINOv2-S) and D-FINE. Note that none of the RF-DETR models for COCO are finetuned.   

<table><tr><td rowspan=1 colspan=1>Method (Backbone)</td><td rowspan=1 colspan=2>S                M</td><td rowspan=1 colspan=1>L</td><td rowspan=1 colspan=1>XL</td></tr><tr><td rowspan=1 colspan=1>RF-DETR (DINOv2-S)</td><td rowspan=1 colspan=1>+2.3</td><td rowspan=1 colspan=1>+0.9</td><td rowspan=1 colspan=1>-0.4</td><td rowspan=1 colspan=1>-1.1</td></tr><tr><td rowspan=1 colspan=1>RF-DETR (DINOv2-B)</td><td rowspan=1 colspan=1>-3.1</td><td rowspan=1 colspan=1>-1.3</td><td rowspan=1 colspan=1>-1.2</td><td rowspan=1 colspan=1>-0.7</td></tr></table>

Table 12: COCO Detection Evaluation for Larger Model Variants. We present RF-DETR’s performance for L, XL, and 2XL sizes on COCO below. Notably, D-FINE (x-large) outperforms RF-DETR ( $\mathbf { \dot { x } }$ -large) on mAP 50:95. However, RF-DETR $2 \mathbf { x }$ -large) beats D-FINE by 0.8 AP, and is the first real-time detector to surpass 60 AP on COCO.   

<table><tr><td>Model</td><td>Size</td><td>#Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs APM</td><td>APL</td></tr><tr><td>Real-Time ObjectDetection w/NMS</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>YOLOv8 (Jocher et al.,2023)</td><td>L</td><td>43.7M</td><td>165.2</td><td>8.0</td><td>49.5</td><td>64.7</td><td>54.0</td><td>30.2 55.1</td><td>68.5</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>L</td><td>25.3M</td><td>86.9</td><td>6.5</td><td>49.9</td><td>64.9</td><td>54.5</td><td>30.4 55.9</td><td>68.1</td></tr><tr><td>YOLOv8(Jocher et al.,2023)</td><td>XL</td><td>68.2M</td><td>257.8</td><td>11.3</td><td>50.5</td><td>65.6</td><td>55.1 30.0</td><td>56.2</td><td>69.5</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>XL</td><td>56.9M</td><td>194.9</td><td>10.5</td><td>50.9</td><td>66.1</td><td>55.4 31.5</td><td>56.6</td><td>68.7</td></tr><tr><td colspan="4">End-to-End Real-Time Object Detection</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RT-DETR (Zhao et al.,2024)</td><td>R50</td><td>42M</td><td>136</td><td>8.5</td><td>55.0</td><td>73.3</td><td>59.8 37.9</td><td>59.7</td><td>71.6</td></tr><tr><td>LW-DETR(Chenetal.,2024a)</td><td>L</td><td>46.8M</td><td>137.5</td><td>6.9</td><td>56.1</td><td>74.6</td><td>61.0 37.1</td><td>60.4</td><td>73.0</td></tr><tr><td>D-FINE (Peng et al.,2024)</td><td>L</td><td>31M</td><td>91</td><td>7.5</td><td>57.2</td><td>74.9</td><td>62.2 40.6</td><td>61.4</td><td>73.7</td></tr><tr><td>RF-DETR(Ours)</td><td>L</td><td>33.9M</td><td>125.6</td><td>6.8</td><td>56.5</td><td>75.1</td><td>61.3 39.0</td><td>61.0</td><td>73.9</td></tr><tr><td>RT-DETR(Zhao et al.,2024)</td><td>R101</td><td>76M</td><td>259</td><td>12.0</td><td>56.1</td><td>74.5</td><td>61.1</td><td></td><td></td></tr><tr><td>LW-DETR(Chen et al.,2024a)</td><td>XL</td><td>118.0M</td><td>342.5</td><td>13.0</td><td>58.3</td><td>76.9</td><td>63.3 40.2</td><td>38.1 60.4</td><td>73.4</td></tr><tr><td>D-FINE (Peng et al.,2024)</td><td>XL</td><td>62M</td><td>202</td><td>11.5</td><td>59.3</td><td>76.8</td><td>64.6 42.1</td><td>63.3 64.2</td><td>74.7 76.3</td></tr><tr><td>RF-DETR(Ours)</td><td>XL</td><td>126.4M</td><td>299.3</td><td>11.5</td><td>58.6</td><td>77.4</td><td>63.8 40.3</td><td>63.9</td><td>76.2</td></tr><tr><td>RF-DETR(Ours)</td><td>2XL</td><td>126.9M</td><td>438.4</td><td>17.2</td><td>60.1</td><td>78.5</td><td>65.5 43.2</td><td>64.9</td><td>76.2</td></tr><tr><td>RF-DETR(Ours)</td><td>Max</td><td>132.4M</td><td>1742.5</td><td>98.0</td><td>61.8</td><td>79.7</td><td>67.7 47.5</td><td>66.1</td><td>76.0</td></tr></table>

Table 13: COCO Segmentation Evaluation for Larger Model Variants. We present RF-DETR’s performance for L, XL, and 2XL sizes on the COCO segmentation benchmark below. We find that scaling up RF-DETR yields considerable performance improvements. In contrast, YOLOv8 and YOLOv11 do not significantly improve with scale.   

<table><tr><td>Model</td><td>Size</td><td># Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td colspan="7">Real-Time Instance Segmentation w/NMS</td><td colspan="3"></td><td></td></tr><tr><td>YOLOv8(Jocher et al.,2023)</td><td>L</td><td>46.0M</td><td>220.5</td><td>9.7</td><td>39.0</td><td>60.5</td><td>41.7</td><td>18.0</td><td>44.7</td><td>57.8</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>L</td><td>27.6M</td><td>132.2</td><td>8.3</td><td>39.5</td><td>61.5</td><td>42.1</td><td>18.6</td><td>45.5</td><td>59.4</td></tr><tr><td>YOLOv8 (Jocher et al.,2023)</td><td>XL</td><td>71.8M</td><td>344.1</td><td>14.0</td><td>39.5</td><td>61.3</td><td>42.1</td><td>18.9</td><td>45.6</td><td>58.8</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>XL</td><td>62.1M</td><td>296.4</td><td>13.7</td><td>40.1</td><td>62.4</td><td>42.6</td><td>18.8</td><td>46.4</td><td>60.1</td></tr><tr><td colspan="9">End-to-End Real-Time Instance Segmentation</td><td></td></tr><tr><td>RF-DETR(Ours) RF-DETR(Ours)</td><td>L XL</td><td>36.2M</td><td>151.1 260.0</td><td>8.8</td><td>47.1</td><td>70.5 72.2</td><td>50.9</td><td>28.4</td><td>52.1</td><td>65.6</td></tr><tr><td>RF-DETR(Ours)</td><td>2XL</td><td>38.1M</td><td>435.3</td><td>13.5</td><td>48.8 49.9</td><td>73.1</td><td>53.1</td><td>30.6</td><td>53.3</td><td>65.9</td></tr><tr><td>RF-DETR(Ours)</td><td>Max</td><td>38.6M 40.1M</td><td>1668.2</td><td>21.8 95.6</td><td>50.5</td><td>74.0</td><td>54.5 55.4</td><td>33.9 34.6</td><td>54.1</td><td>65.7</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>54.2</td><td>65.4</td></tr></table>

# F IMPACT ON NAS FINE-TUNING ON COCO

We find that fine-tuning after NAS provides limited benefit for COCO. We posit that the NAS “architecture augmentation” acts as a strong regularizer, and additional training without this regularization leads to degraded performance. Specifically, when models are pre-trained with strong regularization, removing the regularization during fine-tuning leads to overfitting. As shown in Tables 15 and 16, this trend is consistent across both detection and segmentation tasks. Interestingly, models trained on RF100-VL benefit more from fine-tuning, likely because they require more than 100 epochs to converge. In such cases, we posit that reducing the total number of NAS configurations during training, or training for more than 100 epochs with weight-sharing NAS can improve performance.

# G IMPACT OF FIXED ARCHITECTURE ON RF100-VL

We evaluate the impact of transferring a NAS architecture optimized for COCO to RF100-VL in Table 17. We find that these fixed architecture models perform remarkably well without further dataset-specific NAS. Specifically, RF-DETR (large) model with a fixed architecture achieves the best performance among all prior real-time models on COCO. However, dataset-specific NAS yields significant additional gains. Notably, the improvement from LW-DETR to the fixed architecture is comparable to the improvement from the fixed architecture to the NAS-optimized model on the target dataset for N, S, and M scale models.

Table 14: RF100-VL Detection Evaluation for Larger Model Variants. We present RF-DETR’s performance for L, XL, and 2XL sizes on RF100-VL below. Notably, RF-DETR $\mathbf { \dot { x } }$ -large) beats D-FINE by 0.5 AP. Fine-tuning RF-DETR $\mathbf { \dot { x } }$ -large) improves performance by an additional $0 . 4 \mathrm { A P } .$   

<table><tr><td>Model</td><td>Size</td><td># Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td colspan="9">Real-Time Object Detection w/ NMS</td><td></td></tr><tr><td>YOLOv8 (Jocher et al.,2023)</td><td>L</td><td>43.7M</td><td>165.2</td><td>7.9</td><td>56.5</td><td>82.1</td><td>61.1</td><td>7.1</td><td>46.0</td><td>48.9</td></tr><tr><td>YOLOv11(Jocher et al.,2024)</td><td>L</td><td>25.3M</td><td>86.9</td><td>6.4</td><td>56.5</td><td>82.2</td><td>61.0</td><td>6.4</td><td>45.5</td><td>49.0</td></tr><tr><td>YOLOv8 (Jocher et al.,2023)</td><td>XL</td><td>68.2M</td><td>257.8</td><td>11.2</td><td>56.5</td><td>82.3</td><td>61.0</td><td>6.6</td><td>45.7</td><td>47.9</td></tr><tr><td>YOLOv11 (Jocher et al.,2024)</td><td>XL</td><td>56.9M</td><td>194.9</td><td>10.3</td><td>56.2</td><td>81.7</td><td>60.8</td><td>6.1</td><td>45.9</td><td>48.1</td></tr><tr><td colspan="9">End-to-EndReal-TimeObjectDetection</td><td></td></tr><tr><td>RT-DETR(Zhao et al., 2024)</td><td>R50</td><td>42M</td><td>136</td><td>8.4</td><td>61.7</td><td>87.7</td><td>66.9</td><td>38.1</td><td>57.1</td><td>69.4</td></tr><tr><td>LW-DETR (Chen et al.,2024a)</td><td>L</td><td>46.8M</td><td>137.5</td><td>6.8</td><td>61.5</td><td>87.4</td><td>67.0</td><td>37.1</td><td>56.4</td><td>69.0</td></tr><tr><td>D-FINE (Peng et al.,2024)</td><td>L</td><td>31M</td><td>91</td><td>7.5</td><td>61.6</td><td>86.4</td><td>67.2</td><td>37.8</td><td>56.5</td><td>70.1</td></tr><tr><td>RF-DETR(Ours)</td><td>L</td><td>34.1M</td><td>119.1</td><td>6.3</td><td>62.3</td><td>88.2</td><td>68.0</td><td>36.4</td><td>57.3</td><td>70.6</td></tr><tr><td>RF-DETR(Ours) w/Fine-Tuning</td><td>L</td><td>34.1M</td><td>119.1</td><td>6.3</td><td>62.6</td><td>88.4</td><td>68.2</td><td>37.0</td><td>57.5</td><td>70.5</td></tr><tr><td>RT-DETR(Zhao et al.,2024)</td><td>R101</td><td>76M</td><td>259</td><td>11.9</td><td>61.0</td><td>87.4</td><td>66.2</td><td>36.6</td><td>56.3</td><td>68.2</td></tr><tr><td>LW-DETR(Chen etal.,2024a)</td><td>XL</td><td>118.0M</td><td>342.5</td><td>13.0</td><td>62.1</td><td>87.9</td><td>67.6</td><td>37.4</td><td>57.1</td><td>70.2</td></tr><tr><td>D-FINE (Peng et al.,2024)</td><td>XL</td><td>59.3</td><td>76.8</td><td>11.4</td><td>62.2</td><td>86.9</td><td>68.0</td><td>37.6</td><td>57.4</td><td>69.7</td></tr><tr><td>RF-DETR(Ours)</td><td>XL</td><td>35.0M</td><td>199.0</td><td>9.8</td><td>62.7</td><td>88.5</td><td>68.5</td><td>39.3</td><td>58.4</td><td>70.4</td></tr><tr><td>RF-DETR(Ours)w/Fine-Tuning</td><td>XL</td><td>35.0M</td><td>199.0</td><td>9.8</td><td>63.1</td><td>88.6</td><td>69.0</td><td>39.6</td><td>58.5</td><td>70.8</td></tr><tr><td>RF-DETR(Ours)</td><td>2XL</td><td>123.5M</td><td>410.2</td><td>15.6</td><td>63.3</td><td>88.9</td><td>69.0</td><td>38.7</td><td>58.2</td><td>71.6</td></tr><tr><td>RF-DETR(Ours)w/Fine-Tuning</td><td>2XL</td><td>123.5M</td><td>410.2</td><td>15.6</td><td>63.5</td><td>89.0</td><td>69.2</td><td>38.9</td><td>58.3</td><td>71.7</td></tr></table>

Table 15: COCO Detection Fine-Tuning Evaluation. We find that fine-tuning after NAS provides limited benefit for COCO detection, particularly for larger model sizes.   

<table><tr><td>Model</td><td>Size</td><td>#Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td colspan="9">End-to-End Real-Time Object Detectors</td></tr><tr><td>RF-DETR(Ours)</td><td>N</td><td>30.5M</td><td>31.9</td><td>2.3</td><td>48.0</td><td>67.0</td><td>51.4</td><td>25.2</td><td>53.5</td><td>70.0</td></tr><tr><td>RF-DETR(Ours)w/Fine-Tuning</td><td>N</td><td>30.5M</td><td>31.9</td><td>2.3</td><td>+0.4</td><td>+0.6</td><td>+0.3</td><td>+0.1</td><td>+0.1</td><td>+1.3</td></tr><tr><td>RF-DETR(Ours)</td><td>S</td><td>32.1M</td><td>59.8</td><td>3.5</td><td>52.9</td><td>71.9</td><td>57.0</td><td>32.0</td><td>58.3</td><td>73.0</td></tr><tr><td>RF-DETR(Ours) w/Fine-Tuning</td><td>S</td><td>32.1M</td><td>59.8</td><td>3.5</td><td>+0.1</td><td>+0.2</td><td>+0.2</td><td>-0.2</td><td>+0.2</td><td>+0.1</td></tr><tr><td>RF-DETR(Ours)</td><td>M</td><td>33.7M</td><td>78.8</td><td>4.4</td><td>54.7</td><td>73.5</td><td>59.2</td><td>36.1</td><td>59.7</td><td>73.8</td></tr><tr><td>RF-DETR(Ours) w/Fine-Tuning</td><td>M</td><td>33.7M</td><td>78.8</td><td>4.4</td><td>+0.0</td><td>+0.1</td><td>+0.0</td><td>-0.1</td><td>+0.1</td><td>-0.1</td></tr><tr><td>RF-DETR(Ours)</td><td>L</td><td>33.9M</td><td>125.6</td><td>6.8</td><td>56.5</td><td>75.1</td><td>61.3</td><td>39.0</td><td>61.0</td><td>73.9</td></tr><tr><td>RF-DETR(Ours) w/Fine-Tuning</td><td>L</td><td>33.9M</td><td>125.6</td><td>6.8</td><td>+0.0</td><td>+0.0</td><td>+0.0</td><td>-0.1</td><td>+0.1</td><td>+0.1</td></tr><tr><td>RF-DETR(Ours)</td><td>XL</td><td>126.4M</td><td>299.3</td><td>11.5</td><td>58.6</td><td>77.4</td><td>63.8</td><td>40.3</td><td>63.9</td><td>76.2</td></tr><tr><td>RF-DETR(Ours)w/Fine-Tuning</td><td>XL</td><td>126.4M</td><td>299.3</td><td>11.5</td><td>+0.3</td><td>+0.1</td><td>+0.2</td><td>+0.5</td><td>+0.4</td><td>+0.1</td></tr><tr><td>RF-DETR(Ours)</td><td>2XL</td><td>126.9M</td><td>438.4</td><td>17.2</td><td>60.1</td><td>78.5</td><td>65.5</td><td>43.2</td><td>64.9</td><td>76.2</td></tr><tr><td>RF-DETR(Ours) w/Fine-Tuning</td><td>2XL</td><td>126.9M</td><td>438.4</td><td>17.2</td><td>+0.1</td><td>+0.0</td><td>+0.3</td><td>+0.5</td><td>+0.2</td><td>+0.1</td></tr></table>

# H DISCUSSION ON NOTABLE DISCOVERED ARCHITECTURES

All “tunable” knobs are used when defining the Pareto-optimal model families, validating our choice of search space. This suggests that expanding the search space may additionally improve downstream performance.

The Pareto optimal models tend to use the same patch size, excluding the Max variants which are just chosen as the highest accuracy running at less than $1 0 0 \mathrm { m s }$ . For example, the optimal patch size for RF-DETR with a DINOv2-S backbone converges to a size of 16, whereas the DINOv2- B backbone converges to a patch size of 20. The optimal patch size for RF-DETR-Seg with a DINOv2-S backbone is 12. All Pareto-optimal model families simultaneously scale the compute in both the encoder and the decoder. Changing patch size, number of windows, and resolution impacts the encoder, and changing number of decoder layers and number of queries impacts the decoder. For RF-DETR-Seg, scaling resolution also impacts the segmentation head. We find that using 2 windows is typically optimal in the encoder and resolution scales within a family as we increase latency. For the decoder, on COCO the detector model keeps queries constant and scales decoder layers only, while for the segmentation model both are scaled simultaneously. This may be because the depth of the segmentation head is tied to the number of decoder layers, and there may be a certain minimum number of layers in the segmentation head that produce viable masks, so even the lowest latency model uses at least that number of layers. To compensate for the additional latency of those decoder layers, the segmentation model trades off the number of queries it uses to get lower latency for smaller model sizes, which effectively reduces the width of the decoder. While the object detector evaluated on COCO prefers a wide and shallow decoder, the segmentation model prefers a thin and deep decoder.

Table 16: COCO Segmentation Fine-Tuning Evaluation. We find that fine-tuning after NAS provides limited benefit for COCO segmentation, particularly for larger model sizes.   

<table><tr><td>Model</td><td>Size</td><td>#Params.</td><td>GFLOPS</td><td>Latency (ms)</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td>End-to-End Real-Time Object Detectors</td><td></td><td>33.6M</td><td>50.0</td><td>3.4</td><td>40.3</td><td>63.0</td><td>42.6</td><td>16.3</td><td>45.3</td><td>63.6</td></tr><tr><td>RF-DETR-Seg.(Ours) RF-DETR-Seg.w/Fine-Tuning (Ours)</td><td>N N</td><td>33.6M</td><td>50.0</td><td>3.4</td><td>+0.1</td><td>+0.4</td><td>+0.0</td><td>-0.5</td><td>+0.2</td><td>+0.7</td></tr><tr><td>RF-DETR-Seg.(Ours)</td><td>S</td><td>33.7M</td><td>70.6</td><td>4.4</td><td>43.1</td><td>66.2</td><td>45.9</td><td>21.9</td><td>48.5</td><td>64.1</td></tr><tr><td>RF-DETRw/Fine-Tuning(Ours)</td><td>S</td><td>Did</td><td>Not</td><td>Improve</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>RF-DETR-Seg.(Ours)</td><td>M</td><td>35.7M</td><td>102.0</td><td>5.9</td><td>45.3</td><td>68.4</td><td>48.8</td><td>25.5</td><td>50.4</td><td>65.3</td></tr><tr><td>RF-DETRw/Fine-Tuning(Ours)</td><td>M</td><td>Did</td><td>Not</td><td>Improve</td><td>-</td><td>-</td><td>-</td><td>-</td><td>：</td><td>-</td></tr><tr><td>RF-DETR(Ours)</td><td>L</td><td>36.2M</td><td>151.1</td><td>8.8</td><td>47.1</td><td>70.5</td><td>50.9</td><td>28.4</td><td>52.1</td><td>65.6</td></tr><tr><td>RF-DETR(Ours)w/Fine-Tuning</td><td>L</td><td>Did</td><td>Not</td><td>Improve</td><td>-</td><td>1</td><td>-</td><td>1</td><td>-</td><td>-</td></tr><tr><td>RF-DETR(Ours)</td><td>XL</td><td>38.1M</td><td>260.0</td><td>13.5</td><td>48.8</td><td>72.2</td><td>53.1</td><td>30.6</td><td>53.3</td><td>65.9</td></tr><tr><td>RF-DETR(Ours)w/Fine-Tuning</td><td>XL</td><td>Did</td><td>Not</td><td>Improve</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>RF-DETR(Ours)</td><td>2XL</td><td>38.6M</td><td>435.3</td><td>21.8</td><td>49.9</td><td>73.1</td><td>54.5</td><td>33.9</td><td>54.1</td><td>65.7</td></tr><tr><td>RF-DETR(Ours) w/Fine-Tuning</td><td>2XL</td><td>Did</td><td>Not</td><td>Improve</td><td>1</td><td>1</td><td>-</td><td>-</td><td>-</td><td>1</td></tr></table>

![](Images_FL7EZ6Y8/d55ef0310940efc8dff236228cd2ac9b16a71290e31c1f8c07c6e62e83b4f5ab.jpg)  
RF100-VL Object Detection   
Figure 5: Ablating Fixed Architecture RF100-VL. We evaluate the benefit of dataset-specific NAS by transferring the COCO-optimized RF-DETRarchitecture to RF100-VL. Although the fixed architecture was not tuned for RF100-VL, it still outperforms LW-DETR. Running NAS directly on RF100-VL further improves performance over the fixed architecture. Additional fine-tuning provides consistent gains across all model sizes, with particularly strong improvements for smaller models. This is consistent with our observations on COCO object detection.

We find that RF-DETR’s performance depends on the number of spatial locations (e.g. resolution divided by patch size) rather than resolution or patch size individually. Scaling resolution while fixing patch size yields similar results to scaling patch size while fixing resolution, since vision transformers are agnostic to absolute input resolution after the patchify-and-project stage. To verify this, we constructed an alternative family with fixed resolution (640) and varied patch sizes to preserve the number of spatial locations. Specifically, we evaluate RF-DETR (nano) with a patch size of 27, RF-DETR (small) with a patch size of 21, and RF-DETR (medium) with a patch size of 18, achieving results nearly identical to the Pareto-optimal family. Notably, patch sizes of 27 and 18 were unseen during training, demonstrating RF-DETR’s strong generalization to novel patch sizes.

Table 17: RF100-VL Fixed Architecture Evaluation. We evaluate the transfer of architectures optimized for COCO to RF100-VL. Fixed architecture models perform strongly without additional dataset-specific NAS, with the RF-DETR (large) model achieving the best performance among prior real-time models. However, dataset-specific NAS provides significant further gains.   

<table><tr><td>Model</td><td></td><td>Size#Params. GFLOPS Latency (ms)AP AP5o AP75 APs APM APL</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>End-to-End Real-Time Object Detectors</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RF-DETR(Ours) Fixed Architecture</td><td>N</td><td>30.5M 31.9</td><td>2.3</td><td>57.785.0</td><td>61.9</td><td>30.8 51.5</td><td>67.4</td></tr><tr><td>RF-DETR(Ours)</td><td>N</td><td>30.8M 36.3</td><td>2.5</td><td>57.684.9</td><td>62.1</td><td>30.7 52.2</td><td>66.8</td></tr><tr><td>RF-DETRw/Fine-Tuning(Ours)</td><td>N</td><td>30.8M 36.3</td><td>2.5</td><td>58.785.6</td><td>63.5</td><td>32.4 52.7</td><td>67.0</td></tr><tr><td>RF-DETR(Ours) Fixed Architecture</td><td>S</td><td>32.1M 59.8</td><td>3.5</td><td>60.286.7</td><td>65.0</td><td>34.2 54.4</td><td>68.9</td></tr><tr><td>RF-DETR(Ours)</td><td>S</td><td>33.3M 65.5</td><td>3.7</td><td>60.787.0</td><td>66.0</td><td>35.4 55.4</td><td>69.6</td></tr><tr><td>RF-DETRw/Fine-Tuning(Ours)</td><td>S</td><td>33.3M</td><td>65.5 3.7</td><td>61.087.2</td><td>66.4</td><td>35.3 55.9</td><td>69.8</td></tr><tr><td>RF-DETR(Ours)Fixed Architecture</td><td>M</td><td>33.7M</td><td>78.8 4.4</td><td>61.287.4</td><td>66.4</td><td>35.8 56.1</td><td>69.8</td></tr><tr><td>RF-DETR(Ours)</td><td>M</td><td>33.6M</td><td>91.0 4.6</td><td>61.587.7</td><td>67.0</td><td>36.44 56.5</td><td>69.8</td></tr><tr><td>RF-DETRw/Fine-Tuning(Ours)</td><td>M</td><td>33.6M</td><td>91.0 4.6</td><td>61.9 87.9</td><td>67.3</td><td>36.4</td><td>56.6 70.1</td></tr><tr><td>RF-DETR(Ours) w/Fixed Architecture</td><td>L</td><td>33.9M</td><td>125.6 6.8</td><td>62.2 88.2</td><td>67.8</td><td>37.7</td><td>57.0 70.5</td></tr><tr><td>RF-DETR(Ours)</td><td>L</td><td>34.1M</td><td>119.1 6.3</td><td>62.3 88.2</td><td>68.0</td><td>36.4</td><td>57.3 70.6</td></tr><tr><td>RF-DETR(Ours)w/Fine-Tuning</td><td>L</td><td>34.1M</td><td>119.1 6.3</td><td>62.6 88.4</td><td>68.2</td><td>37.0</td><td>57.5 70.5</td></tr><tr><td>RF-DETR(Ours,DINOv2-Base) w/Fixed Architecture</td><td>XL</td><td>126.4M</td><td>299.3 11.5</td><td>62.9 88.5</td><td>68.6</td><td>37.0</td><td>57.5 71.3</td></tr><tr><td>RF-DETR(Ours)</td><td>XL</td><td>35.0M</td><td>199.0 9.8</td><td>62.7 88.5</td><td>68.5</td><td>39.3</td><td>58.4 70.4</td></tr><tr><td>RF-DETR(Ours) w/Fine-Tuning</td><td>XL</td><td>35.0M</td><td>199.0 9.8</td><td>63.1 88.6</td><td>69.0</td><td>39.6</td><td>58.5 70.8</td></tr><tr><td>RF-DETR(Ours,DINOv2-Base)w/Fixed Architecture</td><td>2XL</td><td>126.9M</td><td>438.4 17.1</td><td>63.2 89.0</td><td>69.3</td><td>38.4</td><td>58.4 71.5</td></tr><tr><td>RF-DETR(Ours,DINOv2-Base)</td><td>2XL</td><td>123.5M</td><td>410.2 15.6</td><td>63.3 88.9</td><td>69.0</td><td>38.7</td><td>58.2 71.6</td></tr><tr><td>RF-DETR(Ours,DINOv2-Base) w/Fine-Tuning</td><td>2XL</td><td>123.5M</td><td>410.2 15.6</td><td>63.589.0</td><td>69.2</td><td>38.9</td><td>58.3 71.7</td></tr></table>

However, we find that this trend does not hold for RF-DETR-Seg. The segmentation head features are always upsampled to run at 1/4 scale of the input image resolution. Therefore, scaling resolution affects both the number of spatial locations and the segmentation head resolution. Specifically, RF-DETR-Seg (nano) uses a resolution of 312 and a patch size of 12, yielding a segmentation head resolution of 78 with 26 spatial locations; RF-DETR-Seg (small) uses a resolution of 384 and a patch size of 12, yielding a segmentation head resolution 96 with 32 spatial locations; and RF-DETR-Seg (medium) uses a resolution of 432 and a patch size of 12, yielding a segmentation head resolution of 108 with 36 spatial locations. In contrast, scaling patch size alone (e.g., a patch size of 16 at a resolution of 576) can keep spatial locations fixed while increasing head resolution (to 144). This is a more nuanced interaction between patch size and resolution than is observed in RF-DETR object detection. RF-DETR(medium) uses $5 7 6 / 1 6 = 3 6$ spatial locations, while RF-DETR-Seg (medium) uses $4 3 2 / 1 2 = 3 6$ spatial locations, but at a lower resolution, showing that the additional tying of the segmentation mask resolution changes the Pareto-optimal resolution even if the optimal number of spatial locations is the same for a given latency range.

Dataset characteristics influence optimal discovered architectures. We find that when running NAS on RF100-VL datasets, the optimal lower latency models tend to use fewer queries than the COCO models of equivalent latency, which always uses 300. This may be because RF100-VL datasets tend to have fewer objects per image than COCO, so fewer queries are required to find all the objects, as each query is able to find a single object.

Most Pareto-optimal RF-DETRmodels perform best with 2 windows, whereas LW-DETR achieves the best performance with 4 windows. We attribute this difference to how each architecture handles class tokens. LW-DETRs CAEv2 backbone omits the class token, while RF-DETR’s DINOv2 backbone relies on it as a key part of pre-training. To make windowed attention compatible with class tokens, we duplicate the class token for each window. During global attention, window-level class tokens attend to one another, while all other tokens attend to all class tokens. In practice, RF-DETR (nano), RF-DETR (small), and RF-DETR (medium) all use 2 windows, since duplicating class tokens for additional windows reduces runtime efficiency. As a result, unlike LW-DETR, RF-DETR does not benefit from scaling to 4 windows.

To generate our model based on DINOv2-B, we follow the scaling strategy from ViT-S to ViT-B and just double the width of all layers in the model. We find that this is sufficient to gain strong and differentiated performance when we allow NAS to explore the axis of variation we’ve defined. Notably, different from LW-DETR’s larger variants, we don’t use higher resolution feature maps from the backbone.

# I VISUALIZING MODEL PREDICTIONS

We visualize model predictions from RF-DETR (nano) and compare them with comparable detection and segmentation baselines in Figure 6. We find that RF-DETR (nano) predicts fewer false positives (e..g mistaking sign post for person). Similarly, RF-DETR-Seg. (nano) predicts more precise object boundaries.

![](Images_FL7EZ6Y8/0e1348c00e3f73ef428fb69a08714055b07081e6c4856dc48fe2d9ac4842337e.jpg)  
Figure 6: Visualizing Model Predictions. On the left, we compare detections from RF-DETR (nano) and LW-DETR (tiny). On the right, we compare instance segmentation masks from RFDETR-Seg (nano) and YOLOv11 (nano)