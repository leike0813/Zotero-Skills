# RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision

Shuo Wang\* Chunlong Xia∗ Feng Lv Yifeng Shi† Baidu Inc, China

{wangshuo36, xiachunlong, lvfeng02, shiyifeng}@baidu.com

# Abstract

RT-DETR is the first real-time end-to-end transformerbased object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder’s feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves $4 8 . 1 \%$ AP $^ { \prime } { + } I . 6 \% / { + } I . 4 \% )$ compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the same latency. Furthermore, RT-DETRv3-R101 can attain an impressive $5 4 . 6 \%$ AP outperforming YOLOv10-X. The code will be released at https://github.com/ clxia12/RT-DETRv3.

# 1. Introduction

Object detection is an important fundamental problem in computer vision, which mainly focuses on obtaining the position and category information of objects in the image. Real-time object detection has higher requirements for algorithm performance, such as inference speed greater than 30 FPS. It has enormous value in practical applications such as autonomous driving, video surveillance, and object tracking. In recent years, real-time object detections have garnered significant attention from both researchers and industry professionals due to its efficient inference speed and superior detection accuracy. Among these, the most popular are single-stage real-time object detectors based on CNNs, such as the YOLO series ( [1, 11–13, 22, 24, 26]). They all adopted a one-to-many label assignment strategy, designed an efficient inference framework, and used non-maximum suppression (NMS) to filter redundant prediction results. Although this strategy introduced additional latency, they achieved a trade-off between accuracy and speed.

![](Images_CVV3EPRH/b6409b2f4146c645b7e66a2557b3a34875ecea3b615656b39a1203e2fa8494d6.jpg)  
Figure 1. Compared to other real-time object detectors. Our method has better performance in the trade-off between speed and accuracy. $^ *$ represents adding extra data.

DETR [2] is the first transformer-based end-to-end object detection algorithm. It employs set prediction and is optimized through the Hungarian matching strategy, eliminating the need for NMS post-processing and thereby simplifying the object detection process. Subsequent DETRs (such as DAB-DETR [16], DINO [29], and DN-DETR [14], etc.) further introduce iterative refinement schemes and denoising training, which effectively accelerating the convergence speed of the model and improving its performance. However, its high computational complexity significantly limits its practical applications.

RT-DETR [32] is the first real-time end-to-end transformer-based object detection algorithm. It designed an efficient hybrid encoder and IoU-aware query selection module, and a scalable decoder layer, achieving better results than other real-time detectors. However, the Hungarian matching strategy provides sparse supervision during training, leading to insufficient training of both the encoder and decoder, which limits the optimal performance of the approach. RT-DETRv2 [19] further enhances the flexibility and practicality of RT-DETR [32] by optimizing the training strategy to improve performance without sacrificing speed, although requires longer training time. To effectively address the issue of sparse supervision in object detection, we propose a hierarchical dense positive supervision method, which effectively accelerates model convergence and enhances model performance by introducing multiple auxiliary branches during training. Our main contributions are as follows:

• We introduce a one-to-many label assignment auxiliary head based on CNN, which collaborates with the original detection branch for optimization, further enhancing the representational capability of the encoder.

• We propose a learning strategy with self-attention perturbations aimed at enhancing the supervision of the decoder by diversifying label assignments across multiple query groups. Additionally, we introduced a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. These approaches significantly improve the model’s performance and accelerate convergence without additional inference latency.

• Extensive experiments conducted on the COCO dataset have thoroughly validated the effectiveness of our proposed approach. As shown in Figure 1, RTDETRv3 significantly outperforms other real-time detectors, including the RT-DETR series and YOLO series. For instance, RT-DETRv3-R18 achieves $4 8 . 1 \%$ AP $( + 1 . 6 \% )$ compared to RT-DETR-R18, while maintaining the same latency. Additionally, RT-DETRv3- R50 outperforms YOLOv9-C by $0 . 9 \%$ AP, even with a latency reduction of $1 . 3 \mathrm { m s }$ .

# 2. Related Work

# 2.1. CNN-based real-time object detection.

The current CNN-based real-time object detectors are mainly the YOLO series. YOLOv4 [1] and YOLOv5 [12] optimized the network architecture (e.g., by adopting CSPNet [25] and PAN [17]), while also utilizing Mosaic data augmentation. YOLOv6 [13] further optimized the structure, including the RepVGG [6] backbone, decoupled head, SimSPPF, and more effective training strategy (e.g., SimOTA [7], etc.). YOLOv7 [24] introduces the E-ELAN attention module to better integrate features from different levels and adopts the adaptive anchor mechanism to improve small object detection. YOLOv8 [11] proposed a C2f module for effective feature extraction and fusion. YOLOv9 [26] proposed a new GELAN architecture and designed a PGI to enhance the training process. The PPYOLO series [10,18] is a real-time object detection solution based on the PaddlePaddle framework proposed by Baidu. This series of algorithms has been optimized and improved on the basis of the YOLO series, aiming to improve detection accuracy and speed to meet the needs of practical application scenarios.

# 2.2. Transformer-based real-time object detection.

RT-DETR [32] is the first real-time end-to-end object detector. This approach designs an efficient hybrid encoder that effectively processes multi-scale features by decoupling intra-scale interactions and cross-scale fusion and proposes IoU-aware query selection to further improve performance by providing higher-quality initial object queries to the decoder. Its accuracy and speed are superior to the YOLO series of the same period, and it has received widespread attention. RT-DETRv2 [19] further optimized the training strategy, including dynamic data augmentation and optimized sampling operators for easy deployment, resulting in further improvement of its model performance. However, due to their one-to-one sparse supervision, the convergence speed and final effect are limited. Therefore, introducing a one-to-many label assignment strategy can further improve the model’s performance.

# 2.3. Auxiliary training strategy.

Co-DETR [33] proposed multiple parallel one-to-many label assignment auxiliary head training strategies (e.g., ATSS [30] and Faster RCNN [20]), which can easily enhance the learning ability of the encoder in end-to-end detectors. For example, the integration of ViT-CoMer [27] with Co-DETR [33] has achieved state-of-the-art performance on the COCO detection task. DAC-DETR [9], MSDETR [31], and GroupDETR [4] mainly accelerate the convergence of the model by adding one-to-many supervised information to the decoder of the model. The above approaches accelerate the convergence or improve the performance of the model by adding additional auxiliary branches at different positions of the model, but they are not real-time object detectors. Inspired by these, we introduced multiple one-to-many auxiliary dense supervision modules to both the encoder and decoder of RT-DETR [32]. These modules enhance the convergence speed and improve the overall performance of the RT-DETR [32]. Since these modules are only involved during the training phase, they don’t affect the inference latency of RT-DETR [32].

![](Images_CVV3EPRH/eb3cef6d2d2a892d914eb5cb3c5d4e6da165cc07d3d57aade962d8f185adeb83.jpg)  
Figure 2. Architecture of RT-DETRv3. We preserve the core architecture of RT-DETR(highlighted in yellow)and propose a novel hierarchical decoupled dense supervision method (emphasized in green). Firstly, we enhance the encoder’s representation capability by incorporating a CNN-based one-to-many label assignment auxiliary branch. Secondly, to enhance and strengthen supervision of the decoder, we generate multiple object queries (OQ) through the query selection module and apply random masking to perturb the selfattention mechanism, effectively diversifying the distribution of positive query samples. Additionally, to ensure that multiple relevant queries focus on the same target, we introduce a supplementary one-to-many matching branch. Notably, these auxiliary branches are discarded during evaluation.

# 3.1. Overall Architecture.

dom masks. These masks are applied to the self-attention module, affecting the correlation between queries and thus differentiating the assignments of positive queries. Each set of random masks is paired with a corresponding query, as depicted in the Figure 2 by $O Q _ { o 2 o - 1 } , . . . , O Q _ { o 2 o - n }$ . Furthermore, to ensure that there are more high-quality queries matching each ground truth, we incorporate an one-to-many label assignment branch within the decoder. The following sections provide a detailed description of the modules proposed in this work.

# 3. Method

The overall structure of RT-DETRv3 is shown in Figure 2. We have retained the overall framework of RTDETR [32] (highlighted in yellow) and additionally introduced our proposed hierarchical decoupling dense supervision method (highlighted in green). Initially, the input image is processed through a CNN backbone (e.g., ResNet [8]) and a feature fusion module, termed the efficient hybrid encoder, to obtain multi-scale features $\{ C _ { 3 } , C _ { 4 }$ , and $C _ { 5 } \}$ . These features are then fed into a CNN-based one-to-many auxiliary branch and a transformer-based decoder branch in parallel. For the CNN-based one-to-many auxiliary branch, we directly employ existing state-of-the-art dense supervision methods, such as PP-YOLOE [28], to collaboratively supervise the encoder’s representation learning. In the transformer-based decoder branch, the multi-scale features are first flattened and concatenated. We then use a query selection module to select the top- $\mathbf { \nabla } \cdot \mathbf { k }$ features from them to generate object queries. Within the decoder, we introduce a mask generator that produces multiple sets of ran

# 3.2. Overview of RT-DETR.

RT-DETR [32] is a real-time detection framework designed for object detection tasks. It integrates the advantages of end-to-end prediction from DETR [3] while optimizing inference speed and detection accuracy. To achieve real-time performance, the encoder module is replaced with a lightweight CNN backbone, and an Efficient Hybrid Encoder module which designed for efficient feature fusion. RT-DETR [32] proposed an Uncertaintyminimal query selection module to select high-confidence feature as object queries, reducing the difficulty of query optimization. Subsequently, multiple layers of the decoder enhance these queries through self-attention, cross-attention and feed-forward network (FFN) modules, with the prediction results produced by MLP layers. During the training optimization process, RT-DETR [32] employs Hungarian matching for one-to-one assignment. For loss calculation, it uses L1 loss and GIoU loss to supervise box regression, and variable focus loss (VFL) to supervise the learning of the classification task.

![](Images_CVV3EPRH/0fb865f97b6fc6b60d8ffe52097bcad1b158bb74553910833af3e6f114ec7b04.jpg)  
Figure 3. Mask self-attention module. $M _ { i }$ represents the perturbation mask corresponding to the $_ { i }$ -th set of object queries. $\otimes$ denotes matrix multiplication, and $\odot$ denotes element-wise multiplication.

# 3.3. One-to-Many Auxiliary Branch Based on CNN.

To alleviate the problem of sparse supervision in encoder output caused by the decoder’s one-to-one set matching scheme, we introduce an auxiliary detection head with one-to-many assignment, such as PP-YOLOE [28]. This strategy can effectively strengthen the supervision of the encoder, enabling it to have sufficient representation ability to accelerate the convergence of the model. Specifically, we directly integrate the output features $\{ C _ { 3 } , C _ { 4 }$ , and $C _ { 5 } \}$ of the encoder into PP-YOLOE head, For the one-to-many matching algorithm, we follow the configuration of the PPYOLOE head and use the ATSS matching algorithm in the early stage of training, and then switch to the TaskAlign matching algorithm. For the learning of classification and localization tasks, VFL and distributed focus loss (DFL) were respectively selected. Among them, VFL uses IoU scores as the target for positive samples, which makes positive samples with high IoU contribute relatively more to the loss. This also makes the model focus more on high-quality samples rather than low-quality samples during training. Specifically, decoder head also use VFL loss to ensure consistency in task definition. We denote the overall loss of the CNN auxiliary branch as $L _ { a u x }$ , with the corresponding loss weight denoted as $\alpha$ .

# 3.4. Multi-Group Self-Attention Perturbation Branches Based on Transformer.

The decoder consists of a series of transformer blocks, with each block incorporating a self-attention, crossattention, and FFN (Feed-Forward Network) module. Initially, the queries interact with each other through the selfattention module to enhance or diminish their feature representations. Subsequently, each query updates itself by retrieving information from the encoder’s output features via the cross-attention module. Lastly, the FFN predicts the class and bounding box coordinates of the target corresponding to each query. However, the adoption of a oneto-one set matching in the RT-DETR leads to sparse supervision information, ultimately impairing the model’s performance.

To ensure that multiple related queries associated with the same target have the opportunity to participate in positive sample learning, we propose multiple self-attention perturbation modules based on Mask Self-Attention. The implementation details of this perturbation module are shown in Figure 2. First, we generate multiple sets of object queries through the query selection module, denoted as $O Q _ { i }$ $\it { i = } 1 . . . N$ , where $N$ is the number of sets). Correspondingly, we use a mask generator to generate a random perturbation mask $M _ { i }$ for each set of $O Q _ { i }$ . Both $O Q i$ and $M _ { i }$ are fed into the Mask Self-Attention module, resulting in the perturbed and fused features.

The detailed implementation of the Mask Self-Attention module is shown in Figure 3, $O Q _ { i }$ is first linearly projected to obtain $Q _ { i } , K _ { i }$ , and $V _ { i }$ . Then, $Q _ { i }$ and $K _ { i }$ are multiplied to compute the attention weight, which is further multiplied by $M _ { i }$ and passed through a softmax function to yield the perturbed attention weight. Finally, this perturbed attention weight is multiplied by $V _ { i }$ to obtain the fused result $\tilde { V } _ { i }$ . The process can be represented as:

$$
Q _ { i } , K _ { i } , V _ { i } = L i n e a r ( O Q _ { i } )
$$

$$
W _ { i } = S o f t m a x ( M _ { i } ( Q _ { i } K _ { i } ^ { T } ) )
$$

$$
\tilde { V } _ { i } = W _ { i } V _ { i }
$$

The introduction of multiple sets of random perturbations diversifies the features of the queries, allowing multiple related queries associated with the same target to have a chance of being assigned as positive sample queries, thereby enriching the supervision information. During training, multiple sets of object queries are concatenated and fed into a single decoder branch, enabling parameter sharing and enhancing training efficiency. The loss computation and label assignment scheme remain consistent with RT-DETR. We denote the loss of the $_ { i - t h }$ set as $L o s s _ { o 2 o } ^ { i }$ and the total loss for $\mathbf { N }$ perturbation sets is calculated as follow:

$$
L _ { o 2 o } = \frac { 1 } { N } \sum _ { 1 } ^ { N } L _ { o 2 o } ^ { i }
$$

with the corresponding loss weight denoted as $\beta$

# 3.5. One-to-Many Dense Supervision Branch Based on Transformer.

To maximize benefits of multi-group self-attention perturbation branches, we introduce an additional dense supervision branch with shared weights in the decoder. This ensures more high-quality queries matching each ground truth. Specifically, we employ a query selection module to generate a unique set of object queries. During the sample matching phase, an augmented target set is generated by replicating the training labels by a factor of $m$ , with a default value of 4. This augmented set is subsequently matched against the prediction of the query. The loss computation remains consistent with the original detection loss, and we designate $L _ { o 2 m }$ as the loss function for this branch, with a loss weight of $\gamma$ .

<table><tr><td>Model</td><td>Backbone</td><td>#Params(M)</td><td>GFLOPs</td><td>Latency (ms)</td><td>APval 1x</td><td> APval 3x</td><td>APval 6x</td></tr><tr><td>RT-DETR [32]</td><td rowspan="2">R18</td><td rowspan="2">20</td><td rowspan="2">60</td><td rowspan="2">4.6</td><td>38.7</td><td>44.5</td><td>46.5</td></tr><tr><td>RT-DETRv2 [19]</td><td>39.8</td><td>44.9</td><td>46.7/47.9†</td></tr><tr><td> RT-DETRv3 (ours)</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"> 41.5</td><td rowspan="2">46.1</td><td rowspan="2"> 48.1/48.7+</td></tr><tr><td></td></tr><tr><td>RT-DETR [32]</td><td rowspan="2">R34</td><td rowspan="2">31</td><td rowspan="2">92</td><td rowspan="2">6.3</td><td rowspan="2">42.8</td><td rowspan="2">47.5</td><td rowspan="2">48.9 49.0/49.9†</td></tr><tr><td>RT-DETRv2 [19]</td></tr><tr><td></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2">43.0 44.7</td><td rowspan="2">47.2 48.6</td><td rowspan="2"> 49.9/50.1†</td></tr><tr><td> RT-DETRv3 (ours)</td></tr><tr><td>RT-DETR [32]</td><td rowspan="2">R50</td><td rowspan="2"></td><td rowspan="2">136</td><td rowspan="2"></td><td rowspan="2">48.9</td><td rowspan="2">52.2</td><td rowspan="2">53.1</td></tr><tr><td>RT-DETRv2 [19]</td></tr><tr><td></td><td rowspan="2"></td><td rowspan="2">42</td><td rowspan="2"></td><td rowspan="2">9.2</td><td rowspan="2">1</td><td rowspan="2">1</td><td rowspan="2">53.4 53.4</td></tr><tr><td> RT-DETRv3 (ours)</td></tr><tr><td>RT-DETR [32]</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2">50.2 50.2</td><td rowspan="2">53.0 53.5</td><td rowspan="2">54.3</td></tr><tr><td></td></tr><tr><td>RT-DETRv2 [19]</td><td rowspan="2">R101</td><td rowspan="2">76</td><td rowspan="2">259</td><td rowspan="2">13.5</td><td rowspan="2">-</td><td rowspan="2">1 54.2</td><td rowspan="2">54.3 54.6</td></tr><tr><td> RT-DETRv3 (ours)</td></tr></table>

# 3.6. Total Loss.

In summary, the overall loss function of our proposed approach is as follows:

$$
L = \alpha L _ { a u x } + \beta L _ { o 2 o } + \gamma L _ { o 2 m }
$$

where $L _ { a u x }$ is responsible for dense supervision of the encoder, $L _ { o 2 o }$ enriches the one-to-one supervision information for the decoder while preserving the end-to-end prediction characteristics, and $L _ { o 2 m }$ provides one-to-many dense supervision to the decoder. By default, the loss weights $\alpha$ $\beta$ , and $\gamma$ are set to 1.

# 4. Experiments

# 4.1. Datasets and Evaluation Metrics.

We selected the MS COCO 2017 [15] object detection dataset as the evaluation benchmark for our approach. This dataset consists of $1 1 5 \mathrm { k }$ training images and $5 \mathrm { k }$ test images.

We adopted the same evaluation metric, AP, as used in the RT-DETR [32] approach. We compared the performance of RT-DETRv3 with other real-time object detectors in terms of convergence efficiency, inference speed, and effectiveness, which include both transformer-based and CNN-based real-time object detectors. Additionally, we conducted ablation studies on the modules mentioned in this paper. All experimental details and results are elaborated in the following sections.

# 4.2. Implementation Details.

We integrated proposed hierarchical dense supervision branches into the RT-DETR [32] framework. The CNNbased dense supervision auxiliary branch directly employed the PP-YOLOE head, with its sample matching strategy, loss calculation, and all other configurations consistent with those of PP-YOLOE [28]. We reused the RT-DETR [32] decoder structure as the main branch and additionally added three groups of parameter-shared self-attention perturbation branches. The sample matching method is consistent with the main branch, utilizing Hungarian matching algorithm. We also added a parameter-shared one-to-many matching branch, where each ground truth is matched with four object queries by default and set 300 object queries in total. The AdamW optimizer, integrated with a weight decay factor of 0.0001, was employed, ensuring that all other training configurations adhered strictly to the RT-DETR [32], encompassing both data augmentation and pre-training. We used a $1 0 \mathrm { x }$ (120 epochs) training schedule for smaller backbones (R18, R34) and a 6x (72 epochs) training schedule for larger backbones (R50, R101). We use four NVIDIA

Table 2. Compared to CNN-based real-time object detectors on COCO val2017.   

<table><tr><td>Model</td><td>#Epochs</td><td>#Params (M)</td><td>GFLOPs</td><td>Latency (ms)</td><td>APual</td></tr><tr><td>YOLOv6-3.0-S [13]</td><td>300</td><td>18.5</td><td>45.3</td><td>3.4</td><td>44.3</td></tr><tr><td>Gold-YOLO-S [23]</td><td>300</td><td>21.5</td><td>46.0</td><td>3.8</td><td>45.4</td></tr><tr><td>YOLO-MS-S [5]</td><td>300</td><td>8.1</td><td>31.2</td><td>10.1</td><td>46.2</td></tr><tr><td>YOLOv8-S [11]</td><td>500</td><td>11.2</td><td>28.6</td><td>7.1</td><td>46.2</td></tr><tr><td>YOLOv9-S [26]</td><td>500</td><td>7.1</td><td>26.4</td><td>-</td><td>46.7</td></tr><tr><td>YOLOV10-S [22]</td><td>500</td><td>7.2</td><td>21.6</td><td>2.5</td><td>46.3</td></tr><tr><td> RT-DETRv3-R18 (ours)</td><td>120</td><td>20</td><td>60</td><td>4.6</td><td>48.7</td></tr><tr><td>YOLOv6-3.0-M[13]</td><td>300</td><td>34.9</td><td>85.8</td><td>5.6</td><td>49.1</td></tr><tr><td>Gold-YOLO-M [23]</td><td>300</td><td>41.3</td><td>87.5</td><td>6.4</td><td>49.8</td></tr><tr><td>YOLO-MS [5]</td><td>300</td><td>22.2</td><td>80.2</td><td>12.4</td><td>51.0</td></tr><tr><td>YOLOv8-M[11]</td><td>500</td><td>25.9</td><td>78.9</td><td>9.5</td><td>50.6</td></tr><tr><td>YOLOv9-M[26]</td><td>500</td><td>20.0</td><td>76.3</td><td>：</td><td>51.1</td></tr><tr><td>YOLOV10-M[22]</td><td>500</td><td>15.4</td><td>59.1</td><td>4.7</td><td>51.1</td></tr><tr><td> RT-DETRv3-R34 (ours)</td><td>120</td><td>31</td><td>92</td><td>6.3</td><td>50.1</td></tr><tr><td> RT-DETRv3-R50m (ours)</td><td>72</td><td>36</td><td>100</td><td>6.89</td><td> 51.7</td></tr><tr><td>Gold-YOLO-L [23]</td><td>300</td><td>75.1</td><td>151.7</td><td>9.0</td><td>51.8</td></tr><tr><td>YOLOv5-X [12]</td><td>300</td><td>86</td><td>205</td><td>23.3</td><td>50.7</td></tr><tr><td>PPYOLOE-L [28]</td><td>300</td><td>52</td><td>110</td><td>10.6</td><td>51.4</td></tr><tr><td>YOLOv6-L [13]</td><td>300</td><td>59</td><td>150</td><td>10.1</td><td>52.8</td></tr><tr><td>YOLOv7-L</td><td>300</td><td>36</td><td>104</td><td>18.2</td><td>51.2</td></tr><tr><td>YOLOV8-L [11]</td><td>500</td><td>43</td><td>165</td><td>14.1</td><td>52.9</td></tr><tr><td>YOLOv9-C [26]</td><td>500</td><td>25.3</td><td>102.1</td><td>10.57</td><td>52.5</td></tr><tr><td>YOLOV10-L [22]</td><td>500</td><td>24.4</td><td>120.3</td><td>7.28</td><td>53.2</td></tr><tr><td> RT-DETRv3-R50 (ours)</td><td>72</td><td>42</td><td>136</td><td>9.2</td><td> 53.4</td></tr><tr><td>YOLOv8-X [11]</td><td>500</td><td>68.2</td><td>257.8</td><td>16.9</td><td>53.9</td></tr><tr><td>YOLOv10-X [22]</td><td>500</td><td>29.5</td><td>160.4</td><td>10.7</td><td>54.4</td></tr><tr><td>RT-DETRv3-R101 (ours)</td><td>72</td><td>76</td><td>259</td><td>13.5</td><td> 54.6</td></tr></table>

A100 GPUs to train our proposed method with a batch size of 64. Moreover, the latencies of all models are tested on T4 GPU with TensorRT FP16, following [32]. We have observed that, in comparison to most detectors employing longer training epochs, RT-DETRv3 only needs 72 epochs to achieve superior accuracy.

# 4.3. Comparison with Transformer-based Realtime Object Detectors.

Inference speed and algorithm performance. The real-time object detectors based on transformer architecture are primarily represented by the RT-DETR series. Table 1 presents the comparison results between our approach and the RT-DETR series. Our approach outperforms both

RT-DETR [32] and RT-DETRv2 [19] across various backbone. Specifically, compared to RT-DETR [32], with the 6x training schedule, our approach demonstrates improvements of $1 . 6 \%$ , $1 . 0 \%$ , $0 . 3 \%$ , and $0 . 3 \%$ with the R18, R34, R50, and R101 backbones, respectively. In comparison to RT-DETRv2 [19], we evaluated the R18 and R34 backbones under $6 \mathrm { x } / 1 0 \mathrm { x }$ training schedules, where our approach improvements of $1 . 4 \% / 0 . 8 \%$ and $0 . 9 \% / 0 . 2 \%$ , respectively. Moreover, since the auxiliary dense supervision branches we proposed are training-only, our approach maintains the same inference speed as both RT-DETR [32] and RT-DETRv2 [19].

Convergence speed. Our approach builds on the RTDETR [32] framework by incorporating CNN-based and transformer-based one-to-many dense supervision, which not only boosts model performance but also speeds up convergence. We have conducted extensive experiments to validate the effectiveness of our approach. Table 1 presents a comparative analysis of RT-DETRv3, RT-DETR [32], and RT-DETRv2 [19] across various training schedules. It clearly demonstrates that our method outperforms them in terms of convergence speed in any schedule and only needs half of the training epochs to achieve the comparable performance.

Table 3. Analysis of overfitting. Extra data represents the Object365 dataset [21].   

<table><tr><td>Method</td><td>Extra data</td><td>Epochs</td><td>APual</td></tr><tr><td rowspan="3">RT-DETRv3-R50</td><td>X</td><td>51</td><td rowspan="3">53.4 52.9 (-0.5) 54.2</td></tr><tr><td>X</td><td>72</td></tr><tr><td>√</td><td>51</td></tr><tr><td rowspan="5">RT-DETRv3-R101</td><td>√ X</td><td>72 51</td><td>54.8 (+0.6) 54.6</td></tr><tr><td>X</td><td>72</td><td>54.2 (-0.4)</td></tr><tr><td>√</td><td>51</td><td>54.7</td></tr><tr><td>√</td><td></td><td></td></tr><tr><td></td><td>72</td><td>55.4 (+0.7)</td></tr></table>

Analysis of overfitting. As illustrated in Figure 4, we noticed that as the model size increases, RT-DETRv3 tends to exhibit overfitting. We believe this may be due to a mismatch between the size of the training dataset and the model size. We conducted several experiments, as shown in Table 3, when we added additional training data, the performance of RT-DETRv3 continues to improve as the training epochs increase, and it performs better than the model without the additional data at the same epochs.

# 4.4. Comparison with CNN-Based Real-time Object Detectors.

Inference speed and algorithm performance. We compared the end-to-end speed and accuracy of RTDETRv3 with current advanced CNN-based real-time object detection methods. We categorized the models into small, medium, and large scales based on their inference speed. Under similar inference performance conditions, we compared RT-DETRv3 with other stateof-the-art algorithms such as YOLOv6-3.0 [13], GoldYOLO [23], YOLO-MS [5], YOLOv8 [11], YOLOv9 [26], and YOLOv10 [22]. As shown in Table 2, for smallscale models, the RT-DETRv3-R18 approach outperforms YOLOv6-3.0-S, Gold-YOLO-S, YOLO-MS-S, YOLOv8- S, YOLOv9-S, and YOLOv10-S by $4 . 4 \%$ , $3 . 3 \%$ , $2 . 5 \%$ $2 . 5 \%$ , $2 . 0 \%$ , and $2 . 4 \%$ , respectively. For mediumscale models, RT-DETRv3 also demonstrates superior performance compared to YOLOv6-3.0-M, Gold-YOLO-M,

![](Images_CVV3EPRH/c0a703a181e2b72df544f458a01a36a2622440083f5bd6068f20740294c7ddc4.jpg)  
Figure 4. Convergence curves of RT-DETRv3 across different model sizes. $\star$ represents the best AP.

YOLO-MS-M, YOLOv8-M, YOLOv9-M, and YOLOv10- M. For large-scale models, our approach consistently outperforms CNN-based real-time object detectors. For example, our RT-DETRv3-R101 can achieve 54.6 AP, which is better than YOLOv10-X. However, since we have not yet optimized the overall framework of the RT-DETRv3 detector for lightweight deployment, there is still room for further improving the inference efficiency of RT-DETRv3.

Convergence speed. As shown in Table 2, we are excited to find that our RT-DETRv3, while achieving superior performance, can reduce the training epochs to as little as $60 \%$ or even less compared to CNN-based real-time detectors.

# 4.5. Ablation Study.

Settings. We conducted the ablation experiments using RT-DETR [32] as the baseline and then validated the impact of our proposed approach by sequentially integrating auxiliary CNN-based one-to-many label assignments branch, the auxiliary transformer-based one-to-many label assignments branch, and the multi-group self-attention perturbation modules. These experiments were performed with ResNet18 as the backbone, with a batch size of 64, and four NVIDIA A100 GPUs, while maintaining other configurations consistent with RT-DETR [32].

Ablation for components. We conducted ablation experiments to evaluate proposed modules in this paper. As shown in Table 4, each module significantly enhances the model’s performance. For instance, by adding O2M-T module to RT-DETR [32], we observed a $1 . 0 \%$ improvement in performance over the base model. When all proposed modules are integrated into RT-DETR for algorithm optimization, the model’s performance improves by $1 . 6 \%$ .

Table 4. Ablation studies of key components. O2M-C represents the one-to-many auxiliary branch based on CNN, O2M-T refers to one-to-many dense supervision branch based on transformer, and MGSA stands for multi-group self-attention perturbation branch based on transformer.   

<table><tr><td>Method</td><td>02M-C</td><td>02M-T</td><td>MGSA</td><td>APval</td></tr><tr><td rowspan="4">RT-DETR</td><td>X</td><td>X</td><td>X</td><td>46.5</td></tr><tr><td>√</td><td>X</td><td>X</td><td>47.4</td></tr><tr><td>X</td><td>√</td><td>X</td><td>47.5</td></tr><tr><td>X</td><td>X</td><td>√</td><td>47.5</td></tr><tr><td></td><td>√</td><td>√</td><td>√</td><td>48.1</td></tr></table>

Table 5. Ablation study on the number of self-attention perturbation branches.   

<table><tr><td>Number</td><td>APval</td><td>AP</td></tr><tr><td>2</td><td>47.9</td><td>65.4</td></tr><tr><td>3</td><td>48.1</td><td>65.6</td></tr><tr><td>4</td><td>48.0</td><td>65.3</td></tr></table>

Number of self-attention perturbation branches. To verify the effect varying the number of self-attention perturbation branches on RT-DETRv3 performance, we conducted ablation experiments using RT-DETRv3-R18 with branch counts of 2, 3, and 4, while keeping all other configurations unchanged. As shown in Table 5, when the number of branches was set to 3, the model achieved its optimal performance with AP 48.1. Reducing the number of branches decreased the richness of the supervision signals, leading to lower performance. Conversely, increasing the number of branches excessively raised the model’s learning difficulty without yielding significant performance gains.

# 5. Conclusion

In this paper, we propose a real-time object detection algorithm based on transformer, named RT-DETRv3. This algorithm builds upon RT-DETR by incorporating multiple dense positive sample auxiliary supervision modules. These modules apply one-to-many object supervision to specific features of both the encoder and decoder in RTDETR, thereby accelerating the algorithm’s convergence and improving its performance. It’s important to note that these modules are training-only. We validated the effectiveness of our algorithm on the COCO object detection benchmark, and the experiments demonstrate that our algorithm achieves better results compared to other real-time object detectors. We hope that our work can inspire researchers and developers working on real-time transformer-based object detection.

# References

[1] Alexey Bochkovskiy, Chien-Yao Wang, and HongYuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 1, 2   
[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213–229. Springer, 2020. 2   
[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213–229. Springer, 2020. 3   
[4] Qiang Chen, Xiaokang Chen, Jian Wang, Shan Zhang, Kun Yao, Haocheng Feng, Junyu Han, Errui Ding, Gang Zeng, and Jingdong Wang. Group detr: Fast detr training with group-wise one-to-many assignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6633–6642, 2023. 2   
[5] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin Hou, and Ming-Ming Cheng. Yolo-ms: rethinking multiscale representation learning for real-time object detection. arXiv preprint arXiv:2308.05480, 2023. 6, 7   
[6] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13733–13742, 2021. 2   
[7] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021. 2   
[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 3   
[9] Zhengdong Hu, Yifan Sun, Jingdong Wang, and Yi Yang. Dac-detr: Divide the attention layers and conquer. Advances in Neural Information Processing Systems, 36, 2024. 2   
[10] Xin Huang, Xinxin Wang, Wenyu Lv, Xiaying Bai, Xiang Long, Kaipeng Deng, Qingqing Dang, Shumin Han, Qiwen Liu, Xiaoguang Hu, et al. Pp-yolov2: A practical object detector. arXiv preprint arXiv:2104.10419, 2021. 2   
[11] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics YOLO, Jan. 2023. 1, 2, 6, 7   
[12] Glenn Jocher, Ayush Chaurasia, Alex Stoken, Jirka Borovec, Yonghye Kwon, Kalen Michael, Jiacong Fang, Colin Wong, Zeng Yifu, Diego Montes, et al. ultralytics/yolov5: v6. 2-yolov5 classification models, apple m1, reproducibility, clearml and deci. ai integrations. Zenodo, 2022. 1, 2, 6   
[13] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, et al. Yolov6: A single-stage object detection framework for industrial applications. arXiv preprint arXiv:2209.02976, 2022. 1, 2, 6, 7   
[14] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13619–13627, 2022. 2   
[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence ´ Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5   
[16] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022. 2   
[17] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8759–8768, 2018. 2   
[18] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang, Qingqing Dang, Yuan Gao, Hui Shen, Jianguo Ren, Shumin Han, Errui Ding, et al. Pp-yolo: An effective and efficient implementation of object detector. arXiv preprint arXiv:2007.12099, 2020. 2   
[19] Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, and Yi Liu. Rt-detrv2: Improved baseline with bag-of-freebies for real-time detection transformer. arXiv preprint arXiv:2407.17140, 2024. 2, 5, 6, 7   
[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015. 2   
[21] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430–8439, 2019. 7   
[22] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time endto-end object detection. arXiv preprint arXiv:2405.14458, 2024. 1, 6, 7   
[23] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai Han. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. Advances in Neural Information Processing Systems, 36, 2024. 6, 7   
[24] Chien-Yao Wang, Alexey Bochkovskiy, and HongYuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7464–7475, 2023. 1, 2   
[25] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 390–391, 2020. 2   
[26] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using programmable gradient information. arXiv preprint arXiv:2402.13616, 2024. 1, 2, 6, 7   
[27] Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, and Yifeng Shi. Vit-comer: Vision transformer with convolutional multi-scale feature interaction for dense predictions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5493–5502, 2024. 2   
[28] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, et al. Pp-yoloe: An evolved version of yolo. arXiv preprint arXiv:2203.16250, 2022. 3, 4, 5, 6   
[29] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. 2   
[30] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759–9768, 2020. 2   
[31] Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, and Jingdong Wang. Ms-detr: Efficient detr training with mixed supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17027–17036, 2024. 2   
[32] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16965–16974, 2024. 2, 3, 5, 6, 7   
[33] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6748–6758, 2023. 2