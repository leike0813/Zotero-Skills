# LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection

Qiang Chen $^ { 1 \star }$ , Xiangbo Su $\cdot ^ { 1 \ast }$ ∗, Xinyu Zhang $^ { 2 , 1 }$ ∗, Jian Wang1, Jiahui Chen $^ { 3 , 1 }$ Yunpeng Shen $^ { 1 }$ , Chuchu Han1, Ziliang Chen1, Weixiang $\mathrm { { X u ^ { 1 } } }$ , Fanrong Li $^ 4$ , Shan Zhang $^ 5$ , Kun Yao $^ { 1 }$ , Errui Ding1, Gang Zhang1, and Jingdong Wang1†

1 Baidu Inc., China 2 The University of Adelaide, Australia 3 Beihang University, China   
4 Institute of Automation, Chinese Academy of Sciences, China 5 Australian National University, Australia   
{chenqiang13,suxiangbo,zhangxinyu14,wangjingdong}@baidu.com

Abstract. In this paper, we present a light-weight detection transformer, LW-DETR, which outperforms YOLOs for real-time object detection. The architecture is a simple stack of a ViT encoder, a projector, and a shallow DETR decoder. Our approach leverages recent advanced techniques, such as training-effective techniques, e.g., improved loss and pretraining, and interleaved window and global attentions for reducing the ViT encoder complexity. We improve the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduce windowmajor feature map organization for improving the efficiency of interleaved attention computation. Experimental results demonstrate that the proposed approach is superior over existing real-time detectors, e.g., YOLO and its variants, on COCO and other benchmark datasets. Code and models are available at https://github.com/Atten4Vis/LW-DETR.

Keywords: Object Detection $\cdot$ Real-Time $\cdot$ Detection Transformer

# 1 Introduction

Real-time object detection is an important problem in visual recognition and has wide real-world applications. The current dominant solutions are based on convolutional networks, such as the YOLO series $[ 1 , 2 , 2 0 , 2 9 , 3 2 , 3 3 , 4 6 , 5 9 , 6 5 ]$ . Recently, transformer methods, e.g., detection transformer (DETR) [4], have witnessed significant progress $\left| 7 , 1 9 , 4 1 , 4 7 , 6 3 , 6 7 , 7 4 , 7 5 \right|$ . Unfortunately, DETR for real-time detection remains not fully explored, and it is unclear if the performance is comparable to the state-of-the-art convolutional methods.

In this paper, we build a light-weight DETR approach for real-time object detection. The architecture is very simple: a plain ViT encoder [17] and a DETR decoder that are connected by a convolutional projector [29]. We propose to aggregate the multi-level feature maps, the intermediate and final feature maps in the encoder, forming stronger encoded feature maps. Our approach takes advantage of effective training techniques. For example, we use the deformable cross-attention forming the decoder [74], the IoU-aware classification loss [3], and the encoder-decoder pretraining strategy [7, 67, 71].

![](Images_4AGKLBUD/6d8094cae62858be835d1ca44d007c26309ac6251f06e88cf0772d49e3707901.jpg)  
Fig. 1: Our approach outperforms previous SoTA real-time detectors. The x-axis corresponds to the inference time. The y-axis corresponds to the mAP score on COCO val2017. All the models are trained with pretraining on Objects365. The NMS post-processing times are included for other models and measured on the COCO val2017 with the setting from the official implementation [1,29,46], and the well-tuned NMS postprocessing setting (labeled as “\*”).

On the other hand, our approach exploits inference-efficient techniques. For example, we adopt interleaved window and global attentions [36, 37], replacing some global attentions with window attentions in the plain ViT encoder to reduce the complexity. We use an efficient implementation for the interleaved attentions through a window-major feature map organization method, effectively reducing the costly memory permutation operations.

Figure 1 shows that the proposed simple baseline surprisingly outperforms the previous real-time detectors on COCO [39], e.g., YOLO-NAS [1], YOLOv8 [29], and RTMDet [46]. These models are improved with pretraining on Objects365 [54], and the end-to-end time cost, including the NMS time, is measured using the setting from the official implementations6.

We conduct extensive experiments for the comparisons with existing realtime detection algorithms [1,12,29,46,58]. We further optimize the NMS setting and obtain improved performance for existing algorithms. The proposed baseline still outperforms these algorithms (labeled as “\*” in Figure 1). In addition, we demonstrate the proposed approach with experimental results on more detection benchmarks.

The proposed baseline merely explores simple and easily implemented techniques and shows promising performance. We believe that our approach potentially benefits from other designs, such as efficient multi-scale feature fusion [34], token sparsification [36,72], distillation [5,5,9,64], as well as other training techniques, such as the techniques used in YOLO-NAS [1]. We also show that the proposed approach is applicable to the DETR approach with the convolutional encoder, such as ResNet-18 and ResNet-50 [23], and achieves good performance.

# 2 Related Work

Real-time object detection. Real-time object detection has wide real-world applications [18, 25, 30, 31, 49]. Existing state-of-the-art real-time detectors, such as YOLO-NAS [1], YOLOv8 [29], and RTMDet [46], have been largely improved compared with the first version of YOLO [50] through detection frameworks [20, 56], architecture designs [2,16,32,33,59,65], data augmentations [2,20,69], training techniques [1,20,29], and loss functions [38,68,73]. These detectors are based on convolutions. In this paper, we study transformer-based solutions to real-time detection that remains little explored.

ViT for object detection. Vision Transformer (ViT) [17,66] shows promising performance in image classification. Applying ViT to object detection usually exploits window attentions [17, 43] or hierarchical architectures [21, 43, 62, 70] to reduce the memory and computation cost. UViT [8] uses progressive window attention. ViTDet [36] implements the pre-trained plain ViT with interleaved window and global attentions [37]. Our approach follows ViTDet to use interleaved window and global attentions, and additionally uses window-major order feature map organization for reducing the memory permutation cost.

DETR and its variants. Detection Transformer (DETR) is an end-to-end detection method, with removing the necessity of many hand-crafted components, such as anchor generation [51] and non-maximum suppression (NMS) [24]. There are many followup methods for DETR improvement, such as architecture design [19,47,67,74], object query design [11,41,63], training techniques [6,7,27,35, 48,67,75], and loss function improvement [3,42]. Besides, various works have been done for reducing the computational complexity, by architecture design [34, 40], computational optimization [74], pruning [53,72], and distillation [5,5,9,64]. The interest of this paper is to build a simple DETR baseline for real-time detection that are not explored by these methods.

Concurrent with our work, RT-DETR [45] also applies the DETR framework to construct real-time detectors with a focus on the CNN backbone forming the encoder. There are studies about relatively large models and a lack of tiny models. Our LW-DETR explores the feasibility of plain ViT backbones and DETR framework for real-time detection .

![](Images_4AGKLBUD/0bdcb2a8d9e4e902ec3b6717e64b46c2159bef284b6278b1357f5fd3eba494d5.jpg)  
Fig. 2: An example of transformer encoder with multi-level feature map aggregation and interleaved window and global attentions. The FFN and LayerNorm layers are not depicted for clarification.

# 3 LW-DETR

# 3.1 Architecture

LW-DETR consists of a ViT encoder, a projector, and a DETR decoder.

Encoder. We adopt the ViT for the detection encoder. A plain ViT [17] consists of a patchification layer and transformer encoder layers. A transformer encoder layer in the initial ViT contains a global self-attention layer over all the tokens and an FFN layer. The global self-attention is computationally costly, and its time complexity is quadratic with respect to the number of tokens (patches). We implement some transformer encoder layers with window self-attention to reduce the computational complexity (detailed in Sec. 3.4). We propose to aggregate the multi-level feature maps, the intermediate and final feature maps in the encoder, forming stronger encoded feature maps. An example of the encoder is illustrated in Figure 2.

Decoder. The decoder is a stack of transformer decoder layers. Each layer consists of a self-attention, a cross-attention, and an FFN. We adopt deformable cross-attention [74] for computational efficiency. DETR and its variants usually adopt 6 decoder layers. In our implementation, we use 3 transformer decoder layers. This leads to a time reduction from 1.4 ms to 0.7 ms, which is significant compared to the time cost 1.3 ms of the remaining part for the tiny version in our approach.

We adopt a mixed-query selection scheme [67] to form the object queries as an addition of content queries and spatial queries. The content queries are learnable embeddings, which is similar to DETR. The spatial queries are based on a two-stage scheme: selecting top- $K$ features from the last layer in the Projector, predicting the bounding boxes, and transforming the corresponding boxes into embeddings as spatial queries.

Projector. We use a projector to connect the encoder and the decoder. The projector takes the aggregated encoded feature maps from the encoder as the input.

![](Images_4AGKLBUD/b52b3914ce399230b6ac626f3a51bada8206fc392701f13dcb1fd807d718cff1.jpg)  
Fig. 3: Single-scale projector and multi-scale projector for (a) the tiny, small, and medium models, and (b) the large and xlarge models.

The projector is a C2f block (an extension of cross-stage partial DenseNet [26, 60]) that is implemented in YOLOv8 [29].

When forming the large and xlarge version of LW-DETR, we modify the projector to output two-scale ( $\frac { 1 } { 8 }$ and $\frac { 1 } { 3 2 }$ ) feature maps and accordingly use the multi-scale decoder [74]. The projector contains two parallel C2f blocks. One processes $\frac { 1 } { 8 }$ feature maps, which are obtained by upsampling the input through a deconvolution, and the other processes $\frac { 1 } { 3 2 }$ maps that are obtained by downsampling the input through a stride convolution. Figure 3 shows the pipelines of the single-scale projector and the multi-scale projector.

Objective function. We adopt an IoU-aware classification loss, IA-BCE loss [3],

$$
\ell _ { \mathrm { c 1 s } } = \sum _ { i = 1 } ^ { N _ { p o s } } \mathrm { B C E } ( s _ { i } , t _ { i } ) + \sum _ { j = 1 } ^ { N _ { n e g } } s _ { j } ^ { 2 } \mathrm { B C E } ( s _ { j } , 0 ) ,
$$

where $N _ { p o s }$ and $N _ { n e g }$ are the number of positive and negative samples. $s$ is the predicted classification score. $t$ is the target score absorbing the IoU score $u$ (with the ground truth): $t = s ^ { \alpha } u ^ { 1 - \alpha }$ , and $\alpha$ is empirically set as 0.25 [3].

The overall loss is a combination of the classification loss and the bounding box loss that is the same as in the DETR frameworks [4, 67, 74], which is formulated as follows:

$$
\ell _ { \mathsf { c l s } } + \lambda _ { \mathsf { i o u } } \ell _ { \mathsf { i o u } } + \lambda _ { \ell _ { 1 } } \ell _ { 1 } .
$$

where $\lambda _ { \mathrm { i o u } }$ and $\lambda _ { \ell _ { 1 } }$ are set as 2.0 and 5.0 similar to [4, 67, 74]. $\ell _ { \mathrm { i o u } }$ and $\ell _ { 1 }$ are the generalized IoU (GIoU) loss [52] and the L1 loss for the box regression.

Table 1: Architectures of five LW-DETR instances.   

<table><tr><td rowspan="2">LW-DETR</td><td colspan="4">ViT encoder</td><td colspan="3">Projector</td><td colspan="2">DETR decoder</td></tr><tr><td>#layers</td><td>dim</td><td>#global attention</td><td>#window attention</td><td>#blocks</td><td>dim</td><td>scales</td><td>#layers</td><td>#object queries</td></tr><tr><td>tiny</td><td>6</td><td>192</td><td>3</td><td>3</td><td>1</td><td>256</td><td></td><td>3</td><td>100</td></tr><tr><td>small</td><td>10</td><td>192</td><td>4</td><td>6</td><td>1</td><td>256</td><td></td><td>3</td><td>300</td></tr><tr><td>medium</td><td>10</td><td>384</td><td>4</td><td>6</td><td>1</td><td>256</td><td>广花</td><td>3</td><td>300</td></tr><tr><td>large</td><td>10</td><td>384</td><td>4</td><td>6</td><td>1</td><td>384</td><td></td><td>3</td><td>300</td></tr><tr><td>xlarge</td><td>10</td><td>768</td><td>4</td><td>6</td><td>1</td><td>384</td><td>交</td><td>3</td><td>300</td></tr></table>

# 3.2 Instantiation

We instantiate 5 real-time detectors: tiny, small, medium, large, and xlarge.   
The detailed settings are given in Table 1.

The tiny detector consists of a transformer encoder with 6 layers. Each layer consists of a multi-head self-attention module and a feed-forward network (FFN). Each image patch is linear-mapped to a 192-dimensional representation vector. The projector outputs single-scale feature maps with 256 channels. There are 100 object queries for the decoder.

The small detector contains 10 encoder layers, and 300 object queries. Same as the tiny detector, the dimensions for the input patch representation and the output of the projector are 192 and 256. The medium detector is similar to small, and the differences include that the dimension of the input patch representation is 384, and accordingly the dimension for the encoder is 384.

The large detector consists of a 10-layer encoder and uses two-scale feature maps (see the part Projector in Section 3.1). The dimensions for the input patch representation and the output of the projector are 384 and 384. The xlarge detector is similar to large, and the difference is that the dimension of the input patch representation is 768.

# 3.3 Effective Training

More supervision. Various techniques have been developed to introduce more supervision for accelerating the DETR training, e.g., [6,27,75]. We adopt Group DETR [6] that is easily implemented and does not change the inference process. Following [6], we use 13 parallel weight-sharing decoders for training. For each decoder, we generate the object queries for each group from the output features of the projector. Following [6], we use the primary decoder for the inference.

Pretraining on Objects365. The pretraining process consists of two stages. First, we pretrain the ViT on the dataset Objects365 using a MIM method, CAEv2 [71], based on the pretrained models. This leads to a 0.7 mAP gain on COCO.

Second, we follow [7, 67] to retrain the encoder and train the projector and the decoder on Objects365 in a supervision manner.

# 3.4 Efficient Inference

We make a simple modification [36,37] and adopt interleaved window and global attentions: replacing some global self-attention layers with window self-attention layers. For example, in a 6-layer ViT, the first, third, and fifth layers are implemented with window attentions. The window attention is implemented by partitioning the feature map into non-overlapping windows and performing selfattention over each window separately.

Table 2: The influence of effective training and efficient inference techniques. We show empirical results from an initial detector with global attention layers to the final LW-DETR-small model. ‘†’ means we use the ViTDet implementation. The results except the last row are obtained under 45K iterations (equal to 12 epochs). The last row corresponds to the result of the final model with 180K training iterations.   

<table><tr><td>model settings</td><td>#Params (M)</td><td>FLOPs (G)</td><td>Latency (ms)</td><td>mAP</td></tr><tr><td>initial detector</td><td>10.8</td><td>22.8</td><td>3.6</td><td>35.3</td></tr><tr><td>+ multi-level feature aggregation</td><td>11.0</td><td>23.0</td><td>3.7</td><td>36.0</td></tr><tr><td>+ interleaved window and global attention†</td><td>11.0</td><td>16.6</td><td>3.9</td><td>34.7</td></tr><tr><td>+ window-major feature map organization</td><td>11.0</td><td>16.6</td><td>2.9</td><td>34.7</td></tr><tr><td>+ iou-aware classification loss</td><td>11.0</td><td>16.6</td><td>2.9</td><td>35.4</td></tr><tr><td>+ more supervision</td><td>14.6</td><td>16.6</td><td>2.9</td><td>38.4</td></tr><tr><td>+ bounding box reparameterization</td><td>14.6</td><td>16.6</td><td>2.9</td><td>38.6</td></tr><tr><td>+ pretraining on Objects365</td><td>14.6</td><td>16.6</td><td>2.9</td><td>47.3</td></tr><tr><td>LW-DETR-small</td><td>14.6</td><td>16.6</td><td>2.9</td><td>48.0</td></tr></table>

We adopt a window-major feature map organization scheme for efficient interleaved attention, which organizes the feature maps window by window. The ViTDet implementation [36], where feature maps are organized row by row (rowmajor organization), requires costly permutation operations to transition feature maps from a row-major to a window-major organization for window attention. Our implementation removes these operations and thus reduces model latency.

We illustrate the window-major way using a toy example. Given a $4 \times 4$ feature map

$$
\left[ \begin{array} { c } { { f _ { 1 1 } ~ f _ { 1 2 } ~ f _ { 1 3 } ~ f _ { 1 4 } } } \\ { { f _ { 2 1 } ~ f _ { 2 2 } ~ f _ { 2 3 } ~ f _ { 2 4 } } } \\ { { f _ { 3 1 } ~ f _ { 3 2 } ~ f _ { 3 3 } ~ f _ { 3 4 } } } \\ { { f _ { 4 1 } ~ f _ { 4 2 } ~ f _ { 4 3 } ~ f _ { 4 4 } } } \end{array} \right] ,
$$

the window-major organization for a window size $2 \times 2$ is as below:

$$
\begin{array} { r } { f _ { 1 1 } , f _ { 1 2 } , f _ { 2 1 } , f _ { 2 2 } ; f _ { 1 3 } , f _ { 1 4 } , f _ { 2 3 } , f _ { 2 4 } ; } \\ { f _ { 3 1 } , f _ { 3 2 } , f _ { 4 1 } , f _ { 4 2 } ; f _ { 3 3 } , f _ { 3 4 } , f _ { 4 3 } , f _ { 4 4 } . } \end{array}
$$

This organization is applicable to both window attention and global attention without rearranging the features. The row-major organization,

$$
\begin{array} { r l } & { f _ { 1 1 } , f _ { 1 2 } , f _ { 1 3 } , f _ { 1 4 } ; f _ { 2 1 } , f _ { 2 2 } , f _ { 2 3 } , f _ { 2 4 } ; } \\ & { f _ { 3 1 } , f _ { 3 2 } , f _ { 3 3 } , f _ { 3 4 } ; f _ { 4 1 } , f _ { 4 2 } , f _ { 4 3 } , f _ { 4 4 } , } \end{array}
$$

is fine for global attention, and needs to be processed with the costly permutation operation for performing window attention.

# 3.5 Empirical Study

We empirically show how effective training and efficient inference techniques improve the DETR. We use the small detector as the example. The study is based on an initial detector: the encoder is formed with global attention in all the layers and outputs the feature map of the last layer. The results are shown in Table 2.

Latency improvements. The interleaved window and global attention, adopted by ViTDet, reduces the computational complexity from 23.0 GFlops to 16.6 GFlops, validating the benefit of replacing expensive global attention with cheaper window attention. The latency is not reduced and even increased by 0.2 ms. This is because extra costly permutation operations are needed in the row-major feature map organization. Window-major feature map organization alleviates the side effects and leads to a larger latency reduction of 0.8 ms, from 3.7 ms to 2.9 ms.

Performance improvements. Multi-level feature aggregation brings a 0.7 mAP gain. Iou-aware classification loss and more supervision improve the mAP scores from 34.7 to 35.4 and 38.4. Bounding box reparameterization for box regression target [40] (details in the Supplementary Material) makes a slight performance improvement. The significant improvement comes from pretraining on Objects365 and reaches 8.7 mAP, implying that the transformer indeed benefits from large data. A longer training schedule can give further improvements, forming our LW-DETR-small model.

# 4 Experiments

# 4.1 Settings

Datasets. The dataset for pretraining is Objects365 [54]. We follow [7, 67] to combine the images of the train set and the images in the validate set except the first $5 k$ images for detection pretraining. We use the standard COCO2017 [39] data splitting policy and perform the evaluation on COCO val2017.

Data augmentations. We adopt the data augmentations in the DETR and its variants [4, 74]. We follow the real-time detection algorithms [1, 29, 46] and randomly resize the images into squares for training. For evaluating the performance and the inference time, we follow the evaluation scheme used in the real-time detection algorithms [1, 29, 46] to resize the images to $6 4 0 \times 6 4 0$ . We use a window size of $1 0 \times 1 0$ to make sure that the image size can be divisible by the window size.

Implementation details. We pretrain the detection model on Objects365 [54] for 30 epochs and finetune the model on COCO [39] for a total number of 180K training iterations. We adopt the exponential moving average (EMA) technique [55] with a decay of 0.9997. We use the AdamW optimizer [44] for training.

For pretraining, we set the initial learning rate of the projector and the DETR decoder as $4 \times e ^ { - 4 }$ , the initial learning rate of the ViT backbone is $6 \times e ^ { - 4 }$ , and the batch size is 128. For fine-tuning, we set the initial learning rate of the projector and the DETR decoder as $1 \times e ^ { - 4 }$ , and the initial learning rate of the ViT backbone as $1 . 5 \times e ^ { - 4 }$ . We set the batch size as 32 in the tiny, small, and medium models, and the batch size as 16 in the large and xlarge models.

Table 3: Comparisons with state-of-the-art real-time detectors, including RTMDet [46], YOLOv8 [29], and YOLO-NAS [1]. The total latency is evaluated in an end-to-end manner on COCO val2017 and includes the model latency and the postprocessing procedure NMS for non-DETR methods. We measure the total latency in two settings for NMS: official implementation and tuned score threshold. Our LW-DETR does not need NMS and the total latency is equal to the model latency. ‘pretraining’ means the result is based on pretraining on Objects365.

<table><tr><td rowspan="2">Method</td><td rowspan="2">pretraining</td><td rowspan="2">#Params (M)</td><td rowspan="2">FLOPs （G）</td><td rowspan="2">Model Latency (ms)</td><td colspan="2">official implementation</td><td rowspan="2"></td><td colspan="2">tuned score threshold</td></tr><tr><td>Total Latency (ms)</td><td>mAP</td><td>Total Latency (ms）</td><td>mAP</td></tr><tr><td>RTMDet-tiny</td><td></td><td>4.9</td><td>8.1</td><td>2.1</td><td>7.4</td><td>41.0</td><td></td><td>2.4</td><td>40.8</td></tr><tr><td>RTMDet-tiny</td><td>√</td><td>4.9</td><td>8.1</td><td>2.1</td><td>7.4</td><td></td><td>41.7</td><td>2.4</td><td>41.5</td></tr><tr><td>YOLOv8n</td><td></td><td>3.2</td><td>4.4</td><td>1.5</td><td>6.2</td><td></td><td>37.4</td><td>1.6</td><td>37.3</td></tr><tr><td>YOLOv8n</td><td>√</td><td>3.2</td><td>4.4</td><td>1.5</td><td>6.2</td><td>37.6</td><td></td><td>1.6</td><td>37.5</td></tr><tr><td>LW-DETR-tiny</td><td>√</td><td>12.1</td><td>11.2</td><td>2.0</td><td>2.0</td><td>42.6</td><td></td><td>-</td><td>-</td></tr><tr><td>RTMDet-s</td><td></td><td>8.9</td><td>14.8</td><td>2.8</td><td></td><td>7.9</td><td>44.6</td><td>2.9</td><td>44.4</td></tr><tr><td>RTMDet-s</td><td>√</td><td>8.9</td><td>14.8</td><td>2.8</td><td></td><td>7.9</td><td>44.9</td><td>2.9</td><td>44.7</td></tr><tr><td>YOLOv8s</td><td></td><td>11.2</td><td>14.4</td><td>2.6</td><td>7.0</td><td></td><td>45.0</td><td>2.7</td><td>44.8</td></tr><tr><td>YOLOv8s</td><td>√</td><td>11.2</td><td>14.4</td><td>2.6</td><td>7.0</td><td></td><td>45.2</td><td>2.7</td><td>45.1</td></tr><tr><td>YOLO-NAS-s</td><td>√</td><td>19.0</td><td>17.6</td><td>2.8</td><td>4.7</td><td></td><td>47.6</td><td>2.9</td><td>47.3</td></tr><tr><td>LW-DETR-small</td><td>√</td><td>14.6</td><td>16.6</td><td>2.9</td><td></td><td>2.9</td><td>48.0</td><td>-</td><td>-</td></tr><tr><td>RTMDet-m</td><td>√</td><td>24.7</td><td>39.2</td><td>6.2</td><td></td><td>10.8</td><td>49.3</td><td>6.5</td><td>49.1</td></tr><tr><td>RTMDet-m</td><td></td><td>24.7</td><td>39.2</td><td>6.2</td><td>10.8</td><td></td><td>49.7</td><td>6.5</td><td>49.5</td></tr><tr><td>YOLOv8m</td><td></td><td>25.6</td><td>39.7</td><td>5.9</td><td>10.1</td><td></td><td>50.3</td><td>6.0</td><td>50.0</td></tr><tr><td>YOLOv8m</td><td>√</td><td>25.6</td><td>39.7</td><td>5.9</td><td></td><td>10.1</td><td>50.6</td><td>6.0</td><td>50.4</td></tr><tr><td>YOLO-NAS-m</td><td>√</td><td>51.1</td><td>48.0</td><td>5.5</td><td>7.8</td><td></td><td>51.6</td><td>5.7</td><td>51.1</td></tr><tr><td>LW-DETR-medium</td><td>√</td><td>28.2</td><td>42.8</td><td>5.6</td><td>5.6</td><td></td><td>52.5</td><td>-</td><td>-</td></tr><tr><td>RTMDet-1</td><td rowspan="3">√</td><td>52.3</td><td>80.1</td><td>10.3</td><td></td><td>14.9</td><td>51.4</td><td>10.5</td><td>51.2</td></tr><tr><td>RTMDet-1</td><td></td><td>52.3</td><td>80.1</td><td>10.3</td><td>14.9</td><td>52.4</td><td>10.5</td><td>52.2</td></tr><tr><td>YOLOv8l</td><td></td><td>43.7 82.7</td><td></td><td>9.3</td><td>13.2</td><td>53.0</td><td>9.4</td><td>52.5</td></tr><tr><td>YOLOv81</td><td>√</td><td>43.7</td><td>82.7</td><td>9.3</td><td>13.2</td><td>53.3</td><td></td><td>9.4</td><td>53.0</td></tr><tr><td>YOLO-NAS-1</td><td>√</td><td>66.9</td><td>65.5</td><td>7.5</td><td>8.8</td><td></td><td>52.3</td><td>7.6</td><td>51.9</td></tr><tr><td>LW-DETR-large</td><td>√</td><td>46.8</td><td>71.6</td><td>8.8</td><td>8.8</td><td></td><td>56.1</td><td>-</td><td>-</td></tr><tr><td>RTMDet-x</td><td></td><td>94.9</td><td>141.7</td><td>18.4</td><td></td><td>22.8</td><td>52.8</td><td>18.8</td><td>52.5</td></tr><tr><td>RTMDet-x</td><td>√</td><td>94.9</td><td>141.7</td><td>18.4</td><td></td><td>22.8</td><td>54.0</td><td>18.8</td><td>53.5</td></tr><tr><td>YOLOv8x</td><td></td><td>68.2</td><td>129.3</td><td>14.8</td><td>19.1</td><td></td><td>54.0</td><td>15.0</td><td>53.5</td></tr><tr><td>YOLOv8x</td><td>√</td><td>68.2</td><td>129.3</td><td>14.8</td><td>19.1</td><td></td><td>54.5</td><td>15.0</td><td>54.1</td></tr><tr><td>LW-DETR-xlarge</td><td>√</td><td>118.0</td><td>174.2</td><td>19.1</td><td></td><td>19.1</td><td>58.3</td><td>-</td><td>=</td></tr></table>

The number of training iterations 180K is 50 epochs for the tiny, small, and medium models, and 25 epochs for the large and xlarge models. More details, such as weight decay, layer-wise decay in the ViT encoder, and component-wise decay [7] in the fine-tuning process, are given in the Supplementary Material.

We measure the averaged inference latency in an end-to-end manner with fp16 precision and a batch size of 1 on COCO val2017 with a T4 GPU, where the environment settings are with TensorRT-8.6.1, CUDA-11.6, and CuDNN-8.7.0. The efficientNMSPlugin in TensorRT is adopted for efficient NMS implementation. The performance and the end-to-end latency are measured for all real-time detectors using the official implementations.

# 4.2 Results

The results of our five LW-DETR models are reported in Table 3. LW-DETRtiny achieves 42.6 mAP with 500 FPS on a T4 GPU. LW-DETR-small and LW-DETR-medium get 48.0 mAP with over 340 FPS and 52.5 mAP with a speed of over 178 FPS respectively. The large and xlarge models achieve 56.1 mAP with 113 FPS, and 58.3 mAP with 52 FPS.

Comparisons with state-of-the-art real-time detectors. In Table 3, we report the comparison of the LW-DETR models against representative real-time detectors, including YOLO-NAS [1], YOLOv8 [29], and RTMDet [46]. One can see that LW-DETR consistently outperforms previous SoTA real-time detectors with and without using pretraining. Our LW-DETR shows clear superiority over YOLOv8 and RTMDet in terms of latency and detection performance for the five scales from tiny to xlarge.

In comparison to one of previous best methods YOLO-NAS, that is obtained with neural architecture search, our LW-DETR model outperforms it by 0.4 mAP and 0.9 mAP, and runs $1 . 6 \times$ and ${ \sim } 1 . 4 \times$ faster at the small and medium scales. When the model gets larger, the improvement becomes more significant: a 3.8 mAP improvement when running at the same speed at the large scale.

We further improve other methods by well-tuning the classification score threshold in the NMS procedure, and report the results in the right two columns. The results are greatly improved, and still lower than our LW-DETR. We expect that our approach potentially benefits from other improvements, such as neural architecture search (NAS), data augmentation, pseudo-labeled data, and knowledge distillation that are exploited by previous real-time detectors [1, 29, 46].

Comparison with concurrent works. We compare our LW-DETR with concurrent works in real-time detection, YOLO-MS [12], Gold-YOLO [58], RTDETR [45], and YOLOv10 [57]. YOLO-MS improves the performance by enhancing the multi-scale feature representations. Gold-YOLO boosts the multiscale feature fusion and applies MAE-style pretraining [22] to improve the YOLO performance. YOLOv10 designs several efficiency and accuracy driven modules to improve the performance. RT-DETR [45], closely related to LW-DETR, is also built on the DETR framework, with many differences from our approach in the backbone, the projector, the decoder, and the training schemes.

The comparisons are given in Table 4 and Figure 4. Our LW-DETR consistently achieves a better balance between the detection performance and the latency. YOLO-MS and Gold-YOLO clearly show worse results than our LWDETR for all the model scales. LW-DETR-large outperforms the closely related RT-DETR-R50 by 0.8 mAP and shows faster speed (8.8 ms vs. 9.9 ms). LWDETR with other scales also shows better results than RT-DETR. Compared to the latest work, YOLOv10-X [57], our LW-DETR-large achieves higher performance (56.1 mAP vs. 54.4 mAP) with lower latency (8.8 ms vs. 10.70 ms).

# 4.3 Discussions

NMS post-processing. The DETR method is an end-to-end algorithm that does not need the NMS post-processing procedure. In contrast, existing realtime detectors, such as YOLO-NAS [1], YOLOv8 [29], and RTMDet [46], needs NMS [24] post-processing. The NMS procedure takes extra time. We include the extra time for measuring the end-to-end inference cost, which is counted in

Table 4: Comparisons with concurrent works, including YOLO-MS [12], GoldYOLO [58], YOLOv10 [57], and RT-DETR [45] on COCO. For YOLO-MS and GoldYOLO, we measure the total latency in two settings for NMS: official implementation and tuned score threshold. For YOLOv10, we report the results in the official paper [57]. RT-DETR is based on DETR, and the total latency is equal to the model latency. We provide the best latency among the reported inference time in the paper [45] and the measured time in our environment for RT-DETR. LW-DETR consistently gets superior results. ‘pretraining’ means that the results are based on pretraining on Objects365.

<table><tr><td>Method</td><td>pretraining</td><td>#Params (M)</td><td>FLOPs (G)</td><td>Model Latency</td><td>official implementation Total Latency</td><td>mAP</td><td>tuned score threshold Total Latency</td><td>mAP</td></tr><tr><td></td><td rowspan="5">√ √</td><td>4.5</td><td>8.7</td><td>(ms) 3.0</td><td>(ms) 6.9</td><td>43.4</td><td>(ms) 3.2</td><td>43.3</td></tr><tr><td>YOLO-MS-XS YOLO-MS-XS</td><td>4.5</td><td>8.7</td><td>3.0</td><td>6.9</td><td>43.9</td><td>3.2</td><td>43.8</td></tr><tr><td>YOLO-MS-S</td><td>8.1</td><td>15.6</td><td>5.4</td><td>9.2</td><td>46.2</td><td>5.6</td><td>46.1</td></tr><tr><td>YOLO-MS-S</td><td>8.1</td><td>15.6</td><td>5.4</td><td>9.2</td><td>46.8</td><td>5.6</td><td>46.7</td></tr><tr><td>YOLO-MS</td><td>22.0</td><td>40.1</td><td>8.6</td><td>12.3</td><td>51.0</td><td>9.0</td><td>50.8</td></tr><tr><td></td><td rowspan="4">√</td><td></td><td></td><td></td><td></td><td>45.5</td><td>3.4</td><td>45.4</td></tr><tr><td>Gold-YOLO-S Gold-YOLO-S</td><td>21.5 21.5</td><td>23.0 23.0</td><td>2.9</td><td>3.6 3.6</td><td>46.1</td><td>3.4</td><td></td></tr><tr><td>Gold-YOLO-M</td><td>41.3</td><td>43.8</td><td>2.9</td><td>6.3</td><td>50.2</td><td></td><td>46.0</td></tr><tr><td>Gold-YOLO-M</td><td></td><td>43.8</td><td>5.8</td><td></td><td></td><td>6.1</td><td>50.2</td></tr><tr><td>Gold-YOLO-L</td><td>√</td><td>41.3 75.1</td><td>75.9</td><td>5.8 10.2</td><td>6.3 10.6</td><td>50.4 52.3</td><td>6.1</td><td>50.3 52.2</td></tr><tr><td>YOLOv10-N</td><td rowspan="5"></td><td>2.3</td><td>6.7</td><td></td><td>1.84</td><td>38.5</td><td>10.5</td><td></td></tr><tr><td>YOLOv10-S</td><td>7.2</td><td>21.6</td><td>-</td><td>2.49</td><td>46.3</td><td>-</td><td>- =</td></tr><tr><td>YOLOv10-M</td><td>15.4</td><td>59.1</td><td></td><td>4.74</td><td>51.1</td><td></td><td></td></tr><tr><td>YOLOv10-B</td><td>19.1</td><td>92.0</td><td></td><td>5.74</td><td>52.5</td><td></td><td></td></tr><tr><td>YOLOv10-L</td><td>24.4</td><td>120.3</td><td></td><td>7.28</td><td>53.2</td><td></td><td></td></tr><tr><td>YOLOv10-X</td><td></td><td>29.5</td><td>160.4</td><td>=</td><td>10.70</td><td>54.4</td><td></td><td></td></tr><tr><td>RT-DETR-R18</td><td></td><td></td><td>30.0</td><td></td><td></td><td>46.5</td><td>=</td><td></td></tr><tr><td>RT-DETR-R18</td><td rowspan="3">√</td><td>20 20</td><td>30.0</td><td>4.6 4.6</td><td>4.6 4.6</td><td>49.2</td><td>=</td><td></td></tr><tr><td>RT-DETR-R50</td><td>42</td><td>69.4</td><td>9.3</td><td>9.3</td><td>53.1</td><td></td><td></td></tr><tr><td>RT-DETR-R50</td><td>42</td><td>69.4</td><td>9.3</td><td>9.3</td><td>55.3</td><td></td><td></td></tr><tr><td>RT-DETR-R101</td><td>√</td><td>76</td><td>131.0</td><td>13.5</td><td>13.5</td><td></td><td></td><td></td></tr><tr><td>RT-DETR-R101</td><td>√</td><td>76</td><td>131.0</td><td>13.5</td><td>13.5</td><td>54.3 56.2</td><td></td><td></td></tr><tr><td></td><td>√</td><td>12.1</td><td>11.2</td><td>2.0</td><td>2.0</td><td>42.6</td><td>=</td><td></td></tr><tr><td>LW-DETR-tiny LW-DETR-small</td><td>√</td><td>14.6</td><td>16.6</td><td>2.9</td><td>2.9</td><td>48.0</td><td>-</td><td></td></tr><tr><td>LW-DETR-medium</td><td>√</td><td>28.2</td><td>42.8</td><td>5.6</td><td>5.6</td><td>52.5</td><td></td><td></td></tr><tr><td>LW-DETR-large</td><td>√</td><td>46.8</td><td>71.6</td><td>8.8</td><td>8.8</td><td>56.1</td><td></td><td>=</td></tr><tr><td>LW-DETR-xlarge</td><td>√</td><td>118.0</td><td>174.2</td><td>19.1</td><td>19.1</td><td>58.3</td><td>=</td><td></td></tr></table>

real-world application. The results using the NMS setting in the official implementations are shown in Figure 1 and Table 3.

We further make improvements for the methods with NMS by tuning the classification score threshold for the NMS post-processing. We observe that the default score threshold, 0.001, in YOLO-NAS, YOLOv8, and RTMDet, results in a high mAP, but a large number of boxes and thus high latency. In particular, when the model is small, the end-to-end latency is dominated by the NMS latency. We tune the threshold, obtaining a good balance between the mAP score and the latency. It is observed that the mAP scores are slightly dropped, e.g., by $- 0 . 1$ mAP to $- 0 . 5$ mAP, and the running times are largely reduced, e.g., with a reduction of 4∼5 ms for RTMDet and YOLOv8, a reduction of 1∼2 ms for YOLO-NAS. These reductions are from that fewer predicted boxes are fed into NMS after tuning the score threshold. Detailed results with different score thresholds, along with the distribution of the number of remaining boxes across the COCO val2017 are given in the Supplementary Material.

![](Images_4AGKLBUD/3cf7de83e4b036a05413f4e7dd33c7058a5c97e58040761b863e517dd62e16d8.jpg)  
Fig. 4: Our approach outperforms concurrent works. The $_ \mathrm { x }$ -axis corresponds to the inference time. The y-axis corresponds to the mAP score on COCO val2017. Our LW-DETR, RT-DETR [45], YOLO-MS [12], and Gold-YOLO [58] are trained with pretraining on Objects365, while YOLOv10 [57] is not. The NMS post-processing times are included for YOLO-MS and Gold-YOLO, and measured on the COCO val2017 with the setting from the official implementation, and the well-tuned NMS postprocessing setting (labeled as “\*").

Table 5: The effect of pertaining in our LW-DETR. Pretraining on Objects365 improves our approach a lot. This observation is consistent with the observations from methods with large models [7, 67, 75].   

<table><tr><td>LW-DETR</td><td>#Params (M)</td><td>FLOPs (G)</td><td>Latency (ms)</td><td>mAP w/o pretraining</td><td>mAPw/ pretraining</td></tr><tr><td>tiny</td><td>12.1</td><td>11.2</td><td>2.0</td><td>36.5</td><td>42.6</td></tr><tr><td>small</td><td>14.6</td><td>16.6</td><td>2.9</td><td>43.6</td><td>48.0</td></tr><tr><td>medium</td><td>28.2</td><td>42.8</td><td>5.6</td><td>47.2</td><td>52.5</td></tr><tr><td>large</td><td>46.8</td><td>71.6</td><td>8.8</td><td>49.5</td><td>56.1</td></tr><tr><td>xlarge</td><td>118.0</td><td>174.2</td><td>19.1</td><td>53.0</td><td>58.3</td></tr><tr><td>R18</td><td>21.2</td><td>21.4</td><td>2.5</td><td>40.9</td><td>44.4</td></tr><tr><td>R50</td><td>54.6</td><td>67.7</td><td>8.7</td><td>49.7</td><td>54.4</td></tr></table>

Figure 1 shows the comparison against other methods with well-tuned NMS procedures. The methods with NMS are improved. Our approach still outperforms other methods. The second-best approach, YOLO-NAS, is a network architecture search algorithm and performs very closely to the proposed baseline. We believe that the complicated network architecture search procedure, like the one used in YOLO-NAS, potentially benefits the DETR approach, and further improvement is expected.

Pretraining. We empirically study the effect of pretraining. The results, shown in Table 5, indicate that pretraining leads to significant improvements for our approaches, with an average improvement of 5.5 mAP. The tiny model gets an mAP gain of 6.1, and the xlarge model gets an mAP gain of 5.3. This implies that pretraining on a large dataset is highly beneficial for DETR-based models.

Table 6: The effect of pertaining along with training epochs for non-endto-end detectors.   

<table><tr><td rowspan=1 colspan=1>Model</td><td rowspan=1 colspan=1>pretraining</td><td rowspan=1 colspan=1>20 epochs</td><td rowspan=1 colspan=1>60 epochs</td><td rowspan=1 colspan=1>100 epochs</td><td rowspan=1 colspan=1>200 epochs</td><td rowspan=1 colspan=1>300 epochs</td><td rowspan=1 colspan=1>500 epochs</td></tr><tr><td rowspan=2 colspan=1>YOLOv8nYOLOv8n</td><td rowspan=2 colspan=1>√</td><td rowspan=2 colspan=1>26.231.5</td><td rowspan=2 colspan=1>30.132.8</td><td rowspan=2 colspan=1>32.333.2</td><td rowspan=1 colspan=1>34.0</td><td rowspan=1 colspan=1>35.0</td><td rowspan=1 colspan=1>37.4</td></tr><tr><td rowspan=1 colspan=1>34.3</td><td rowspan=1 colspan=1>35.2</td><td rowspan=1 colspan=1>37.6</td></tr><tr><td rowspan=1 colspan=1>RTMDet-t</td><td rowspan=2 colspan=1>√</td><td rowspan=1 colspan=1>30.4</td><td rowspan=1 colspan=1>34.2</td><td rowspan=1 colspan=1>35.2</td><td rowspan=1 colspan=1>36.8</td><td rowspan=1 colspan=1>41.0</td><td rowspan=1 colspan=1>-</td></tr><tr><td rowspan=1 colspan=1>RTMDet-t</td><td rowspan=1 colspan=1>33.4</td><td rowspan=1 colspan=1>36.5</td><td rowspan=1 colspan=1>36.7</td><td rowspan=1 colspan=1>37.5</td><td rowspan=1 colspan=1>41.7</td><td rowspan=1 colspan=1>-</td></tr><tr><td rowspan=1 colspan=1>YOLO-MS-XS</td><td rowspan=2 colspan=1>√</td><td rowspan=1 colspan=1>24.7</td><td rowspan=1 colspan=1>34.3</td><td rowspan=1 colspan=1>36.4</td><td rowspan=1 colspan=1>39.2</td><td rowspan=1 colspan=1>43.4</td><td rowspan=2 colspan=1>==</td></tr><tr><td rowspan=1 colspan=1>YOLO-MS-XS</td><td rowspan=1 colspan=1>37.5</td><td rowspan=1 colspan=1>38.6</td><td rowspan=1 colspan=1>39.1</td><td rowspan=1 colspan=1>40.0</td><td rowspan=1 colspan=1>43.9</td></tr><tr><td rowspan=1 colspan=1>Gold-YOLO-S</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>33.4</td><td rowspan=1 colspan=1>37.1</td><td rowspan=1 colspan=1>38.2</td><td rowspan=1 colspan=1>43.6</td><td rowspan=1 colspan=1>45.5</td><td rowspan=1 colspan=1>=</td></tr><tr><td rowspan=1 colspan=1>Gold-YOLO-S</td><td rowspan=1 colspan=1>√</td><td rowspan=1 colspan=1>39.3</td><td rowspan=1 colspan=1>41.3</td><td rowspan=1 colspan=1>42.1</td><td rowspan=1 colspan=1>44.9</td><td rowspan=1 colspan=1>46.1</td><td rowspan=1 colspan=1>=</td></tr></table>

We further show that the training procedure applies to the DETR approach with convolutional encoders. We replace transformer encoders with ResNet-18 and ResNet-50. One can see that in Table 5, the results of these LW-DETR variants are close to LW-DETR with transformer encoders in terms of latency and mAP, and the pretraining brings benefits that are similar to and a little lower than LW-DETR with transformer encoders.

Meanwhile, we investigate the pretraining improvements on non-end-to-end detectors. According to the results in Table 3, Table 4 and Table 6, it seems that pretraining on Objects365 only show limited gains for non-end-to-end detectors [12, 29, 46, 58], which is different from the phenomenon in DETR-based detectors, where pretraining give large improvements. As the non-end-to-end detectors train 300 epochs even 500 epochs in YOLOv8, we wonder if the limited gain is related to the training epochs. We compare the improvements brought by the pretrained weights along with the training epochs. Table 6 shows that the improvements diminished along with the training epochs, which partly supports the above hypothesis. The above illustration is a preliminary step. We believe that more investigations are needed to figure out the underlying reasons for the difference in benefits of pretraining.

# 4.4 Experiments on more datasets

We test the generalizability of our LW-DETR on more detection datasets. We consider two types of evaluation methods, cross-domain evaluation and multidomain finetuning. For cross-domain evaluation, we directly evaluate the realtime detectors trained on COCO on the Unidentified Video Objects (UVO) [61]. For multi-domain fine-tuning, we finetune the pretrained real-time detectors on the multi-domain detection dataset Roboflow 100 (RF100) [13]. We do a coarse search on the hyperparameters on each dataset for all models, such as the learning rate. Please refer to the Supplementary Material for more details.

Cross-domain evaluation. One possible way to evaluate the generalizability of the models is to directly evaluate them on the datasets with different domains. We adopt a class-agnostic object detection benchmark, UVO [61], where 57% object instances do not belong to any of the 80 COCO classes. UVO is based on YouTube videos, whose appearance is very different from COCO, e.g., some

Table 7: Cross-domain evaluation on UVO. We evaluate the performance in a class-agnostic way as UVO is class-agnostic. LW-DETR demonstrates higher AP and AR than other detectors.   

<table><tr><td>Method</td><td>mAP</td><td>AP50</td><td>AR@100</td><td>ARs</td><td>ARm</td><td>AR</td></tr><tr><td>RTMDet-s</td><td>29.7</td><td>43.3</td><td>55.7</td><td>26.5</td><td>49.4</td><td>71.5</td></tr><tr><td>YOLOv8-s</td><td>29.1</td><td>42.4</td><td>54.3</td><td>27.4</td><td>48.6</td><td>68.8</td></tr><tr><td>YOLO-NAS-S</td><td>31.0</td><td>44.5</td><td>55.1</td><td>25.8</td><td>48.1</td><td>71.6</td></tr><tr><td>LW-DETR-small</td><td>32.3</td><td>45.1</td><td>59.8</td><td>29.4</td><td>52.4</td><td>77.1</td></tr></table>

Table 8: Multi-domain finetuning on RF100. We compare our LW-DETR with previous real-time detectors, including YOLOv5, YOLOv7, RTMDet, YOLOv8, and YOLO-NAS on all data domains of RF100. Gray entries are the results from the RF100 paper [13]. The data domains are aerial, videogames, microscopic, underwater, documents, electromagnetic, and real world. The AP50 metric is used. ‘-’ means that YOLO-NAS [1] does not report their detailed results.

<table><tr><td rowspan="2">Method</td><td rowspan="2">Average</td><td colspan="7">Domains in Roboflow 100</td></tr><tr><td>aerial</td><td>videogames</td><td>microscopic</td><td>underwater</td><td>documents</td><td>electromagnetic</td><td>real world</td></tr><tr><td>YOLOv5-s</td><td>73.4</td><td>63.6</td><td>85.9</td><td>65.0</td><td>56.0</td><td>71.6</td><td>74.2</td><td>76.9</td></tr><tr><td>YOLOv7-s</td><td>67.4</td><td>50.4</td><td>79.6</td><td>59.1</td><td>66.2</td><td>72.2</td><td>63.9</td><td>70.5</td></tr><tr><td>RTMDet-s</td><td>79.2</td><td>70.1</td><td>88.0</td><td>68.1</td><td>68.0</td><td>81.0</td><td>77.4</td><td>83.3</td></tr><tr><td>YOLOv8-s</td><td>80.1</td><td>70.7</td><td>87.9</td><td>74.7</td><td>70.2</td><td>79.8</td><td>79.0</td><td>82.9</td></tr><tr><td>YOLO-NAS-s</td><td>81.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>=</td><td>-</td></tr><tr><td>YOLO-NAS-m</td><td>81.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>=</td></tr><tr><td>LW-DETR-small</td><td>82.5</td><td>71.8</td><td>88.9</td><td>74.5</td><td>69.6</td><td>86.7</td><td>84.6</td><td>85.3</td></tr><tr><td>LW-DETR-medium</td><td>83.5</td><td>72.9</td><td>90.8</td><td>75.4</td><td>70.5</td><td>86.1</td><td>86.2</td><td>86.4</td></tr></table>

videos are in egocentric views and have significant motion blur. We evaluate the models trained with COCO (taken from Table 3) on the validation split of UVO.

Table 7 provides the results. LW-DETR excels over competing SoTA realtime detectors. Specifically, LW-DETR-small is 1.3 mAP and 4.1 AR higher than the best result among RTMDet-s, YOLOv8-s, and YOLO-NAS-s. In terms of recall, it also shows enhanced abilities to detect more objects across different scales: small, medium, and large. The above findings imply that the superiority of our LW-DETR over previous real-time detectors is attributed not to specific tuning for COCO, but to its capacity for producing more generalizable models.

Multi-domain finetuning. Another way is to finetune the pretrained detectors on small datasets across different domains. RF100 consists of 100 small datasets, 7 imagery domains, 224k images, and 829 class labels. It can help researchers test the model’s generalizability with real-life data. We finetune the real-time detectors on each small dataset of RF100.

The results are given in Table 8. LW-DETR-small shows superiority over current state-of-the-art real-time detectors across different domains. In particular, for the ‘documents’ and the ‘electromagnetic’ domains, our LW-DETR is significantly better than YOLOv5, YOLOv7, RTMDet, and YOLOv8 (5.7 AP and 5.6 AP higher than the best among the four). LW-DETR-medium can give further improvements overall. These findings highlight the versatility of our LWDETR, positioning it as a strong baseline in a range of closed-domain tasks.

# 5 Limitation and future works

Currently, we only demonstrate the effectiveness of LW-DETR in real-time detection. This is the first step. Extending LW-DETR for open-world detection and applying LW-DETR to more vision tasks, such as multi-person pose estimation and multi-view 3D object detection, needs more investigation. We leave them for future work.

# 6 Conclusion

This paper shows that detection transformers achieve competitive and even superior results over existing real-time detectors. Our method is simple and efficient. The success stems from multi-level feature aggregation and training-effective and inference-efficient techniques. We hope our experience can provide insights for building real-time models with transformers in vision tasks.

# References

1. Aharon, S., Louis-Dupont, Ofri Masad, Yurkova, K., Lotem Fridman, Lkdci, Khvedchenya, E., Rubin, R., Bagrov, N., Tymchenko, B., Keren, T., Zhilko, A., Eran-Deci: Super-gradients (2021). https://doi.org/10.5281/ZENODO.7789328, https://zenodo.org/record/7789328   
2. Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934 (2020)   
3. Cai, Z., Liu, S., Wang, G., Ge, Z., Zhang, X., Huang, D.: Align-detr: Improving detr with simple iou-aware bce loss. arXiv preprint arXiv:2304.07527 (2023)   
4. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: Endto-end object detection with transformers. In: European conference on computer vision. pp. 213–229. Springer (2020)   
5. Chang, J., Wang, S., Xu, H.M., Chen, Z., Yang, C., Zhao, F.: Detrdistill: A universal knowledge distillation framework for detr-families. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6898–6908 (2023)   
6. Chen, Q., Chen, X., Wang, J., Zhang, S., Yao, K., Feng, H., Han, J., Ding, E., Zeng, G., Wang, J.: Group detr: Fast detr training with group-wise one-to-many assignment. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2023)   
7. Chen, Q., Wang, J., Han, C., Zhang, S., Li, Z., Chen, X., Chen, J., Wang, X., Han, S., Zhang, G., et al.: Group detr v2: Strong object detector with encoder-decoder pretraining. arXiv preprint arXiv:2211.03594 (2022)   
8. Chen, W., Du, X., Yang, F., Beyer, L., Zhai, X., Lin, T.Y., Chen, H., Li, J., Song, X., Wang, Z., et al.: A simple single-scale vision transformer for object localization and instance segmentation. arXiv preprint arXiv:2112.09747 (2021)   
9. Chen, X., Chen, J., Liu, Y., Zeng, G.: D $^ 3$ etr: Decoder distillation for detection transformer. arXiv preprint arXiv:2211.09768 (2022)   
10. Chen, X., Ding, M., Wang, X., Xin, Y., Mo, S., Wang, Y., Han, S., Luo, P., Zeng, G., Wang, J.: Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026 (2022)   
11. Chen, X., Wei, F., Zeng, G., Wang, J.: Conditional detr v2: Efficient detection transformer with box queries. arXiv preprint arXiv:2207.08914 (2022)   
12. Chen, Y., Yuan, X., Wu, R., Wang, J., Hou, Q., Cheng, M.M.: Yolo-ms: Rethinking multi-scale representation learning for real-time object detection. arXiv preprint arXiv:2308.05480 (2023)   
13. Ciaglia, F., Zuppichini, F.S., Guerrie, P., McQuade, M., Solawetz, J.: Roboflow 100: A rich, multi-domain object detection benchmark. arXiv preprint arXiv:2211.13523 (2022)   
14. Clark, K., Luong, M.T., Le, Q.V., Manning, C.D.: Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555 (2020)   
15. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A largescale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248–255. Ieee (2009)   
16. Ding, X., Zhang, X., Ma, N., Han, J., Ding, G., Sun, J.: Repvgg: Making vgg-style convnets great again. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 13733–13742 (2021)   
17. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)   
18. Feng, D., Haase-Schütz, C., Rosenbaum, L., Hertlein, H., Glaeser, C., Timm, F., Wiesbeck, W., Dietmayer, K.: Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges. IEEE Transactions on Intelligent Transportation Systems 22(3), 1341–1360 (2020)   
19. Gao, Z., Wang, L., Han, B., Guo, S.: Adamixer: A fast-converging query-based object detector. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5364–5373 (2022)   
20. Ge, Z., Liu, S., Wang, F., Li, Z., Sun, J.: Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430 (2021)   
21. Hatamizadeh, A., Heinrich, G., Yin, H., Tao, A., Alvarez, J.M., Kautz, J., Molchanov, P.: Fastervit: Fast vision transformers with hierarchical attention. arXiv preprint arXiv:2306.06189 (2023)   
22. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16000–16009 (2022)   
23. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770–778 (2016)   
24. Hosang, J., Benenson, R., Schiele, B.: Learning non-maximum suppression. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4507–4515 (2017)   
25. Hu, Y., Yang, J., Chen, L., Li, K., Sima, C., Zhu, X., Chai, S., Du, S., Lin, T., Wang, W., et al.: Planning-oriented autonomous driving. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 17853– 17862 (2023)   
26. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4700–4708 (2017)   
27. Jia, D., Yuan, Y., He, H., Wu, X., Yu, H., Lin, W., Sun, L., Zhang, C., Hu, H.: Detrs with hybrid matching. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19702–19712 (2023)   
28. Jocher, G.: Ultralytics yolov5 (2020), https://github.com/ultralytics/yolov5   
29. Jocher, G., Chaurasia, A., Qiu, J.: Ultralytics yolov8 (2023), https://github. com/ultralytics/ultralytics   
30. Karaoguz, H., Jensfelt, P.: Object detection approach for robot grasp detection. In: 2019 International Conference on Robotics and Automation (ICRA). pp. 4953– 4959. IEEE (2019)   
31. Li, B., Ouyang, W., Sheng, L., Zeng, X., Wang, X.: Gs3d: An efficient 3d object detection framework for autonomous driving. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1019–1028 (2019)   
32. Li, C., Li, L., Geng, Y., Jiang, H., Cheng, M., Zhang, B., Ke, Z., Xu, X., Chu, X.: Yolov6 v3. 0: A full-scale reloading. arXiv preprint arXiv:2301.05586 (2023)   
33. Li, C., Li, L., Jiang, H., Weng, K., Geng, Y., Li, L., Ke, Z., Li, Q., Cheng, M., Nie, W., et al.: Yolov6: A single-stage object detection framework for industrial applications. arXiv preprint arXiv:2209.02976 (2022)   
34. Li, F., Zeng, A., Liu, S., Zhang, H., Li, H., Zhang, L., Ni, L.M.: Lite detr: An interleaved multi-scale encoder for efficient detr. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18558–18567 (2023)   
35. Li, F., Zhang, H., Liu, S., Guo, J., Ni, L.M., Zhang, L.: Dn-detr: Accelerate detr training by introducing query denoising. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13619–13627 (2022)   
36. Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: European Conference on Computer Vision. pp. 280–296. Springer (2022)   
37. Li, Y., Xie, S., Chen, X., Dollar, P., He, K., Girshick, R.: Benchmarking detection transfer learning with vision transformers. arXiv preprint arXiv:2111.11429 (2021)   
38. Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P.: Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision. pp. 2980–2988 (2017)   
39. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision– ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740–755. Springer (2014)   
40. Lin, Y., Yuan, Y., Zhang, Z., Li, C., Zheng, N., Hu, H.: Detr doesn’t need multiscale or locality design. arXiv preprint arXiv:2308.01904 (2023)   
41. Liu, S., Li, F., Zhang, H., Yang, X., Qi, X., Su, H., Zhu, J., Zhang, L.: Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329 (2022)   
42. Liu, S., Ren, T., Chen, J., Zeng, Z., Zhang, H., Li, F., Li, H., Huang, J., Su, H., Zhu, J., et al.: Detection transformer with stable matching. arXiv preprint arXiv:2304.04742 (2023)   
43. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10012–10022 (2021)   
44. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)   
45. Lv, W., Xu, S., Zhao, Y., Wang, G., Wei, J., Cui, C., Du, Y., Dang, Q., Liu, Y.: Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069 (2023)   
46. Lyu, C., Zhang, W., Huang, H., Zhou, Y., Wang, Y., Liu, Y., Zhang, S., Chen, K.: Rtmdet: An empirical study of designing real-time object detectors. arXiv preprint arXiv:2212.07784 (2022)   
47. Meng, D., Chen, X., Fan, Z., Zeng, G., Li, H., Yuan, Y., Sun, L., Wang, J.: Conditional detr for fast training convergence. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3651–3660 (2021)   
48. Ouyang-Zhang, J., Cho, J.H., Zhou, X., Krähenbühl, P.: Nms strikes back. arXiv preprint arXiv:2212.06137 (2022)   
49. Paul, S.K., Chowdhury, M.T., Nicolescu, M., Nicolescu, M., Feil-Seifer, D.: Object detection and pose estimation from rgb and depth data for real-time, adaptive robotic grasping. In: Advances in Computer Vision and Computational Biology: Proceedings from IPCV’20, HIMS’20, BIOCOMP’20, and BIOENG’20, pp. 121– 142. Springer (2021)   
50. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 779–788 (2016)   
51. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems 28 (2015)   
52. Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S.: Generalized intersection over union: A metric and a loss for bounding box regression. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 658–666 (2019)   
53. Roh, B., Shin, J., Shin, W., Kim, S.: Sparse detr: Efficient end-to-end object detection with learnable sparsity. arXiv preprint arXiv:2111.14330 (2021)   
54. Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J., Sun, J.: Objects365: A large-scale, high-quality dataset for object detection. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 8430–8439 (2019)   
55. Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems 30 (2017)   
56. Tian, Z., Shen, C., Chen, H., He, T.: Fcos: Fully convolutional one-stage object detection. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9627–9636 (2019)   
57. Wang, A., Chen, H., Liu, L., Chen, K., Lin, Z., Han, J., Ding, G.: Yolov10: Realtime end-to-end object detection. arXiv preprint arXiv:2405.14458 (2024)   
58. Wang, C., He, W., Nie, Y., Guo, J., Liu, C., Han, K., Wang, Y.: Gold-yolo: Efficient object detector via gather-and-distribute mechanism. arXiv preprint arXiv:2309.11331 (2023)   
59. Wang, C.Y., Bochkovskiy, A., Liao, H.Y.M.: Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7464– 7475 (2023)   
60. Wang, C.Y., Liao, H.Y.M., Wu, Y.H., Chen, P.Y., Hsieh, J.W., Yeh, I.H.: Cspnet: A new backbone that can enhance learning capability of cnn. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp. 390–391 (2020)   
61. Wang, W., Feiszli, M., Wang, H., Tran, D.: Unidentified video objects: A benchmark for dense, open-world segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10776–10785 (2021)   
62. Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 568–578 (2021)   
63. Wang, Y., Zhang, X., Yang, T., Sun, J.: Anchor detr: Query design for transformerbased detector. In: Proceedings of the AAAI conference on artificial intelligence. vol. 36, pp. 2567–2575 (2022)   
64. Wang, Y., Li, X., Wen, S., Yang, F., Zhang, W., Zhang, G., Feng, H., Han, J., Ding, E.: Knowledge distillation for detection transformer with consistent distillation points sampling. arXiv preprint arXiv:2211.08071 (2022)   
65. Xu, S., Wang, X., Lv, W., Chang, Q., Cui, C., Deng, K., Wang, G., Dang, Q., Wei, S., Du, Y., et al.: Pp-yoloe: An evolved version of yolo. arXiv preprint arXiv:2203.16250 (2022)   
66. Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling vision transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12104–12113 (2022)   
67. Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L.M., Shum, H.Y.: Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605 (2022)   
68. Zhang, H., Wang, Y., Dayoub, F., Sunderhauf, N.: Varifocalnet: An iou-aware dense object detector. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8514–8523 (2021)   
69. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017)   
70. Zhang, X., Tian, Y., Huang, W., Ye, Q., Dai, Q., Xie, L., Tian, Q.: Hivit: Hierarchical vision transformer meets masked image modeling. arXiv preprint arXiv:2205.14949 (2022)   
71. Zhang, X., Chen, J., Yuan, J., Chen, Q., Wang, J., Wang, X., Han, S., Chen, X., Pi, J., Yao, K., Han, J., Ding, E., Wang, J.: CAE v2: Context autoencoder with CLIP latent alignment. Transactions on Machine Learning Research (2023)   
72. Zheng, D., Dong, W., Hu, H., Chen, X., Wang, Y.: Less is more: Focus attention for efficient detr. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6674–6683 (2023)   
73. Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., Ren, D.: Distance-iou loss: Faster and better learning for bounding box regression. In: Proceedings of the AAAI conference on artificial intelligence. vol. 34, pp. 12993–13000 (2020)   
74. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020)   
75. Zong, Z., Song, G., Liu, Y.: Detrs with collaborative hybrid assignments training. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6748–6758 (2023)

# Supplementary Material

# A Experimental Details

This section includes details on the hyper-parameters of pretraining on Objects365 [54], finetuning on COCO [39], and finetuning on Roboflow 100 [13], on the architectures of convolutional encoders, and on the modeling of box regression. We represent the tiny/small/medium/large/xlarge versions of our LW-DETR with T/S/M/L/X in the tables for neat representations.

# A.1 Experimental settings

Pretraining settings. The default settings are in Table 9. We do not use the learning rate drop schedule and keep the initial learning rate along with the training process. When performing window attention in the ViT encoder, we fix the number of windows as 16 for different image resolutions for easy implementation. We use layer-wise lr decay [14] following previous MIM methods [10,22,71].

Table 9: Pretraining settings.   

<table><tr><td>Setting</td><td>Value</td></tr><tr><td>optimizer</td><td>AdamW</td></tr><tr><td>base learning rate</td><td>4.0×e-4</td></tr><tr><td>encoder learning rate</td><td>6.0×e-4</td></tr><tr><td>weight decay</td><td>1×e-4</td></tr><tr><td>batch size</td><td>128</td></tr><tr><td>epochs</td><td>30</td></tr><tr><td>training images resolutions</td><td>[448,512,576,640,704,768,832,896]</td></tr><tr><td>encoder layer-wise lr decay</td><td>0.8 (T/S),0.7 (M/L),0.75 (X)</td></tr><tr><td>number of encoder layers</td><td>6 (T),10(S/M/L/X)</td></tr><tr><td>drop path</td><td>0 (T/S/M),0.05 (L/X)</td></tr><tr><td>window numbers</td><td>16</td></tr><tr><td>window attention indexes</td><td>[0,2,4](T),[0,1,3,6,7,9] (S/M/L/X)</td></tr><tr><td>output feature indexes</td><td>[0,2,4](T),[2,4,5,9] (S/M/L/X)</td></tr><tr><td>feature scales</td><td>1（T/S/M）,1,32|(L/X) 1</td></tr><tr><td>number of object queries</td><td>100 (T),300 (S/M/L/X)</td></tr><tr><td>number of decoder layers</td><td>3</td></tr><tr><td>hidden dimensions</td><td>256 (T/S/M),384 (L/X)</td></tr><tr><td>decoder self-attention heads</td><td>8 (T/S/M),12 (L/X)</td></tr><tr><td>decoder cross-attention heads</td><td>16 (T/S/M),24 (L/X)</td></tr><tr><td>decoder sampling points</td><td>2 (T/S/M),4(L/X)</td></tr><tr><td>group detr</td><td>13</td></tr><tr><td>ema decay</td><td>0.997</td></tr></table>

COCO experimental settings. Most of the settings follow the ones in the pretraining stage. We share the modifications of settings in Table 10. When finetuning LW-DETR on COCO, we use component-wise lr decay [7], which gives different scale factors for the learning rate in the ViT encoder, the Projector, and the DETR decoder. For example, the component-wise lr decay is 0.7 means that we set the lr scale factor as $0 . 7 ^ { 0 }$ for the prediction heads, $0 . 7 ^ { 1 }$ for the transformer decoder layers in DETR decoder, $0 . 7 ^ { 2 }$ for the Projector, and $0 . 7 ^ { 3 }$ for the ViT encoder.

Table 10: COCO experimental settings.   

<table><tr><td>Setting</td><td>Value</td></tr><tr><td>base learning rate</td><td>1.0×e-4</td></tr><tr><td>encoder learning rate</td><td>1.5×e-4</td></tr><tr><td>weight decay</td><td>1 × e-4(T/S/M/L),1×e-3 (X)</td></tr><tr><td>batch size</td><td>32 (T/S/M),16 (L/X)</td></tr><tr><td>epochs</td><td>50 (T/S/M),25 (L/X)</td></tr><tr><td>drop path</td><td>0 (T/S/M),0.1 (L/X)</td></tr><tr><td>component-wise lr decay</td><td>0.7 (T/S/M),0.5 (L/X)</td></tr></table>

Table 11: Roboflow 100 experimental settings.   

<table><tr><td>Setting</td><td>Value</td></tr><tr><td>base learning rate</td><td>8.0 ×e-4 (S),3.0×e-4(M)</td></tr><tr><td>batch size</td><td>16 (S/M)</td></tr><tr><td>encoder learning rate</td><td>1.2 ×e-3 (S),4.5 × e-4 (M)</td></tr><tr><td>encoder layer-wise lr decay</td><td>0.9 (S),0.8 (M)</td></tr><tr><td>component-wise lr decay</td><td>0.7 (S),0.9 (M)</td></tr></table>

Roboflow 100 experimental settings. Roboflow 100 [13] consists of 100 small datasets. We finetune our LW-DETR on these datasets based on the pretrained model on Objects365 [54]. As the training images are insufficient, we set the batch size as 16 and finetune the model for 100 epochs following [13] on all small datasets to make sure to have sufficient training iterations.

We tune the learning rate, encoder layer-wise lr decay, and component-wise lr decay (as shown in Table 11), we do a coarse search on the ‘microscopic’ domain and fix these hyper-parameters for other datasets. The other hyper-parameters are kept same with the finetuning experiments on COCO. We also perform the same processes for RTMDet [46] and YOLOv8 [29] for fair comparisons.

# A.2 Settings for convolutional encoders

We also explore convolutional encoders, ResNet-18 and ResNet-50, in our LWDETR. We load the ImageNet [15] pretrained encoder weights from the RTDETR repo7. Instead of directly outputting multi-level feature maps with scales of $[ \frac { 1 } { 8 } , \frac { 1 } { 1 6 } , \frac { 1 } { 3 2 } ]$ , we make a simple modification to only output a feature map in $\textstyle { \frac { 1 } { 1 6 } }$ We first upsample the feature map in $\frac { 1 } { 3 2 }$ scale to $\textstyle { \frac { 1 } { 1 6 } }$ , downsample the feature map in $\frac { 1 } { 8 }$ to $\textstyle { \frac { 1 } { 1 6 } }$ , and then concatenates all the feature maps. We add additional convolution layers to reduce the feature dimension to prevent the final concatenated feature map from having extremely large feature dimensions.

# A.3 Box regression target reparameterization

Box regression target parameterization is a widely used technique in two-stage and one-stage detectors [2, 28, 38, 51], which predicts the parameters for a box transformation that transforms an input proposal into a predicted box. We follow Plain DETR [40] to use this technique in our LW-DETR.

For box regression in the first stage and each decoder layer, we predict four parameters $[ \delta _ { x } , \delta _ { y } , \delta _ { w } , \delta _ { h } ]$ to a transformation, which transforms a proposal

Table 12: Tuning score threshold for non-end-to-end detectors. We show how the score threshold affects the time of NMS and the detection performance in YOLO-NAS, YOLOv8, RTMDet, YOLO-MS, and Gold-YOLO. We share the detection performance and total latency under three different score thresholds. The first score threshold is the default one in the official implementations. The score threshold in bold represents a good balance between the mAP score and the NMS latency.

<table><tr><td>Model</td><td>Model Latency (ms)</td><td>mAP</td><td>Total Latency (ms)</td><td>mAP</td><td>Total Latency (ms)</td><td>mAP</td><td>Total Latency (ms)</td></tr><tr><td colspan="2">Thresholds</td><td></td><td>score = 0.018</td><td></td><td>score: =0.1</td><td></td><td>score = 0.15</td></tr><tr><td>YOLO-NAS-s</td><td>2.75</td><td>47.6</td><td>4.68</td><td>47.3</td><td>2.88</td><td>46.7</td><td>2.82</td></tr><tr><td>YOLO-NAS-m</td><td>5.52</td><td>51.6</td><td>7.76</td><td>51.1</td><td>5.70</td><td>50.6</td><td>5.53</td></tr><tr><td>YOLO-NAS-1</td><td>7.49</td><td>52.3</td><td>8.84</td><td>51.9</td><td>7.64</td><td>51.2</td><td>7.52</td></tr><tr><td colspan="2">Thresholds</td><td>score =</td><td>0.001</td><td colspan="2">score :0.01</td><td colspan="2">score = 0.05</td></tr><tr><td>YOLOv8n</td><td>1.51</td><td>37.4</td><td>6.21</td><td>37.3</td><td>1.62</td><td>36.0</td><td>1.54</td></tr><tr><td>YOLOv8s</td><td>2.64</td><td>45.0</td><td>7.00</td><td>44.8</td><td>2.71</td><td>43.7</td><td>2.70</td></tr><tr><td>YOLOv8m</td><td>5.90</td><td>50.3</td><td>10.11</td><td>50.0</td><td>6.06</td><td>49.0</td><td>5.92</td></tr><tr><td>YOLOv8l</td><td>9.30</td><td>53.0</td><td>13.16</td><td>52.5</td><td>9.44</td><td>51.3</td><td>9.31</td></tr><tr><td>YOLOv8x</td><td>14.88</td><td>54.0</td><td>19.17</td><td>53.5</td><td>15.09</td><td>52.3</td><td>14.91</td></tr><tr><td colspan="2">Thresholds</td><td colspan="2">score =</td><td colspan="2">score</td><td colspan="2">score =0.25</td></tr><tr><td>RTMDet-t</td><td>2.16</td><td>41.0</td><td>7.41</td><td>40.8</td><td>2.45</td><td>39.1</td><td>2.37</td></tr><tr><td>RTMDet-s</td><td>2.88</td><td>44.6</td><td>7.88</td><td>44.4</td><td>2.94</td><td>42.6</td><td>2.93</td></tr><tr><td>RTMDet-m</td><td>6.27</td><td>49.3</td><td>10.82</td><td>49.1</td><td>6.52</td><td>47.2</td><td>6.31</td></tr><tr><td>RTMDet-1</td><td>10.37</td><td>51.4</td><td>14.84</td><td>51.2</td><td>10.54</td><td>49.2</td><td>10.48</td></tr><tr><td>RTMDet-x</td><td>18.44</td><td>52.8</td><td>22.81</td><td>52.5</td><td>18.88</td><td>50.5</td><td>18.69</td></tr><tr><td colspan="2">Thresholds</td><td colspan="2">score =</td><td></td><td>score =0.1</td><td colspan="2">score =0.25</td></tr><tr><td>YOLO-MS-XS</td><td>3.02</td><td>43.4</td><td>6.99</td><td>43.3</td><td>3.26</td><td>41.9</td><td>3.19</td></tr><tr><td>YOLO-MS-S</td><td>5.40</td><td>46.2</td><td>9.18</td><td>46.1</td><td>5.62</td><td>44.2</td><td>5.56</td></tr><tr><td>YOLO-MS</td><td>8.56</td><td>51.0</td><td>12.38</td><td>50.8</td><td>9.09</td><td>48.8</td><td>8.87</td></tr><tr><td colspan="2">Thresholds</td><td colspan="2">score</td><td colspan="2">score</td><td colspan="2">score = ：0.25</td></tr><tr><td>Gold-YOLO-S</td><td>2.94</td><td>45.5</td><td>3.63</td><td>45.4</td><td>3.35</td><td>43.1</td><td>3.08</td></tr><tr><td>Gold-YOLO-M</td><td>5.84</td><td>50.2</td><td>6.34</td><td>50.2</td><td>6.14</td><td>47.6</td><td>5.98</td></tr><tr><td>Gold-YOLO-L</td><td>10.15</td><td>52.3</td><td>10.58</td><td>52.2</td><td>10.46</td><td>50.5</td><td>10.22</td></tr></table>

$[ p _ { c _ { x } } , p _ { c _ { y } } , p _ { w } , p _ { h } ]$ to a predicted bounding box $[ b _ { c _ { x } } , b _ { c _ { y } } , b _ { w } , b _ { h } ]$ by applying:

$$
\begin{array} { r l r } & { } & { b _ { c _ { x } } = \delta _ { x } * p _ { w } + p _ { c _ { x } } , b _ { c _ { y } } = \delta _ { y } * p _ { h } + p _ { c _ { y } } , } \\ & { } & { b _ { w } = \exp ( \delta _ { w } ) * p _ { w } , b _ { h } = \exp ( \delta _ { h } ) * p _ { h } . } \end{array}
$$

The predicted box $[ b _ { c _ { x } } , b _ { c _ { y } } , b _ { w } , b _ { h } ]$ is used for calculating the box regression losses and for output.

# B Analysis on NMS

Tuning score threshold. The score threshold in non-end-to-end detectors, decides the number of predicted boxes that are passed to the NMS, which largely affects the NMS latency. Table 12 verifies it with YOLO-NAS, YOLOv8, RTMDet, YOLO-MS, and Gold-YOLO. A large score threshold can largely reduce the overhead of NMS, but bring negative results to detection performance. We optimize the NMS latency by carefully tuning the score thresholds for non-endto-end detectors, achieving a balance between the detection performance and the total latency. The overhead brought by NMS is significantly reduced to 0.1∼0.5 ms with slight drops in detection performance.

Distribution of the number of boxes for NMS. The latency is measured on the COCO val2017, which is an average of 5000 images. Figure 5 gives the distribution of the number of remaining boxes in NMS across the COCO val2017 under different score thresholds in YOLO-NAS. The large overhead brought by the NMS is due to the large number of remaining boxes under the default score threshold. Tuning the score threshold effectively decreases the remaining boxes in NMS, thus providing optimization in total latency for non-end-to-end detectors.

![](Images_4AGKLBUD/64e65ede137f6f608bd5f99eb9646473d4ba38f4ff78ea299256e9b76277da08.jpg)  
Fig. 5: Distribution of the number of boxes. The x-axis corresponds to the number of boxes that are fed into NMS. The y-axis corresponds to the number of images on COCO val2017 whose remaining box numbers are in the corresponding interval. (a) is under the default score threshold. (b) is tuning the score threshold to get the balance between detection performance and latency. (c) is tuning a higher score threshold.