{
  "selectionType": "mixed",
  "items": {
    "parents": [
      {
        "item": {
          "id": 2,
          "key": "S86GB385",
          "itemType": "conferencePaper",
          "title": "Accelerating DETR Convergence via Semantic-Aligned Matching",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "S86GB385",
            "version": 0,
            "itemType": "conferencePaper",
            "title": "Accelerating DETR Convergence via Semantic-Aligned Matching",
            "date": "2022",
            "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.html",
            "accessDate": "2026-01-27T00:48:29Z",
            "language": "en",
            "libraryCatalog": "openaccess.thecvf.com",
            "extra": "Citation Key: zhang_accelerating-detr_2022",
            "pages": "949-958",
            "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "creators": [
              {
                "firstName": "Gongjie",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Zhipeng",
                "lastName": "Luo",
                "creatorType": "author"
              },
              {
                "firstName": "Yingchen",
                "lastName": "Yu",
                "creatorType": "author"
              },
              {
                "firstName": "Kaiwen",
                "lastName": "Cui",
                "creatorType": "author"
              },
              {
                "firstName": "Shijian",
                "lastName": "Lu",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:48:29Z",
            "dateModified": "2026-01-27T10:22:48Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 80,
              "key": "P5SUAUDR",
              "itemType": "attachment",
              "title": "Zhang 等 - 2022 - Accelerating DETR Convergence via Semantic-Aligned Matching.md",
              "libraryID": 1,
              "parentItemID": 2,
              "data": {
                "key": "P5SUAUDR",
                "version": 0,
                "itemType": "attachment",
                "title": "Zhang 等 - 2022 - Accelerating DETR Convergence via Semantic-Aligned Matching.md",
                "parentItem": "S86GB385",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/P5SUAUDR/Zhang 等 - 2022 - Accelerating DETR Convergence via Semantic-Aligned Matching.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:16:30Z",
                "dateModified": "2026-01-27T01:16:31Z"
              }
            },
            "parent": {
              "id": 2,
              "key": "S86GB385",
              "itemType": "conferencePaper",
              "title": "Accelerating DETR Convergence via Semantic-Aligned Matching",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "S86GB385",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Accelerating DETR Convergence via Semantic-Aligned Matching",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:48:29Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: zhang_accelerating-detr_2022",
                "pages": "949-958",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Gongjie",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zhipeng",
                    "lastName": "Luo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yingchen",
                    "lastName": "Yu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Kaiwen",
                    "lastName": "Cui",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shijian",
                    "lastName": "Lu",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:48:29Z",
                "dateModified": "2026-01-27T10:22:48Z"
              }
            },
            "filePath": "attachments/P5SUAUDR/Zhang 等 - 2022 - Accelerating DETR Convergence via Semantic-Aligned Matching.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 4,
              "key": "EXKUYHMH",
              "itemType": "attachment",
              "title": "Zhang 等 - 2022 - Accelerating DETR Convergence via Semantic-Aligned Matching.pdf",
              "libraryID": 1,
              "parentItemID": 2,
              "data": {
                "key": "EXKUYHMH",
                "version": 0,
                "itemType": "attachment",
                "title": "Zhang 等 - 2022 - Accelerating DETR Convergence via Semantic-Aligned Matching.pdf",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.pdf",
                "accessDate": "2026-01-27T00:48:32Z",
                "parentItem": "S86GB385",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/EXKUYHMH/Zhang 等 - 2022 - Accelerating DETR Convergence via Semantic-Aligned Matching.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:48:32Z",
                "dateModified": "2026-01-27T00:48:32Z"
              }
            },
            "parent": {
              "id": 2,
              "key": "S86GB385",
              "itemType": "conferencePaper",
              "title": "Accelerating DETR Convergence via Semantic-Aligned Matching",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "S86GB385",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Accelerating DETR Convergence via Semantic-Aligned Matching",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:48:29Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: zhang_accelerating-detr_2022",
                "pages": "949-958",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Gongjie",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zhipeng",
                    "lastName": "Luo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yingchen",
                    "lastName": "Yu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Kaiwen",
                    "lastName": "Cui",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shijian",
                    "lastName": "Lu",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:48:29Z",
                "dateModified": "2026-01-27T10:22:48Z"
              }
            },
            "filePath": "attachments/EXKUYHMH/Zhang 等 - 2022 - Accelerating DETR Convergence via Semantic-Aligned Matching.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 5,
          "key": "JMWG9XVD",
          "itemType": "conferencePaper",
          "title": "An End-to-End Transformer Model for 3D Object Detection",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "JMWG9XVD",
            "version": 0,
            "itemType": "conferencePaper",
            "title": "An End-to-End Transformer Model for 3D Object Detection",
            "date": "2021",
            "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.html?ref=https://githubhelp.com",
            "accessDate": "2026-01-27T00:49:26Z",
            "language": "en",
            "libraryCatalog": "openaccess.thecvf.com",
            "extra": "Citation Key: misra_endtoend-transformer_2021",
            "pages": "2906-2917",
            "conferenceName": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "creators": [
              {
                "firstName": "Ishan",
                "lastName": "Misra",
                "creatorType": "author"
              },
              {
                "firstName": "Rohit",
                "lastName": "Girdhar",
                "creatorType": "author"
              },
              {
                "firstName": "Armand",
                "lastName": "Joulin",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:49:26Z",
            "dateModified": "2026-01-27T10:22:49Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 81,
              "key": "JQEMRSLF",
              "itemType": "attachment",
              "title": "Misra 等 - 2021 - An End-to-End Transformer Model for 3D Object Detection.md",
              "libraryID": 1,
              "parentItemID": 5,
              "data": {
                "key": "JQEMRSLF",
                "version": 0,
                "itemType": "attachment",
                "title": "Misra 等 - 2021 - An End-to-End Transformer Model for 3D Object Detection.md",
                "parentItem": "JMWG9XVD",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/JQEMRSLF/Misra 等 - 2021 - An End-to-End Transformer Model for 3D Object Detection.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:16:36Z",
                "dateModified": "2026-01-27T01:16:36Z"
              }
            },
            "parent": {
              "id": 5,
              "key": "JMWG9XVD",
              "itemType": "conferencePaper",
              "title": "An End-to-End Transformer Model for 3D Object Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "JMWG9XVD",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "An End-to-End Transformer Model for 3D Object Detection",
                "date": "2021",
                "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.html?ref=https://githubhelp.com",
                "accessDate": "2026-01-27T00:49:26Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: misra_endtoend-transformer_2021",
                "pages": "2906-2917",
                "conferenceName": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "creators": [
                  {
                    "firstName": "Ishan",
                    "lastName": "Misra",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Rohit",
                    "lastName": "Girdhar",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Armand",
                    "lastName": "Joulin",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:49:26Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/JQEMRSLF/Misra 等 - 2021 - An End-to-End Transformer Model for 3D Object Detection.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 7,
              "key": "BG7NXEQ2",
              "itemType": "attachment",
              "title": "Misra 等 - 2021 - An End-to-End Transformer Model for 3D Object Detection.pdf",
              "libraryID": 1,
              "parentItemID": 5,
              "data": {
                "key": "BG7NXEQ2",
                "version": 0,
                "itemType": "attachment",
                "title": "Misra 等 - 2021 - An End-to-End Transformer Model for 3D Object Detection.pdf",
                "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf",
                "accessDate": "2026-01-27T00:49:29Z",
                "parentItem": "JMWG9XVD",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/BG7NXEQ2/Misra 等 - 2021 - An End-to-End Transformer Model for 3D Object Detection.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:49:29Z",
                "dateModified": "2026-01-27T00:49:29Z"
              }
            },
            "parent": {
              "id": 5,
              "key": "JMWG9XVD",
              "itemType": "conferencePaper",
              "title": "An End-to-End Transformer Model for 3D Object Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "JMWG9XVD",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "An End-to-End Transformer Model for 3D Object Detection",
                "date": "2021",
                "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.html?ref=https://githubhelp.com",
                "accessDate": "2026-01-27T00:49:26Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: misra_endtoend-transformer_2021",
                "pages": "2906-2917",
                "conferenceName": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "creators": [
                  {
                    "firstName": "Ishan",
                    "lastName": "Misra",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Rohit",
                    "lastName": "Girdhar",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Armand",
                    "lastName": "Joulin",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:49:26Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/BG7NXEQ2/Misra 等 - 2021 - An End-to-End Transformer Model for 3D Object Detection.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 8,
          "key": "W4CDLU28",
          "itemType": "conferencePaper",
          "title": "Conditional DETR for Fast Training Convergence",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "W4CDLU28",
            "version": 0,
            "itemType": "conferencePaper",
            "title": "Conditional DETR for Fast Training Convergence",
            "date": "2021",
            "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Meng_Conditional_DETR_for_Fast_Training_Convergence_ICCV_2021_paper.html",
            "accessDate": "2026-01-27T00:49:31Z",
            "language": "en",
            "libraryCatalog": "openaccess.thecvf.com",
            "extra": "Citation Key: meng_conditional-detr_2021",
            "pages": "3651-3660",
            "conferenceName": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "creators": [
              {
                "firstName": "Depu",
                "lastName": "Meng",
                "creatorType": "author"
              },
              {
                "firstName": "Xiaokang",
                "lastName": "Chen",
                "creatorType": "author"
              },
              {
                "firstName": "Zejia",
                "lastName": "Fan",
                "creatorType": "author"
              },
              {
                "firstName": "Gang",
                "lastName": "Zeng",
                "creatorType": "author"
              },
              {
                "firstName": "Houqiang",
                "lastName": "Li",
                "creatorType": "author"
              },
              {
                "firstName": "Yuhui",
                "lastName": "Yuan",
                "creatorType": "author"
              },
              {
                "firstName": "Lei",
                "lastName": "Sun",
                "creatorType": "author"
              },
              {
                "firstName": "Jingdong",
                "lastName": "Wang",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:49:31Z",
            "dateModified": "2026-01-27T10:22:49Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 82,
              "key": "8VEUH6U2",
              "itemType": "attachment",
              "title": "Meng 等 - 2021 - Conditional DETR for Fast Training Convergence.md",
              "libraryID": 1,
              "parentItemID": 8,
              "data": {
                "key": "8VEUH6U2",
                "version": 0,
                "itemType": "attachment",
                "title": "Meng 等 - 2021 - Conditional DETR for Fast Training Convergence.md",
                "parentItem": "W4CDLU28",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/8VEUH6U2/Meng 等 - 2021 - Conditional DETR for Fast Training Convergence.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:16:44Z",
                "dateModified": "2026-01-27T01:16:44Z"
              }
            },
            "parent": {
              "id": 8,
              "key": "W4CDLU28",
              "itemType": "conferencePaper",
              "title": "Conditional DETR for Fast Training Convergence",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "W4CDLU28",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Conditional DETR for Fast Training Convergence",
                "date": "2021",
                "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Meng_Conditional_DETR_for_Fast_Training_Convergence_ICCV_2021_paper.html",
                "accessDate": "2026-01-27T00:49:31Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: meng_conditional-detr_2021",
                "pages": "3651-3660",
                "conferenceName": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "creators": [
                  {
                    "firstName": "Depu",
                    "lastName": "Meng",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiaokang",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zejia",
                    "lastName": "Fan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Gang",
                    "lastName": "Zeng",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Houqiang",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yuhui",
                    "lastName": "Yuan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lei",
                    "lastName": "Sun",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jingdong",
                    "lastName": "Wang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:49:31Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/8VEUH6U2/Meng 等 - 2021 - Conditional DETR for Fast Training Convergence.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 10,
              "key": "A8BVFHT3",
              "itemType": "attachment",
              "title": "Meng 等 - 2021 - Conditional DETR for Fast Training Convergence.pdf",
              "libraryID": 1,
              "parentItemID": 8,
              "data": {
                "key": "A8BVFHT3",
                "version": 0,
                "itemType": "attachment",
                "title": "Meng 等 - 2021 - Conditional DETR for Fast Training Convergence.pdf",
                "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_Conditional_DETR_for_Fast_Training_Convergence_ICCV_2021_paper.pdf",
                "accessDate": "2026-01-27T00:49:34Z",
                "parentItem": "W4CDLU28",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/A8BVFHT3/Meng 等 - 2021 - Conditional DETR for Fast Training Convergence.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:49:34Z",
                "dateModified": "2026-01-27T00:49:34Z"
              }
            },
            "parent": {
              "id": 8,
              "key": "W4CDLU28",
              "itemType": "conferencePaper",
              "title": "Conditional DETR for Fast Training Convergence",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "W4CDLU28",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Conditional DETR for Fast Training Convergence",
                "date": "2021",
                "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Meng_Conditional_DETR_for_Fast_Training_Convergence_ICCV_2021_paper.html",
                "accessDate": "2026-01-27T00:49:31Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: meng_conditional-detr_2021",
                "pages": "3651-3660",
                "conferenceName": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "creators": [
                  {
                    "firstName": "Depu",
                    "lastName": "Meng",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiaokang",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zejia",
                    "lastName": "Fan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Gang",
                    "lastName": "Zeng",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Houqiang",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yuhui",
                    "lastName": "Yuan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lei",
                    "lastName": "Sun",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jingdong",
                    "lastName": "Wang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:49:31Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/A8BVFHT3/Meng 等 - 2021 - Conditional DETR for Fast Training Convergence.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 142,
          "key": "IY3FMWQM",
          "itemType": "preprint",
          "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "IY3FMWQM",
            "version": 5,
            "itemType": "preprint",
            "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
            "abstractNote": "We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \\url{https://github.com/SlongLiu/DAB-DETR}.",
            "date": "2022-03-30",
            "DOI": "10.48550/arXiv.2201.12329",
            "url": "http://arxiv.org/abs/2201.12329",
            "accessDate": "2026-01-03T14:43:30Z",
            "shortTitle": "DAB-DETR",
            "libraryCatalog": "arXiv.org",
            "extra": "arXiv:2201.12329 [cs]\nCitation Key: liu_dabdetr-dynamic_2022",
            "repository": "arXiv",
            "archiveID": "arXiv:2201.12329",
            "creators": [
              {
                "firstName": "Shilong",
                "lastName": "Liu",
                "creatorType": "author"
              },
              {
                "firstName": "Feng",
                "lastName": "Li",
                "creatorType": "author"
              },
              {
                "firstName": "Hao",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Xiao",
                "lastName": "Yang",
                "creatorType": "author"
              },
              {
                "firstName": "Xianbiao",
                "lastName": "Qi",
                "creatorType": "author"
              },
              {
                "firstName": "Hang",
                "lastName": "Su",
                "creatorType": "author"
              },
              {
                "firstName": "Jun",
                "lastName": "Zhu",
                "creatorType": "author"
              },
              {
                "firstName": "Lei",
                "lastName": "Zhang",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "Computer Science - Computer Vision and Pattern Recognition",
                "type": 1
              },
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:43:30Z",
            "dateModified": "2026-01-27T10:21:34Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 177,
              "key": "SUXAAYX9",
              "itemType": "attachment",
              "title": "Liu 等 - 2022 - DAB-DETR Dynamic Anchor Boxes are Better Queries for DETR.md",
              "libraryID": 1,
              "parentItemID": 142,
              "data": {
                "key": "SUXAAYX9",
                "version": 101,
                "itemType": "attachment",
                "title": "Liu 等 - 2022 - DAB-DETR Dynamic Anchor Boxes are Better Queries for DETR.md",
                "parentItem": "IY3FMWQM",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/SUXAAYX9/Liu 等 - 2022 - DAB-DETR Dynamic Anchor Boxes are Better Queries for DETR.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T11:29:06Z",
                "dateModified": "2026-01-16T11:29:07Z"
              }
            },
            "parent": {
              "id": 142,
              "key": "IY3FMWQM",
              "itemType": "preprint",
              "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "IY3FMWQM",
                "version": 5,
                "itemType": "preprint",
                "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
                "abstractNote": "We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \\url{https://github.com/SlongLiu/DAB-DETR}.",
                "date": "2022-03-30",
                "DOI": "10.48550/arXiv.2201.12329",
                "url": "http://arxiv.org/abs/2201.12329",
                "accessDate": "2026-01-03T14:43:30Z",
                "shortTitle": "DAB-DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2201.12329 [cs]\nCitation Key: liu_dabdetr-dynamic_2022",
                "repository": "arXiv",
                "archiveID": "arXiv:2201.12329",
                "creators": [
                  {
                    "firstName": "Shilong",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Feng",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hao",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiao",
                    "lastName": "Yang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xianbiao",
                    "lastName": "Qi",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hang",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jun",
                    "lastName": "Zhu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lei",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:30Z",
                "dateModified": "2026-01-27T10:21:34Z"
              }
            },
            "filePath": "attachments/SUXAAYX9/Liu 等 - 2022 - DAB-DETR Dynamic Anchor Boxes are Better Queries for DETR.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 226,
              "key": "39KG9BMT",
              "itemType": "attachment",
              "title": "Liu 等 - 2022 - DAB-DETR Dynamic Anchor Boxes are Better Queries for DETR.pdf",
              "libraryID": 1,
              "parentItemID": 142,
              "data": {
                "key": "39KG9BMT",
                "version": 68,
                "itemType": "attachment",
                "title": "Liu 等 - 2022 - DAB-DETR Dynamic Anchor Boxes are Better Queries for DETR.pdf",
                "url": "http://arxiv.org/pdf/2201.12329v4",
                "accessDate": "2026-01-03T14:43:31Z",
                "parentItem": "IY3FMWQM",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/39KG9BMT/Liu 等 - 2022 - DAB-DETR Dynamic Anchor Boxes are Better Queries for DETR.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:31Z",
                "dateModified": "2026-01-03T14:43:31Z"
              }
            },
            "parent": {
              "id": 142,
              "key": "IY3FMWQM",
              "itemType": "preprint",
              "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "IY3FMWQM",
                "version": 5,
                "itemType": "preprint",
                "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
                "abstractNote": "We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \\url{https://github.com/SlongLiu/DAB-DETR}.",
                "date": "2022-03-30",
                "DOI": "10.48550/arXiv.2201.12329",
                "url": "http://arxiv.org/abs/2201.12329",
                "accessDate": "2026-01-03T14:43:30Z",
                "shortTitle": "DAB-DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2201.12329 [cs]\nCitation Key: liu_dabdetr-dynamic_2022",
                "repository": "arXiv",
                "archiveID": "arXiv:2201.12329",
                "creators": [
                  {
                    "firstName": "Shilong",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Feng",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hao",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiao",
                    "lastName": "Yang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xianbiao",
                    "lastName": "Qi",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hang",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jun",
                    "lastName": "Zhu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lei",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:30Z",
                "dateModified": "2026-01-27T10:21:34Z"
              }
            },
            "filePath": "attachments/39KG9BMT/Liu 等 - 2022 - DAB-DETR Dynamic Anchor Boxes are Better Queries for DETR.pdf",
            "mimeType": null
          },
          {
            "item": {
              "id": 225,
              "key": "E9WVEIHL",
              "itemType": "attachment",
              "title": "Snapshot",
              "libraryID": 1,
              "parentItemID": 142,
              "data": {
                "key": "E9WVEIHL",
                "version": 5,
                "itemType": "attachment",
                "title": "Snapshot",
                "url": "http://arxiv.org/abs/2201.12329",
                "accessDate": "2026-01-03T14:43:32Z",
                "parentItem": "IY3FMWQM",
                "linkMode": "imported_url",
                "contentType": "text/html",
                "charset": "utf-8",
                "filename": "2201.html",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:32Z",
                "dateModified": "2026-01-03T14:43:32Z",
                "path": "attachments/E9WVEIHL/Snapshot"
              }
            },
            "parent": {
              "id": 142,
              "key": "IY3FMWQM",
              "itemType": "preprint",
              "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "IY3FMWQM",
                "version": 5,
                "itemType": "preprint",
                "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
                "abstractNote": "We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \\url{https://github.com/SlongLiu/DAB-DETR}.",
                "date": "2022-03-30",
                "DOI": "10.48550/arXiv.2201.12329",
                "url": "http://arxiv.org/abs/2201.12329",
                "accessDate": "2026-01-03T14:43:30Z",
                "shortTitle": "DAB-DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2201.12329 [cs]\nCitation Key: liu_dabdetr-dynamic_2022",
                "repository": "arXiv",
                "archiveID": "arXiv:2201.12329",
                "creators": [
                  {
                    "firstName": "Shilong",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Feng",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hao",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiao",
                    "lastName": "Yang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xianbiao",
                    "lastName": "Qi",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hang",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jun",
                    "lastName": "Zhu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lei",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:30Z",
                "dateModified": "2026-01-27T10:21:34Z"
              }
            },
            "filePath": "attachments/E9WVEIHL/Snapshot",
            "mimeType": null
          }
        ],
        "notes": [
          {
            "id": 227,
            "key": "L7UH9D4Y",
            "itemType": "note",
            "title": "Comment: Accepted to ICLR 2022",
            "libraryID": 1,
            "parentItemID": 142,
            "data": {
              "key": "L7UH9D4Y",
              "version": 5,
              "itemType": "note",
              "parentItem": "IY3FMWQM",
              "note": "Comment: Accepted to ICLR 2022",
              "tags": [],
              "relations": {},
              "dateAdded": "2026-01-03T14:43:30Z",
              "dateModified": "2026-01-03T14:43:30Z"
            },
            "note": "Comment: Accepted to ICLR 2022"
          }
        ],
        "tags": [
          {
            "tag": "Computer Science - Computer Vision and Pattern Recognition",
            "type": 1
          },
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 134,
          "key": "5HBHAWIV",
          "itemType": "preprint",
          "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "5HBHAWIV",
            "version": 9,
            "itemType": "preprint",
            "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
            "abstractNote": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.",
            "date": "2021-03-18",
            "DOI": "10.48550/arXiv.2010.04159",
            "url": "http://arxiv.org/abs/2010.04159",
            "accessDate": "2026-01-03T14:47:28Z",
            "shortTitle": "Deformable DETR",
            "libraryCatalog": "arXiv.org",
            "extra": "arXiv:2010.04159 [cs]\nCitation Key: zhu_deformable-detr_2021",
            "repository": "arXiv",
            "archiveID": "arXiv:2010.04159",
            "creators": [
              {
                "firstName": "Xizhou",
                "lastName": "Zhu",
                "creatorType": "author"
              },
              {
                "firstName": "Weijie",
                "lastName": "Su",
                "creatorType": "author"
              },
              {
                "firstName": "Lewei",
                "lastName": "Lu",
                "creatorType": "author"
              },
              {
                "firstName": "Bin",
                "lastName": "Li",
                "creatorType": "author"
              },
              {
                "firstName": "Xiaogang",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Jifeng",
                "lastName": "Dai",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "Computer Science - Computer Vision and Pattern Recognition",
                "type": 1
              },
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:47:28Z",
            "dateModified": "2026-01-27T10:21:56Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 195,
              "key": "SUB2BQSR",
              "itemType": "attachment",
              "title": "Snapshot",
              "libraryID": 1,
              "parentItemID": 134,
              "data": {
                "key": "SUB2BQSR",
                "version": 10,
                "itemType": "attachment",
                "title": "Snapshot",
                "url": "http://arxiv.org/abs/2010.04159",
                "accessDate": "2026-01-03T14:47:31Z",
                "parentItem": "5HBHAWIV",
                "linkMode": "imported_url",
                "contentType": "text/html",
                "charset": "utf-8",
                "filename": "2010.html",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:31Z",
                "dateModified": "2026-01-03T14:47:31Z",
                "path": "attachments/SUB2BQSR/Snapshot"
              }
            },
            "parent": {
              "id": 134,
              "key": "5HBHAWIV",
              "itemType": "preprint",
              "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "5HBHAWIV",
                "version": 9,
                "itemType": "preprint",
                "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
                "abstractNote": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.",
                "date": "2021-03-18",
                "DOI": "10.48550/arXiv.2010.04159",
                "url": "http://arxiv.org/abs/2010.04159",
                "accessDate": "2026-01-03T14:47:28Z",
                "shortTitle": "Deformable DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2010.04159 [cs]\nCitation Key: zhu_deformable-detr_2021",
                "repository": "arXiv",
                "archiveID": "arXiv:2010.04159",
                "creators": [
                  {
                    "firstName": "Xizhou",
                    "lastName": "Zhu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Weijie",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lewei",
                    "lastName": "Lu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Bin",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiaogang",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jifeng",
                    "lastName": "Dai",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:28Z",
                "dateModified": "2026-01-27T10:21:56Z"
              }
            },
            "filePath": "attachments/SUB2BQSR/Snapshot",
            "mimeType": null
          },
          {
            "item": {
              "id": 173,
              "key": "JK9NLHWL",
              "itemType": "attachment",
              "title": "Zhu 等 - 2021 - Deformable DETR Deformable Transformers for End-to-End Object Detection.md",
              "libraryID": 1,
              "parentItemID": 134,
              "data": {
                "key": "JK9NLHWL",
                "version": 108,
                "itemType": "attachment",
                "title": "Zhu 等 - 2021 - Deformable DETR Deformable Transformers for End-to-End Object Detection.md",
                "parentItem": "5HBHAWIV",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/JK9NLHWL/Zhu 等 - 2021 - Deformable DETR Deformable Transformers for End-to-End Object Detection.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T11:30:30Z",
                "dateModified": "2026-01-16T11:30:31Z"
              }
            },
            "parent": {
              "id": 134,
              "key": "5HBHAWIV",
              "itemType": "preprint",
              "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "5HBHAWIV",
                "version": 9,
                "itemType": "preprint",
                "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
                "abstractNote": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.",
                "date": "2021-03-18",
                "DOI": "10.48550/arXiv.2010.04159",
                "url": "http://arxiv.org/abs/2010.04159",
                "accessDate": "2026-01-03T14:47:28Z",
                "shortTitle": "Deformable DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2010.04159 [cs]\nCitation Key: zhu_deformable-detr_2021",
                "repository": "arXiv",
                "archiveID": "arXiv:2010.04159",
                "creators": [
                  {
                    "firstName": "Xizhou",
                    "lastName": "Zhu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Weijie",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lewei",
                    "lastName": "Lu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Bin",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiaogang",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jifeng",
                    "lastName": "Dai",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:28Z",
                "dateModified": "2026-01-27T10:21:56Z"
              }
            },
            "filePath": "attachments/JK9NLHWL/Zhu 等 - 2021 - Deformable DETR Deformable Transformers for End-to-End Object Detection.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 194,
              "key": "UXGYIWKK",
              "itemType": "attachment",
              "title": "Zhu 等 - 2021 - Deformable DETR Deformable Transformers for End-to-End Object Detection.pdf",
              "libraryID": 1,
              "parentItemID": 134,
              "data": {
                "key": "UXGYIWKK",
                "version": 69,
                "itemType": "attachment",
                "title": "Zhu 等 - 2021 - Deformable DETR Deformable Transformers for End-to-End Object Detection.pdf",
                "url": "http://arxiv.org/pdf/2010.04159v4",
                "accessDate": "2026-01-03T14:47:30Z",
                "parentItem": "5HBHAWIV",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/UXGYIWKK/Zhu 等 - 2021 - Deformable DETR Deformable Transformers for End-to-End Object Detection.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:30Z",
                "dateModified": "2026-01-03T14:47:31Z"
              }
            },
            "parent": {
              "id": 134,
              "key": "5HBHAWIV",
              "itemType": "preprint",
              "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "5HBHAWIV",
                "version": 9,
                "itemType": "preprint",
                "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
                "abstractNote": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.",
                "date": "2021-03-18",
                "DOI": "10.48550/arXiv.2010.04159",
                "url": "http://arxiv.org/abs/2010.04159",
                "accessDate": "2026-01-03T14:47:28Z",
                "shortTitle": "Deformable DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2010.04159 [cs]\nCitation Key: zhu_deformable-detr_2021",
                "repository": "arXiv",
                "archiveID": "arXiv:2010.04159",
                "creators": [
                  {
                    "firstName": "Xizhou",
                    "lastName": "Zhu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Weijie",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lewei",
                    "lastName": "Lu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Bin",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiaogang",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jifeng",
                    "lastName": "Dai",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:28Z",
                "dateModified": "2026-01-27T10:21:56Z"
              }
            },
            "filePath": "attachments/UXGYIWKK/Zhu 等 - 2021 - Deformable DETR Deformable Transformers for End-to-End Object Detection.pdf",
            "mimeType": null
          }
        ],
        "notes": [
          {
            "id": 196,
            "key": "5EIDSSGU",
            "itemType": "note",
            "title": "Comment: ICLR 2021 Oral",
            "libraryID": 1,
            "parentItemID": 134,
            "data": {
              "key": "5EIDSSGU",
              "version": 10,
              "itemType": "note",
              "parentItem": "5HBHAWIV",
              "note": "Comment: ICLR 2021 Oral",
              "tags": [],
              "relations": {},
              "dateAdded": "2026-01-03T14:47:28Z",
              "dateModified": "2026-01-03T14:47:28Z"
            },
            "note": "Comment: ICLR 2021 Oral"
          },
          {
            "id": 169,
            "key": "XS823CMB",
            "itemType": "note",
            "title": "Note",
            "libraryID": 1,
            "parentItemID": 134,
            "data": {
              "key": "XS823CMB",
              "version": 120,
              "itemType": "note",
              "parentItem": "5HBHAWIV",
              "note": "<div data-schema-version=\"9\"><h1>Note</h1>\n<p>test_note</p>\n</div>",
              "tags": [
                {
                  "tag": "test-tag"
                }
              ],
              "relations": {},
              "dateAdded": "2026-01-17T08:11:04Z",
              "dateModified": "2026-01-17T08:46:19Z"
            },
            "note": "<div data-schema-version=\"9\"><h1>Note</h1>\n<p>test_note</p>\n</div>"
          }
        ],
        "tags": [
          {
            "tag": "Computer Science - Computer Vision and Pattern Recognition",
            "type": 1
          },
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 19,
          "key": "3JUY9GBQ",
          "itemType": "conferencePaper",
          "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "3JUY9GBQ",
            "version": 0,
            "itemType": "conferencePaper",
            "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries",
            "abstractNote": "We introduce a framework for multi-camera 3D object detection. In contrast to existing works, which estimate 3D bounding boxes directly from monocular images or use depth prediction networks to generate input for 3D object detection from 2D information, our method manipulates predictions directly in 3D space. Our architecture extracts 2D  features from multiple camera images and then uses a sparse set of 3D object queries to index into these 2D features, linking 3D positions to multi-view images using camera transformation matrices. Finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. This top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. Moreover, our method does not require  post-processing such as non-maximum suppression, dramatically improving inference speed. We achieve state-of-the-art performance on the nuScenes autonomous driving benchmark.",
            "date": "2022-01-11",
            "url": "https://proceedings.mlr.press/v164/wang22b.html",
            "accessDate": "2026-01-27T00:52:39Z",
            "shortTitle": "DETR3D",
            "language": "en",
            "libraryCatalog": "proceedings.mlr.press",
            "extra": "Citation Key: wang_detr3d-3d_2022",
            "publisher": "PMLR",
            "pages": "180-191",
            "ISSN": "2640-3498",
            "proceedingsTitle": "Proceedings of the 5th Conference on Robot Learning",
            "conferenceName": "Conference on Robot Learning",
            "creators": [
              {
                "firstName": "Yue",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Vitor Campagnolo",
                "lastName": "Guizilini",
                "creatorType": "author"
              },
              {
                "firstName": "Tianyuan",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Yilun",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Hang",
                "lastName": "Zhao",
                "creatorType": "author"
              },
              {
                "firstName": "Justin",
                "lastName": "Solomon",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:52:39Z",
            "dateModified": "2026-01-27T10:22:49Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 85,
              "key": "I336WEDD",
              "itemType": "attachment",
              "title": "Wang 等 - 2022 - DETR3D 3D Object Detection from Multi-view Images via 3D-to-2D Queries.md",
              "libraryID": 1,
              "parentItemID": 19,
              "data": {
                "key": "I336WEDD",
                "version": 0,
                "itemType": "attachment",
                "title": "Wang 等 - 2022 - DETR3D 3D Object Detection from Multi-view Images via 3D-to-2D Queries.md",
                "parentItem": "3JUY9GBQ",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/I336WEDD/Wang 等 - 2022 - DETR3D 3D Object Detection from Multi-view Images via 3D-to-2D Queries.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:17:00Z",
                "dateModified": "2026-01-27T01:17:01Z"
              }
            },
            "parent": {
              "id": 19,
              "key": "3JUY9GBQ",
              "itemType": "conferencePaper",
              "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "3JUY9GBQ",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries",
                "abstractNote": "We introduce a framework for multi-camera 3D object detection. In contrast to existing works, which estimate 3D bounding boxes directly from monocular images or use depth prediction networks to generate input for 3D object detection from 2D information, our method manipulates predictions directly in 3D space. Our architecture extracts 2D  features from multiple camera images and then uses a sparse set of 3D object queries to index into these 2D features, linking 3D positions to multi-view images using camera transformation matrices. Finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. This top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. Moreover, our method does not require  post-processing such as non-maximum suppression, dramatically improving inference speed. We achieve state-of-the-art performance on the nuScenes autonomous driving benchmark.",
                "date": "2022-01-11",
                "url": "https://proceedings.mlr.press/v164/wang22b.html",
                "accessDate": "2026-01-27T00:52:39Z",
                "shortTitle": "DETR3D",
                "language": "en",
                "libraryCatalog": "proceedings.mlr.press",
                "extra": "Citation Key: wang_detr3d-3d_2022",
                "publisher": "PMLR",
                "pages": "180-191",
                "ISSN": "2640-3498",
                "proceedingsTitle": "Proceedings of the 5th Conference on Robot Learning",
                "conferenceName": "Conference on Robot Learning",
                "creators": [
                  {
                    "firstName": "Yue",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Vitor Campagnolo",
                    "lastName": "Guizilini",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tianyuan",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yilun",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hang",
                    "lastName": "Zhao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Justin",
                    "lastName": "Solomon",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:39Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/I336WEDD/Wang 等 - 2022 - DETR3D 3D Object Detection from Multi-view Images via 3D-to-2D Queries.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 24,
              "key": "TS4595M7",
              "itemType": "attachment",
              "title": "Wang 等 - 2022 - DETR3D 3D Object Detection from Multi-view Images via 3D-to-2D Queries.pdf",
              "libraryID": 1,
              "parentItemID": 19,
              "data": {
                "key": "TS4595M7",
                "version": 0,
                "itemType": "attachment",
                "title": "Wang 等 - 2022 - DETR3D 3D Object Detection from Multi-view Images via 3D-to-2D Queries.pdf",
                "url": "https://proceedings.mlr.press/v164/wang22b/wang22b.pdf",
                "accessDate": "2026-01-27T00:52:42Z",
                "parentItem": "3JUY9GBQ",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/TS4595M7/Wang 等 - 2022 - DETR3D 3D Object Detection from Multi-view Images via 3D-to-2D Queries.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:42Z",
                "dateModified": "2026-01-27T00:52:43Z"
              }
            },
            "parent": {
              "id": 19,
              "key": "3JUY9GBQ",
              "itemType": "conferencePaper",
              "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "3JUY9GBQ",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries",
                "abstractNote": "We introduce a framework for multi-camera 3D object detection. In contrast to existing works, which estimate 3D bounding boxes directly from monocular images or use depth prediction networks to generate input for 3D object detection from 2D information, our method manipulates predictions directly in 3D space. Our architecture extracts 2D  features from multiple camera images and then uses a sparse set of 3D object queries to index into these 2D features, linking 3D positions to multi-view images using camera transformation matrices. Finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. This top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. Moreover, our method does not require  post-processing such as non-maximum suppression, dramatically improving inference speed. We achieve state-of-the-art performance on the nuScenes autonomous driving benchmark.",
                "date": "2022-01-11",
                "url": "https://proceedings.mlr.press/v164/wang22b.html",
                "accessDate": "2026-01-27T00:52:39Z",
                "shortTitle": "DETR3D",
                "language": "en",
                "libraryCatalog": "proceedings.mlr.press",
                "extra": "Citation Key: wang_detr3d-3d_2022",
                "publisher": "PMLR",
                "pages": "180-191",
                "ISSN": "2640-3498",
                "proceedingsTitle": "Proceedings of the 5th Conference on Robot Learning",
                "conferenceName": "Conference on Robot Learning",
                "creators": [
                  {
                    "firstName": "Yue",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Vitor Campagnolo",
                    "lastName": "Guizilini",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tianyuan",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yilun",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hang",
                    "lastName": "Zhao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Justin",
                    "lastName": "Solomon",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:39Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/TS4595M7/Wang 等 - 2022 - DETR3D 3D Object Detection from Multi-view Images via 3D-to-2D Queries.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 23,
          "key": "CBJWE4JX",
          "itemType": "conferencePaper",
          "title": "DETRs Beat YOLOs on Real-time Object Detection",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "CBJWE4JX",
            "version": 0,
            "itemType": "conferencePaper",
            "title": "DETRs Beat YOLOs on Real-time Object Detection",
            "date": "2024",
            "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.html",
            "accessDate": "2026-01-27T00:52:43Z",
            "language": "en",
            "libraryCatalog": "openaccess.thecvf.com",
            "extra": "Citation Key: zhao_detrs-beat_2024",
            "pages": "16965-16974",
            "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "creators": [
              {
                "firstName": "Yian",
                "lastName": "Zhao",
                "creatorType": "author"
              },
              {
                "firstName": "Wenyu",
                "lastName": "Lv",
                "creatorType": "author"
              },
              {
                "firstName": "Shangliang",
                "lastName": "Xu",
                "creatorType": "author"
              },
              {
                "firstName": "Jinman",
                "lastName": "Wei",
                "creatorType": "author"
              },
              {
                "firstName": "Guanzhong",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Qingqing",
                "lastName": "Dang",
                "creatorType": "author"
              },
              {
                "firstName": "Yi",
                "lastName": "Liu",
                "creatorType": "author"
              },
              {
                "firstName": "Jie",
                "lastName": "Chen",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:52:43Z",
            "dateModified": "2026-01-27T10:22:49Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 86,
              "key": "9VTZPVEB",
              "itemType": "attachment",
              "title": "Zhao 等 - 2024 - DETRs Beat YOLOs on Real-time Object Detection.md",
              "libraryID": 1,
              "parentItemID": 23,
              "data": {
                "key": "9VTZPVEB",
                "version": 0,
                "itemType": "attachment",
                "title": "Zhao 等 - 2024 - DETRs Beat YOLOs on Real-time Object Detection.md",
                "parentItem": "CBJWE4JX",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/9VTZPVEB/Zhao 等 - 2024 - DETRs Beat YOLOs on Real-time Object Detection.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:17:06Z",
                "dateModified": "2026-01-27T01:17:07Z"
              }
            },
            "parent": {
              "id": 23,
              "key": "CBJWE4JX",
              "itemType": "conferencePaper",
              "title": "DETRs Beat YOLOs on Real-time Object Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "CBJWE4JX",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "DETRs Beat YOLOs on Real-time Object Detection",
                "date": "2024",
                "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.html",
                "accessDate": "2026-01-27T00:52:43Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: zhao_detrs-beat_2024",
                "pages": "16965-16974",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Yian",
                    "lastName": "Zhao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wenyu",
                    "lastName": "Lv",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shangliang",
                    "lastName": "Xu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jinman",
                    "lastName": "Wei",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Guanzhong",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Qingqing",
                    "lastName": "Dang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yi",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jie",
                    "lastName": "Chen",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:43Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/9VTZPVEB/Zhao 等 - 2024 - DETRs Beat YOLOs on Real-time Object Detection.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 34,
              "key": "4GZM2DR5",
              "itemType": "attachment",
              "title": "Zhao 等 - 2024 - DETRs Beat YOLOs on Real-time Object Detection.pdf",
              "libraryID": 1,
              "parentItemID": 23,
              "data": {
                "key": "4GZM2DR5",
                "version": 0,
                "itemType": "attachment",
                "title": "Zhao 等 - 2024 - DETRs Beat YOLOs on Real-time Object Detection.pdf",
                "url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf",
                "accessDate": "2026-01-27T00:52:47Z",
                "parentItem": "CBJWE4JX",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/4GZM2DR5/Zhao 等 - 2024 - DETRs Beat YOLOs on Real-time Object Detection.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:47Z",
                "dateModified": "2026-01-27T00:52:50Z"
              }
            },
            "parent": {
              "id": 23,
              "key": "CBJWE4JX",
              "itemType": "conferencePaper",
              "title": "DETRs Beat YOLOs on Real-time Object Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "CBJWE4JX",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "DETRs Beat YOLOs on Real-time Object Detection",
                "date": "2024",
                "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.html",
                "accessDate": "2026-01-27T00:52:43Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: zhao_detrs-beat_2024",
                "pages": "16965-16974",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Yian",
                    "lastName": "Zhao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wenyu",
                    "lastName": "Lv",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shangliang",
                    "lastName": "Xu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jinman",
                    "lastName": "Wei",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Guanzhong",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Qingqing",
                    "lastName": "Dang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yi",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jie",
                    "lastName": "Chen",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:43Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/4GZM2DR5/Zhao 等 - 2024 - DETRs Beat YOLOs on Real-time Object Detection.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 25,
          "key": "7AERRYC7",
          "itemType": "conferencePaper",
          "title": "DETRs with Collaborative Hybrid Assignments Training",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "7AERRYC7",
            "version": 0,
            "itemType": "conferencePaper",
            "title": "DETRs with Collaborative Hybrid Assignments Training",
            "date": "2023",
            "url": "https://openaccess.thecvf.com/content/ICCV2023/html/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.html",
            "accessDate": "2026-01-27T00:52:46Z",
            "language": "en",
            "libraryCatalog": "openaccess.thecvf.com",
            "extra": "Citation Key: zong_detrs-collaborative_2023",
            "pages": "6748-6758",
            "conferenceName": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "creators": [
              {
                "firstName": "Zhuofan",
                "lastName": "Zong",
                "creatorType": "author"
              },
              {
                "firstName": "Guanglu",
                "lastName": "Song",
                "creatorType": "author"
              },
              {
                "firstName": "Yu",
                "lastName": "Liu",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:52:46Z",
            "dateModified": "2026-01-27T10:22:49Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 87,
              "key": "8XWSN4K5",
              "itemType": "attachment",
              "title": "Zong 等 - 2023 - DETRs with Collaborative Hybrid Assignments Training.md",
              "libraryID": 1,
              "parentItemID": 25,
              "data": {
                "key": "8XWSN4K5",
                "version": 0,
                "itemType": "attachment",
                "title": "Zong 等 - 2023 - DETRs with Collaborative Hybrid Assignments Training.md",
                "parentItem": "7AERRYC7",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/8XWSN4K5/Zong 等 - 2023 - DETRs with Collaborative Hybrid Assignments Training.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:17:11Z",
                "dateModified": "2026-01-27T01:17:12Z"
              }
            },
            "parent": {
              "id": 25,
              "key": "7AERRYC7",
              "itemType": "conferencePaper",
              "title": "DETRs with Collaborative Hybrid Assignments Training",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "7AERRYC7",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "DETRs with Collaborative Hybrid Assignments Training",
                "date": "2023",
                "url": "https://openaccess.thecvf.com/content/ICCV2023/html/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.html",
                "accessDate": "2026-01-27T00:52:46Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: zong_detrs-collaborative_2023",
                "pages": "6748-6758",
                "conferenceName": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "creators": [
                  {
                    "firstName": "Zhuofan",
                    "lastName": "Zong",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Guanglu",
                    "lastName": "Song",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yu",
                    "lastName": "Liu",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:46Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/8XWSN4K5/Zong 等 - 2023 - DETRs with Collaborative Hybrid Assignments Training.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 32,
              "key": "KU3GJDWR",
              "itemType": "attachment",
              "title": "Zong 等 - 2023 - DETRs with Collaborative Hybrid Assignments Training.pdf",
              "libraryID": 1,
              "parentItemID": 25,
              "data": {
                "key": "KU3GJDWR",
                "version": 0,
                "itemType": "attachment",
                "title": "Zong 等 - 2023 - DETRs with Collaborative Hybrid Assignments Training.pdf",
                "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.pdf",
                "accessDate": "2026-01-27T00:52:48Z",
                "parentItem": "7AERRYC7",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/KU3GJDWR/Zong 等 - 2023 - DETRs with Collaborative Hybrid Assignments Training.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:48Z",
                "dateModified": "2026-01-27T00:52:49Z"
              }
            },
            "parent": {
              "id": 25,
              "key": "7AERRYC7",
              "itemType": "conferencePaper",
              "title": "DETRs with Collaborative Hybrid Assignments Training",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "7AERRYC7",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "DETRs with Collaborative Hybrid Assignments Training",
                "date": "2023",
                "url": "https://openaccess.thecvf.com/content/ICCV2023/html/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.html",
                "accessDate": "2026-01-27T00:52:46Z",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: zong_detrs-collaborative_2023",
                "pages": "6748-6758",
                "conferenceName": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "creators": [
                  {
                    "firstName": "Zhuofan",
                    "lastName": "Zong",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Guanglu",
                    "lastName": "Song",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yu",
                    "lastName": "Liu",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:46Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/KU3GJDWR/Zong 等 - 2023 - DETRs with Collaborative Hybrid Assignments Training.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 138,
          "key": "HPLZ65Z2",
          "itemType": "preprint",
          "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "HPLZ65Z2",
            "version": 8,
            "itemType": "preprint",
            "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
            "abstractNote": "We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising anch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\\textbf{+6.0}$\\textbf{AP} and $\\textbf{+2.7}$\\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017} ($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev} (\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \\url{https://github.com/IDEACVR/DINO}.",
            "date": "2022-07-11",
            "DOI": "10.48550/arXiv.2203.03605",
            "url": "http://arxiv.org/abs/2203.03605",
            "accessDate": "2026-01-03T14:46:50Z",
            "shortTitle": "DINO",
            "libraryCatalog": "arXiv.org",
            "extra": "arXiv:2203.03605 [cs]\nTLDR: DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction.\nCitation Key: zhang_dino-detr_2022",
            "repository": "arXiv",
            "archiveID": "arXiv:2203.03605",
            "creators": [
              {
                "firstName": "Hao",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Feng",
                "lastName": "Li",
                "creatorType": "author"
              },
              {
                "firstName": "Shilong",
                "lastName": "Liu",
                "creatorType": "author"
              },
              {
                "firstName": "Lei",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Hang",
                "lastName": "Su",
                "creatorType": "author"
              },
              {
                "firstName": "Jun",
                "lastName": "Zhu",
                "creatorType": "author"
              },
              {
                "firstName": "Lionel M.",
                "lastName": "Ni",
                "creatorType": "author"
              },
              {
                "firstName": "Heung-Yeung",
                "lastName": "Shum",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "Computer Science - Computer Vision and Pattern Recognition",
                "type": 1
              },
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:46:51Z",
            "dateModified": "2026-01-27T10:22:41Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 185,
              "key": "6RDCJXP3",
              "itemType": "attachment",
              "title": "Zhang 等 - 2022 - DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.md",
              "libraryID": 1,
              "parentItemID": 138,
              "data": {
                "key": "6RDCJXP3",
                "version": 85,
                "itemType": "attachment",
                "title": "Zhang 等 - 2022 - DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.md",
                "parentItem": "HPLZ65Z2",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/6RDCJXP3/Zhang 等 - 2022 - DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T10:01:07Z",
                "dateModified": "2026-01-16T10:01:08Z"
              }
            },
            "parent": {
              "id": 138,
              "key": "HPLZ65Z2",
              "itemType": "preprint",
              "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "HPLZ65Z2",
                "version": 8,
                "itemType": "preprint",
                "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
                "abstractNote": "We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising anch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\\textbf{+6.0}$\\textbf{AP} and $\\textbf{+2.7}$\\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017} ($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev} (\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \\url{https://github.com/IDEACVR/DINO}.",
                "date": "2022-07-11",
                "DOI": "10.48550/arXiv.2203.03605",
                "url": "http://arxiv.org/abs/2203.03605",
                "accessDate": "2026-01-03T14:46:50Z",
                "shortTitle": "DINO",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2203.03605 [cs]\nTLDR: DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction.\nCitation Key: zhang_dino-detr_2022",
                "repository": "arXiv",
                "archiveID": "arXiv:2203.03605",
                "creators": [
                  {
                    "firstName": "Hao",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Feng",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shilong",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lei",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hang",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jun",
                    "lastName": "Zhu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lionel M.",
                    "lastName": "Ni",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Heung-Yeung",
                    "lastName": "Shum",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:46:51Z",
                "dateModified": "2026-01-27T10:22:41Z"
              }
            },
            "filePath": "attachments/6RDCJXP3/Zhang 等 - 2022 - DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 201,
              "key": "C6J58W26",
              "itemType": "attachment",
              "title": "Zhang 等 - 2022 - DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.pdf",
              "libraryID": 1,
              "parentItemID": 138,
              "data": {
                "key": "C6J58W26",
                "version": 69,
                "itemType": "attachment",
                "title": "Zhang 等 - 2022 - DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.pdf",
                "url": "http://arxiv.org/pdf/2203.03605v4",
                "accessDate": "2026-01-03T14:46:52Z",
                "parentItem": "HPLZ65Z2",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/C6J58W26/Zhang 等 - 2022 - DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:46:52Z",
                "dateModified": "2026-01-03T14:46:53Z"
              }
            },
            "parent": {
              "id": 138,
              "key": "HPLZ65Z2",
              "itemType": "preprint",
              "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "HPLZ65Z2",
                "version": 8,
                "itemType": "preprint",
                "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
                "abstractNote": "We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising anch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\\textbf{+6.0}$\\textbf{AP} and $\\textbf{+2.7}$\\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017} ($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev} (\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \\url{https://github.com/IDEACVR/DINO}.",
                "date": "2022-07-11",
                "DOI": "10.48550/arXiv.2203.03605",
                "url": "http://arxiv.org/abs/2203.03605",
                "accessDate": "2026-01-03T14:46:50Z",
                "shortTitle": "DINO",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2203.03605 [cs]\nTLDR: DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction.\nCitation Key: zhang_dino-detr_2022",
                "repository": "arXiv",
                "archiveID": "arXiv:2203.03605",
                "creators": [
                  {
                    "firstName": "Hao",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Feng",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shilong",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lei",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hang",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jun",
                    "lastName": "Zhu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lionel M.",
                    "lastName": "Ni",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Heung-Yeung",
                    "lastName": "Shum",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:46:51Z",
                "dateModified": "2026-01-27T10:22:41Z"
              }
            },
            "filePath": "attachments/C6J58W26/Zhang 等 - 2022 - DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "Computer Science - Computer Vision and Pattern Recognition",
            "type": 1
          },
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 30,
          "key": "NXLIGKF5",
          "itemType": "conferencePaper",
          "title": "DN-DETR: Accelerate DETR Training by Introducing Query DeNoising",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "NXLIGKF5",
            "version": 0,
            "itemType": "conferencePaper",
            "title": "DN-DETR: Accelerate DETR Training by Introducing Query DeNoising",
            "date": "2022",
            "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.html",
            "accessDate": "2026-01-27T00:52:53Z",
            "shortTitle": "DN-DETR",
            "language": "en",
            "libraryCatalog": "openaccess.thecvf.com",
            "extra": "Citation Key: li_dndetr-accelerate_2022",
            "pages": "13619-13627",
            "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "creators": [
              {
                "firstName": "Feng",
                "lastName": "Li",
                "creatorType": "author"
              },
              {
                "firstName": "Hao",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Shilong",
                "lastName": "Liu",
                "creatorType": "author"
              },
              {
                "firstName": "Jian",
                "lastName": "Guo",
                "creatorType": "author"
              },
              {
                "firstName": "Lionel M.",
                "lastName": "Ni",
                "creatorType": "author"
              },
              {
                "firstName": "Lei",
                "lastName": "Zhang",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:52:53Z",
            "dateModified": "2026-01-27T10:22:49Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 89,
              "key": "7YXZJKNL",
              "itemType": "attachment",
              "title": "Li 等 - 2022 - DN-DETR Accelerate DETR Training by Introducing Query DeNoising.md",
              "libraryID": 1,
              "parentItemID": 30,
              "data": {
                "key": "7YXZJKNL",
                "version": 0,
                "itemType": "attachment",
                "title": "Li 等 - 2022 - DN-DETR Accelerate DETR Training by Introducing Query DeNoising.md",
                "parentItem": "NXLIGKF5",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/7YXZJKNL/Li 等 - 2022 - DN-DETR Accelerate DETR Training by Introducing Query DeNoising.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:17:24Z",
                "dateModified": "2026-01-27T01:17:25Z"
              }
            },
            "parent": {
              "id": 30,
              "key": "NXLIGKF5",
              "itemType": "conferencePaper",
              "title": "DN-DETR: Accelerate DETR Training by Introducing Query DeNoising",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "NXLIGKF5",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "DN-DETR: Accelerate DETR Training by Introducing Query DeNoising",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:52:53Z",
                "shortTitle": "DN-DETR",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: li_dndetr-accelerate_2022",
                "pages": "13619-13627",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Feng",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hao",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shilong",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Guo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lionel M.",
                    "lastName": "Ni",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lei",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:53Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/7YXZJKNL/Li 等 - 2022 - DN-DETR Accelerate DETR Training by Introducing Query DeNoising.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 36,
              "key": "YM5G5G2T",
              "itemType": "attachment",
              "title": "Li 等 - 2022 - DN-DETR Accelerate DETR Training by Introducing Query DeNoising.pdf",
              "libraryID": 1,
              "parentItemID": 30,
              "data": {
                "key": "YM5G5G2T",
                "version": 0,
                "itemType": "attachment",
                "title": "Li 等 - 2022 - DN-DETR Accelerate DETR Training by Introducing Query DeNoising.pdf",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.pdf",
                "accessDate": "2026-01-27T00:52:55Z",
                "parentItem": "NXLIGKF5",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/YM5G5G2T/Li 等 - 2022 - DN-DETR Accelerate DETR Training by Introducing Query DeNoising.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:55Z",
                "dateModified": "2026-01-27T00:52:56Z"
              }
            },
            "parent": {
              "id": 30,
              "key": "NXLIGKF5",
              "itemType": "conferencePaper",
              "title": "DN-DETR: Accelerate DETR Training by Introducing Query DeNoising",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "NXLIGKF5",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "DN-DETR: Accelerate DETR Training by Introducing Query DeNoising",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:52:53Z",
                "shortTitle": "DN-DETR",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: li_dndetr-accelerate_2022",
                "pages": "13619-13627",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Feng",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Hao",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shilong",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Guo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lionel M.",
                    "lastName": "Ni",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Lei",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:52:53Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/YM5G5G2T/Li 等 - 2022 - DN-DETR Accelerate DETR Training by Introducing Query DeNoising.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 135,
          "key": "EIMSDEU3",
          "itemType": "conferencePaper",
          "title": "End-to-End Object Detection with Transformers",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "EIMSDEU3",
            "version": 3,
            "itemType": "conferencePaper",
            "title": "End-to-End Object Detection with Transformers",
            "abstractNote": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
            "date": "2020",
            "DOI": "10.1007/978-3-030-58452-8_13",
            "language": "en",
            "libraryCatalog": "Springer Link",
            "extra": "TLDR: This work presents a new method that views object detection as a direct set prediction problem, and demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset.\nCitation Key: carion_endtoend-object_2020",
            "publisher": "Springer International Publishing",
            "place": "Cham",
            "ISBN": "978-3-030-58452-8",
            "pages": "213-229",
            "proceedingsTitle": "Computer Vision – ECCV 2020",
            "creators": [
              {
                "firstName": "Nicolas",
                "lastName": "Carion",
                "creatorType": "author"
              },
              {
                "firstName": "Francisco",
                "lastName": "Massa",
                "creatorType": "author"
              },
              {
                "firstName": "Gabriel",
                "lastName": "Synnaeve",
                "creatorType": "author"
              },
              {
                "firstName": "Nicolas",
                "lastName": "Usunier",
                "creatorType": "author"
              },
              {
                "firstName": "Alexander",
                "lastName": "Kirillov",
                "creatorType": "author"
              },
              {
                "firstName": "Sergey",
                "lastName": "Zagoruyko",
                "creatorType": "author"
              },
              {
                "firstName": "Andrea",
                "lastName": "Vedaldi",
                "creatorType": "editor"
              },
              {
                "firstName": "Horst",
                "lastName": "Bischof",
                "creatorType": "editor"
              },
              {
                "firstName": "Thomas",
                "lastName": "Brox",
                "creatorType": "editor"
              },
              {
                "firstName": "Jan-Michael",
                "lastName": "Frahm",
                "creatorType": "editor"
              }
            ],
            "tags": [
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:42:32Z",
            "dateModified": "2026-01-27T10:22:02Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 171,
              "key": "NJNNS2MB",
              "itemType": "attachment",
              "title": "Carion 等 - 2020 - End-to-End Object Detection with Transformers.md",
              "libraryID": 1,
              "parentItemID": 135,
              "data": {
                "key": "NJNNS2MB",
                "version": 112,
                "itemType": "attachment",
                "title": "Carion 等 - 2020 - End-to-End Object Detection with Transformers.md",
                "parentItem": "EIMSDEU3",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/NJNNS2MB/Carion 等 - 2020 - End-to-End Object Detection with Transformers.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T11:31:06Z",
                "dateModified": "2026-01-16T11:31:07Z"
              }
            },
            "parent": {
              "id": 135,
              "key": "EIMSDEU3",
              "itemType": "conferencePaper",
              "title": "End-to-End Object Detection with Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "EIMSDEU3",
                "version": 3,
                "itemType": "conferencePaper",
                "title": "End-to-End Object Detection with Transformers",
                "abstractNote": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
                "date": "2020",
                "DOI": "10.1007/978-3-030-58452-8_13",
                "language": "en",
                "libraryCatalog": "Springer Link",
                "extra": "TLDR: This work presents a new method that views object detection as a direct set prediction problem, and demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset.\nCitation Key: carion_endtoend-object_2020",
                "publisher": "Springer International Publishing",
                "place": "Cham",
                "ISBN": "978-3-030-58452-8",
                "pages": "213-229",
                "proceedingsTitle": "Computer Vision – ECCV 2020",
                "creators": [
                  {
                    "firstName": "Nicolas",
                    "lastName": "Carion",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Francisco",
                    "lastName": "Massa",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Gabriel",
                    "lastName": "Synnaeve",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Nicolas",
                    "lastName": "Usunier",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Alexander",
                    "lastName": "Kirillov",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Sergey",
                    "lastName": "Zagoruyko",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Andrea",
                    "lastName": "Vedaldi",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Horst",
                    "lastName": "Bischof",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Thomas",
                    "lastName": "Brox",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Jan-Michael",
                    "lastName": "Frahm",
                    "creatorType": "editor"
                  }
                ],
                "tags": [
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:42:32Z",
                "dateModified": "2026-01-27T10:22:02Z"
              }
            },
            "filePath": "attachments/NJNNS2MB/Carion 等 - 2020 - End-to-End Object Detection with Transformers.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 241,
              "key": "ZEY2RXUE",
              "itemType": "attachment",
              "title": "Carion 等 - 2020 - End-to-End Object Detection with Transformers.pdf",
              "libraryID": 1,
              "parentItemID": 135,
              "data": {
                "key": "ZEY2RXUE",
                "version": 69,
                "itemType": "attachment",
                "title": "Carion 等 - 2020 - End-to-End Object Detection with Transformers.pdf",
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-58452-8_13.pdf",
                "accessDate": "2026-01-03T14:42:35Z",
                "parentItem": "EIMSDEU3",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/ZEY2RXUE/Carion 等 - 2020 - End-to-End Object Detection with Transformers.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:42:35Z",
                "dateModified": "2026-01-03T14:42:35Z"
              }
            },
            "parent": {
              "id": 135,
              "key": "EIMSDEU3",
              "itemType": "conferencePaper",
              "title": "End-to-End Object Detection with Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "EIMSDEU3",
                "version": 3,
                "itemType": "conferencePaper",
                "title": "End-to-End Object Detection with Transformers",
                "abstractNote": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
                "date": "2020",
                "DOI": "10.1007/978-3-030-58452-8_13",
                "language": "en",
                "libraryCatalog": "Springer Link",
                "extra": "TLDR: This work presents a new method that views object detection as a direct set prediction problem, and demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset.\nCitation Key: carion_endtoend-object_2020",
                "publisher": "Springer International Publishing",
                "place": "Cham",
                "ISBN": "978-3-030-58452-8",
                "pages": "213-229",
                "proceedingsTitle": "Computer Vision – ECCV 2020",
                "creators": [
                  {
                    "firstName": "Nicolas",
                    "lastName": "Carion",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Francisco",
                    "lastName": "Massa",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Gabriel",
                    "lastName": "Synnaeve",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Nicolas",
                    "lastName": "Usunier",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Alexander",
                    "lastName": "Kirillov",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Sergey",
                    "lastName": "Zagoruyko",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Andrea",
                    "lastName": "Vedaldi",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Horst",
                    "lastName": "Bischof",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Thomas",
                    "lastName": "Brox",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Jan-Michael",
                    "lastName": "Frahm",
                    "creatorType": "editor"
                  }
                ],
                "tags": [
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:42:32Z",
                "dateModified": "2026-01-27T10:22:02Z"
              }
            },
            "filePath": "attachments/ZEY2RXUE/Carion 等 - 2020 - End-to-End Object Detection with Transformers.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 146,
          "key": "CGJBFE6C",
          "itemType": "preprint",
          "title": "Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "CGJBFE6C",
            "version": 4,
            "itemType": "preprint",
            "title": "Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation",
            "abstractNote": "The design of the query is crucial for the performance of DETR and its variants. Each query consists of two components: a content part and a positional one. Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance. In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation. The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling. This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects. However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones. To overcome this, we propose a query aggregation strategy to cooperate with SACQ. It merges similar predicted candidates from different queries, easing the optimization. Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP.",
            "date": "2024-05-06",
            "DOI": "10.48550/arXiv.2405.03318",
            "url": "http://arxiv.org/abs/2405.03318",
            "accessDate": "2026-01-03T14:43:07Z",
            "libraryCatalog": "arXiv.org",
            "extra": "arXiv:2405.03318 [cs]\nTLDR: A novel plug-and-play module, Self-Adaptive Content Query (SACQ), is introduced, which utilizes features from the transformer encoder to generate content queries via self-attention pooling to address sub-optimal performance of DETR and its variants.\nCitation Key: zhang_enhancing-detrs_2024",
            "repository": "arXiv",
            "archiveID": "arXiv:2405.03318",
            "creators": [
              {
                "firstName": "Yingying",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Chuangji",
                "lastName": "Shi",
                "creatorType": "author"
              },
              {
                "firstName": "Xin",
                "lastName": "Guo",
                "creatorType": "author"
              },
              {
                "firstName": "Jiangwei",
                "lastName": "Lao",
                "creatorType": "author"
              },
              {
                "firstName": "Jian",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Jiaotuan",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Jingdong",
                "lastName": "Chen",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "Computer Science - Computer Vision and Pattern Recognition",
                "type": 1
              },
              {
                "tag": "Computer Science - Multimedia",
                "type": 1
              },
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:43:07Z",
            "dateModified": "2026-01-27T10:22:07Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 234,
              "key": "CY6YGNHH",
              "itemType": "attachment",
              "title": "Snapshot",
              "libraryID": 1,
              "parentItemID": 146,
              "data": {
                "key": "CY6YGNHH",
                "version": 4,
                "itemType": "attachment",
                "title": "Snapshot",
                "url": "http://arxiv.org/abs/2405.03318",
                "accessDate": "2026-01-03T14:43:10Z",
                "parentItem": "CGJBFE6C",
                "linkMode": "imported_url",
                "contentType": "text/html",
                "charset": "utf-8",
                "filename": "2405.html",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:10Z",
                "dateModified": "2026-01-03T14:43:10Z",
                "path": "attachments/CY6YGNHH/Snapshot"
              }
            },
            "parent": {
              "id": 146,
              "key": "CGJBFE6C",
              "itemType": "preprint",
              "title": "Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "CGJBFE6C",
                "version": 4,
                "itemType": "preprint",
                "title": "Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation",
                "abstractNote": "The design of the query is crucial for the performance of DETR and its variants. Each query consists of two components: a content part and a positional one. Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance. In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation. The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling. This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects. However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones. To overcome this, we propose a query aggregation strategy to cooperate with SACQ. It merges similar predicted candidates from different queries, easing the optimization. Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP.",
                "date": "2024-05-06",
                "DOI": "10.48550/arXiv.2405.03318",
                "url": "http://arxiv.org/abs/2405.03318",
                "accessDate": "2026-01-03T14:43:07Z",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2405.03318 [cs]\nTLDR: A novel plug-and-play module, Self-Adaptive Content Query (SACQ), is introduced, which utilizes features from the transformer encoder to generate content queries via self-attention pooling to address sub-optimal performance of DETR and its variants.\nCitation Key: zhang_enhancing-detrs_2024",
                "repository": "arXiv",
                "archiveID": "arXiv:2405.03318",
                "creators": [
                  {
                    "firstName": "Yingying",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Chuangji",
                    "lastName": "Shi",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xin",
                    "lastName": "Guo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiangwei",
                    "lastName": "Lao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiaotuan",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jingdong",
                    "lastName": "Chen",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "Computer Science - Multimedia",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:07Z",
                "dateModified": "2026-01-27T10:22:07Z"
              }
            },
            "filePath": "attachments/CY6YGNHH/Snapshot",
            "mimeType": null
          },
          {
            "item": {
              "id": 181,
              "key": "9WX7UESK",
              "itemType": "attachment",
              "title": "Zhang 等 - 2024 - Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation.md",
              "libraryID": 1,
              "parentItemID": 146,
              "data": {
                "key": "9WX7UESK",
                "version": 93,
                "itemType": "attachment",
                "title": "Zhang 等 - 2024 - Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation.md",
                "parentItem": "CGJBFE6C",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/9WX7UESK/Zhang 等 - 2024 - Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T11:27:34Z",
                "dateModified": "2026-01-16T11:27:35Z"
              }
            },
            "parent": {
              "id": 146,
              "key": "CGJBFE6C",
              "itemType": "preprint",
              "title": "Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "CGJBFE6C",
                "version": 4,
                "itemType": "preprint",
                "title": "Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation",
                "abstractNote": "The design of the query is crucial for the performance of DETR and its variants. Each query consists of two components: a content part and a positional one. Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance. In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation. The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling. This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects. However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones. To overcome this, we propose a query aggregation strategy to cooperate with SACQ. It merges similar predicted candidates from different queries, easing the optimization. Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP.",
                "date": "2024-05-06",
                "DOI": "10.48550/arXiv.2405.03318",
                "url": "http://arxiv.org/abs/2405.03318",
                "accessDate": "2026-01-03T14:43:07Z",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2405.03318 [cs]\nTLDR: A novel plug-and-play module, Self-Adaptive Content Query (SACQ), is introduced, which utilizes features from the transformer encoder to generate content queries via self-attention pooling to address sub-optimal performance of DETR and its variants.\nCitation Key: zhang_enhancing-detrs_2024",
                "repository": "arXiv",
                "archiveID": "arXiv:2405.03318",
                "creators": [
                  {
                    "firstName": "Yingying",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Chuangji",
                    "lastName": "Shi",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xin",
                    "lastName": "Guo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiangwei",
                    "lastName": "Lao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiaotuan",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jingdong",
                    "lastName": "Chen",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "Computer Science - Multimedia",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:07Z",
                "dateModified": "2026-01-27T10:22:07Z"
              }
            },
            "filePath": "attachments/9WX7UESK/Zhang 等 - 2024 - Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 233,
              "key": "8BVUFWMZ",
              "itemType": "attachment",
              "title": "Zhang 等 - 2024 - Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation.pdf",
              "libraryID": 1,
              "parentItemID": 146,
              "data": {
                "key": "8BVUFWMZ",
                "version": 68,
                "itemType": "attachment",
                "title": "Zhang 等 - 2024 - Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation.pdf",
                "url": "http://arxiv.org/pdf/2405.03318v1",
                "accessDate": "2026-01-03T14:43:12Z",
                "parentItem": "CGJBFE6C",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/8BVUFWMZ/Zhang 等 - 2024 - Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:12Z",
                "dateModified": "2026-01-03T14:43:12Z"
              }
            },
            "parent": {
              "id": 146,
              "key": "CGJBFE6C",
              "itemType": "preprint",
              "title": "Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "CGJBFE6C",
                "version": 4,
                "itemType": "preprint",
                "title": "Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation",
                "abstractNote": "The design of the query is crucial for the performance of DETR and its variants. Each query consists of two components: a content part and a positional one. Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance. In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation. The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling. This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects. However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones. To overcome this, we propose a query aggregation strategy to cooperate with SACQ. It merges similar predicted candidates from different queries, easing the optimization. Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP.",
                "date": "2024-05-06",
                "DOI": "10.48550/arXiv.2405.03318",
                "url": "http://arxiv.org/abs/2405.03318",
                "accessDate": "2026-01-03T14:43:07Z",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2405.03318 [cs]\nTLDR: A novel plug-and-play module, Self-Adaptive Content Query (SACQ), is introduced, which utilizes features from the transformer encoder to generate content queries via self-attention pooling to address sub-optimal performance of DETR and its variants.\nCitation Key: zhang_enhancing-detrs_2024",
                "repository": "arXiv",
                "archiveID": "arXiv:2405.03318",
                "creators": [
                  {
                    "firstName": "Yingying",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Chuangji",
                    "lastName": "Shi",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xin",
                    "lastName": "Guo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiangwei",
                    "lastName": "Lao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiaotuan",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jingdong",
                    "lastName": "Chen",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "Computer Science - Multimedia",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:07Z",
                "dateModified": "2026-01-27T10:22:07Z"
              }
            },
            "filePath": "attachments/8BVUFWMZ/Zhang 等 - 2024 - Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation.pdf",
            "mimeType": null
          }
        ],
        "notes": [
          {
            "id": 235,
            "key": "AQI9V7P9",
            "itemType": "note",
            "title": "Comment: 11 pages, 7 figures",
            "libraryID": 1,
            "parentItemID": 146,
            "data": {
              "key": "AQI9V7P9",
              "version": 4,
              "itemType": "note",
              "parentItem": "CGJBFE6C",
              "note": "Comment: 11 pages, 7 figures",
              "tags": [],
              "relations": {},
              "dateAdded": "2026-01-03T14:43:07Z",
              "dateModified": "2026-01-03T14:43:07Z"
            },
            "note": "Comment: 11 pages, 7 figures"
          }
        ],
        "tags": [
          {
            "tag": "Computer Science - Computer Vision and Pattern Recognition",
            "type": 1
          },
          {
            "tag": "Computer Science - Multimedia",
            "type": 1
          },
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 45,
          "key": "M3AU5AC9",
          "itemType": "journalArticle",
          "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "M3AU5AC9",
            "version": 0,
            "itemType": "journalArticle",
            "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
            "abstractNote": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
            "date": "2017-06",
            "DOI": "10.1109/TPAMI.2016.2577031",
            "url": "https://ieeexplore.ieee.org/abstract/document/7485869",
            "accessDate": "2026-01-27T00:56:38Z",
            "shortTitle": "Faster R-CNN",
            "libraryCatalog": "IEEE Xplore",
            "extra": "TLDR: This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features.\nCitation Key: ren_faster-rcnn_2017",
            "volume": "39",
            "pages": "1137-1149",
            "publicationTitle": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "ISSN": "1939-3539",
            "issue": "6",
            "creators": [
              {
                "firstName": "Shaoqing",
                "lastName": "Ren",
                "creatorType": "author"
              },
              {
                "firstName": "Kaiming",
                "lastName": "He",
                "creatorType": "author"
              },
              {
                "firstName": "Ross",
                "lastName": "Girshick",
                "creatorType": "author"
              },
              {
                "firstName": "Jian",
                "lastName": "Sun",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              },
              {
                "tag": "Convolutional codes",
                "type": 1
              },
              {
                "tag": "convolutional neural network",
                "type": 1
              },
              {
                "tag": "Detectors",
                "type": 1
              },
              {
                "tag": "Feature extraction",
                "type": 1
              },
              {
                "tag": "Object detection",
                "type": 1
              },
              {
                "tag": "Proposals",
                "type": 1
              },
              {
                "tag": "region proposal",
                "type": 1
              },
              {
                "tag": "Search problems",
                "type": 1
              },
              {
                "tag": "Training",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:56:38Z",
            "dateModified": "2026-01-27T10:22:49Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 92,
              "key": "FDZ7J8AQ",
              "itemType": "attachment",
              "title": "Ren 等 - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.md",
              "libraryID": 1,
              "parentItemID": 45,
              "data": {
                "key": "FDZ7J8AQ",
                "version": 0,
                "itemType": "attachment",
                "title": "Ren 等 - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.md",
                "parentItem": "M3AU5AC9",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/FDZ7J8AQ/Ren 等 - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:17:43Z",
                "dateModified": "2026-01-27T01:17:43Z"
              }
            },
            "parent": {
              "id": 45,
              "key": "M3AU5AC9",
              "itemType": "journalArticle",
              "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "M3AU5AC9",
                "version": 0,
                "itemType": "journalArticle",
                "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
                "abstractNote": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
                "date": "2017-06",
                "DOI": "10.1109/TPAMI.2016.2577031",
                "url": "https://ieeexplore.ieee.org/abstract/document/7485869",
                "accessDate": "2026-01-27T00:56:38Z",
                "shortTitle": "Faster R-CNN",
                "libraryCatalog": "IEEE Xplore",
                "extra": "TLDR: This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features.\nCitation Key: ren_faster-rcnn_2017",
                "volume": "39",
                "pages": "1137-1149",
                "publicationTitle": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "ISSN": "1939-3539",
                "issue": "6",
                "creators": [
                  {
                    "firstName": "Shaoqing",
                    "lastName": "Ren",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Kaiming",
                    "lastName": "He",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ross",
                    "lastName": "Girshick",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Sun",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  },
                  {
                    "tag": "Convolutional codes",
                    "type": 1
                  },
                  {
                    "tag": "convolutional neural network",
                    "type": 1
                  },
                  {
                    "tag": "Detectors",
                    "type": 1
                  },
                  {
                    "tag": "Feature extraction",
                    "type": 1
                  },
                  {
                    "tag": "Object detection",
                    "type": 1
                  },
                  {
                    "tag": "Proposals",
                    "type": 1
                  },
                  {
                    "tag": "region proposal",
                    "type": 1
                  },
                  {
                    "tag": "Search problems",
                    "type": 1
                  },
                  {
                    "tag": "Training",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:38Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/FDZ7J8AQ/Ren 等 - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 56,
              "key": "HCZHAYPP",
              "itemType": "attachment",
              "title": "Ren 等 - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf",
              "libraryID": 1,
              "parentItemID": 45,
              "data": {
                "key": "HCZHAYPP",
                "version": 0,
                "itemType": "attachment",
                "title": "Ren 等 - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf",
                "url": "https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=7485869&ref=",
                "accessDate": "2026-01-27T00:56:44Z",
                "parentItem": "M3AU5AC9",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/HCZHAYPP/Ren 等 - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:44Z",
                "dateModified": "2026-01-27T00:56:48Z"
              }
            },
            "parent": {
              "id": 45,
              "key": "M3AU5AC9",
              "itemType": "journalArticle",
              "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "M3AU5AC9",
                "version": 0,
                "itemType": "journalArticle",
                "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
                "abstractNote": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
                "date": "2017-06",
                "DOI": "10.1109/TPAMI.2016.2577031",
                "url": "https://ieeexplore.ieee.org/abstract/document/7485869",
                "accessDate": "2026-01-27T00:56:38Z",
                "shortTitle": "Faster R-CNN",
                "libraryCatalog": "IEEE Xplore",
                "extra": "TLDR: This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features.\nCitation Key: ren_faster-rcnn_2017",
                "volume": "39",
                "pages": "1137-1149",
                "publicationTitle": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "ISSN": "1939-3539",
                "issue": "6",
                "creators": [
                  {
                    "firstName": "Shaoqing",
                    "lastName": "Ren",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Kaiming",
                    "lastName": "He",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ross",
                    "lastName": "Girshick",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Sun",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  },
                  {
                    "tag": "Convolutional codes",
                    "type": 1
                  },
                  {
                    "tag": "convolutional neural network",
                    "type": 1
                  },
                  {
                    "tag": "Detectors",
                    "type": 1
                  },
                  {
                    "tag": "Feature extraction",
                    "type": 1
                  },
                  {
                    "tag": "Object detection",
                    "type": 1
                  },
                  {
                    "tag": "Proposals",
                    "type": 1
                  },
                  {
                    "tag": "region proposal",
                    "type": 1
                  },
                  {
                    "tag": "Search problems",
                    "type": 1
                  },
                  {
                    "tag": "Training",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:38Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/HCZHAYPP/Ren 等 - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          },
          {
            "tag": "Convolutional codes",
            "type": 1
          },
          {
            "tag": "convolutional neural network",
            "type": 1
          },
          {
            "tag": "Detectors",
            "type": 1
          },
          {
            "tag": "Feature extraction",
            "type": 1
          },
          {
            "tag": "Object detection",
            "type": 1
          },
          {
            "tag": "Proposals",
            "type": 1
          },
          {
            "tag": "region proposal",
            "type": 1
          },
          {
            "tag": "Search problems",
            "type": 1
          },
          {
            "tag": "Training",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 143,
          "key": "4IUAXCCZ",
          "itemType": "preprint",
          "title": "Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "4IUAXCCZ",
            "version": 9,
            "itemType": "preprint",
            "title": "Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios",
            "abstractNote": "Underwater degraded images greatly challenge existing algorithms to detect objects of interest. Recently, researchers attempt to adopt attention mechanisms or composite connections for improving the feature representation of detectors. However, this solution does \\textit{not} eliminate the impact of degradation on image content such as color and texture, achieving minimal improvements. Another feasible solution for underwater object detection is to develop sophisticated deep architectures in order to enhance image quality or features. Nevertheless, the visually appealing output of these enhancement modules do \\textit{not} necessarily generate high accuracy for deep detectors. More recently, some multi-task learning methods jointly learn underwater detection and image enhancement, accessing promising improvements. Typically, these methods invoke huge architecture and expensive computations, rendering inefficient inference. Definitely, underwater object detection and image enhancement are two interrelated tasks. Leveraging information coming from the two tasks can benefit each task. Based on these factual opinions, we propose a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks. DPNet with one shared module and two task subnets learns from the two different tasks, seeking a shared representation. The shared representation provides more structural details for image enhancement and rich content information for object detection. Finally, we derive a cooperative training strategy to optimize parameters for DPNet. Extensive experiments on real-world and synthetic underwater datasets demonstrate that our method outputs visually favoring images and higher detection accuracy.",
            "date": "2023-07-07",
            "DOI": "10.48550/arXiv.2307.03536",
            "url": "http://arxiv.org/abs/2307.03536",
            "accessDate": "2026-01-03T14:47:19Z",
            "libraryCatalog": "arXiv.org",
            "extra": "arXiv:2307.03536 [cs]\nTLDR: This work proposes a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks.\nCitation Key: fu_joint-perceptual_2023",
            "repository": "arXiv",
            "archiveID": "arXiv:2307.03536",
            "creators": [
              {
                "firstName": "Chenping",
                "lastName": "Fu",
                "creatorType": "author"
              },
              {
                "firstName": "Wanqi",
                "lastName": "Yuan",
                "creatorType": "author"
              },
              {
                "firstName": "Jiewen",
                "lastName": "Xiao",
                "creatorType": "author"
              },
              {
                "firstName": "Risheng",
                "lastName": "Liu",
                "creatorType": "author"
              },
              {
                "firstName": "Xin",
                "lastName": "Fan",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "Computer Science - Computer Vision and Pattern Recognition",
                "type": 1
              },
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:47:19Z",
            "dateModified": "2026-01-27T10:22:18Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 179,
              "key": "G8XAS3KX",
              "itemType": "attachment",
              "title": "Fu 等 - 2023 - Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios.md",
              "libraryID": 1,
              "parentItemID": 143,
              "data": {
                "key": "G8XAS3KX",
                "version": 97,
                "itemType": "attachment",
                "title": "Fu 等 - 2023 - Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios.md",
                "parentItem": "4IUAXCCZ",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/G8XAS3KX/Fu 等 - 2023 - Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T11:28:15Z",
                "dateModified": "2026-01-16T11:28:16Z"
              }
            },
            "parent": {
              "id": 143,
              "key": "4IUAXCCZ",
              "itemType": "preprint",
              "title": "Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "4IUAXCCZ",
                "version": 9,
                "itemType": "preprint",
                "title": "Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios",
                "abstractNote": "Underwater degraded images greatly challenge existing algorithms to detect objects of interest. Recently, researchers attempt to adopt attention mechanisms or composite connections for improving the feature representation of detectors. However, this solution does \\textit{not} eliminate the impact of degradation on image content such as color and texture, achieving minimal improvements. Another feasible solution for underwater object detection is to develop sophisticated deep architectures in order to enhance image quality or features. Nevertheless, the visually appealing output of these enhancement modules do \\textit{not} necessarily generate high accuracy for deep detectors. More recently, some multi-task learning methods jointly learn underwater detection and image enhancement, accessing promising improvements. Typically, these methods invoke huge architecture and expensive computations, rendering inefficient inference. Definitely, underwater object detection and image enhancement are two interrelated tasks. Leveraging information coming from the two tasks can benefit each task. Based on these factual opinions, we propose a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks. DPNet with one shared module and two task subnets learns from the two different tasks, seeking a shared representation. The shared representation provides more structural details for image enhancement and rich content information for object detection. Finally, we derive a cooperative training strategy to optimize parameters for DPNet. Extensive experiments on real-world and synthetic underwater datasets demonstrate that our method outputs visually favoring images and higher detection accuracy.",
                "date": "2023-07-07",
                "DOI": "10.48550/arXiv.2307.03536",
                "url": "http://arxiv.org/abs/2307.03536",
                "accessDate": "2026-01-03T14:47:19Z",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2307.03536 [cs]\nTLDR: This work proposes a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks.\nCitation Key: fu_joint-perceptual_2023",
                "repository": "arXiv",
                "archiveID": "arXiv:2307.03536",
                "creators": [
                  {
                    "firstName": "Chenping",
                    "lastName": "Fu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wanqi",
                    "lastName": "Yuan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiewen",
                    "lastName": "Xiao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Risheng",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xin",
                    "lastName": "Fan",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:19Z",
                "dateModified": "2026-01-27T10:22:18Z"
              }
            },
            "filePath": "attachments/G8XAS3KX/Fu 等 - 2023 - Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 197,
              "key": "E39GK2KW",
              "itemType": "attachment",
              "title": "Fu 等 - 2023 - Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios.pdf",
              "libraryID": 1,
              "parentItemID": 143,
              "data": {
                "key": "E39GK2KW",
                "version": 68,
                "itemType": "attachment",
                "title": "Fu 等 - 2023 - Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios.pdf",
                "url": "http://arxiv.org/pdf/2307.03536v1",
                "accessDate": "2026-01-03T14:47:23Z",
                "parentItem": "4IUAXCCZ",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/E39GK2KW/Fu 等 - 2023 - Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:23Z",
                "dateModified": "2026-01-03T14:47:24Z"
              }
            },
            "parent": {
              "id": 143,
              "key": "4IUAXCCZ",
              "itemType": "preprint",
              "title": "Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "4IUAXCCZ",
                "version": 9,
                "itemType": "preprint",
                "title": "Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios",
                "abstractNote": "Underwater degraded images greatly challenge existing algorithms to detect objects of interest. Recently, researchers attempt to adopt attention mechanisms or composite connections for improving the feature representation of detectors. However, this solution does \\textit{not} eliminate the impact of degradation on image content such as color and texture, achieving minimal improvements. Another feasible solution for underwater object detection is to develop sophisticated deep architectures in order to enhance image quality or features. Nevertheless, the visually appealing output of these enhancement modules do \\textit{not} necessarily generate high accuracy for deep detectors. More recently, some multi-task learning methods jointly learn underwater detection and image enhancement, accessing promising improvements. Typically, these methods invoke huge architecture and expensive computations, rendering inefficient inference. Definitely, underwater object detection and image enhancement are two interrelated tasks. Leveraging information coming from the two tasks can benefit each task. Based on these factual opinions, we propose a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks. DPNet with one shared module and two task subnets learns from the two different tasks, seeking a shared representation. The shared representation provides more structural details for image enhancement and rich content information for object detection. Finally, we derive a cooperative training strategy to optimize parameters for DPNet. Extensive experiments on real-world and synthetic underwater datasets demonstrate that our method outputs visually favoring images and higher detection accuracy.",
                "date": "2023-07-07",
                "DOI": "10.48550/arXiv.2307.03536",
                "url": "http://arxiv.org/abs/2307.03536",
                "accessDate": "2026-01-03T14:47:19Z",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2307.03536 [cs]\nTLDR: This work proposes a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks.\nCitation Key: fu_joint-perceptual_2023",
                "repository": "arXiv",
                "archiveID": "arXiv:2307.03536",
                "creators": [
                  {
                    "firstName": "Chenping",
                    "lastName": "Fu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wanqi",
                    "lastName": "Yuan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiewen",
                    "lastName": "Xiao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Risheng",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xin",
                    "lastName": "Fan",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:19Z",
                "dateModified": "2026-01-27T10:22:18Z"
              }
            },
            "filePath": "attachments/E39GK2KW/Fu 等 - 2023 - Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios.pdf",
            "mimeType": null
          },
          {
            "item": {
              "id": 198,
              "key": "FBSSAXXH",
              "itemType": "attachment",
              "title": "Snapshot",
              "libraryID": 1,
              "parentItemID": 143,
              "data": {
                "key": "FBSSAXXH",
                "version": 9,
                "itemType": "attachment",
                "title": "Snapshot",
                "url": "http://arxiv.org/abs/2307.03536",
                "accessDate": "2026-01-03T14:47:23Z",
                "parentItem": "4IUAXCCZ",
                "linkMode": "imported_url",
                "contentType": "text/html",
                "charset": "utf-8",
                "filename": "2307.html",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:23Z",
                "dateModified": "2026-01-03T14:47:23Z",
                "path": "attachments/FBSSAXXH/Snapshot"
              }
            },
            "parent": {
              "id": 143,
              "key": "4IUAXCCZ",
              "itemType": "preprint",
              "title": "Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "4IUAXCCZ",
                "version": 9,
                "itemType": "preprint",
                "title": "Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios",
                "abstractNote": "Underwater degraded images greatly challenge existing algorithms to detect objects of interest. Recently, researchers attempt to adopt attention mechanisms or composite connections for improving the feature representation of detectors. However, this solution does \\textit{not} eliminate the impact of degradation on image content such as color and texture, achieving minimal improvements. Another feasible solution for underwater object detection is to develop sophisticated deep architectures in order to enhance image quality or features. Nevertheless, the visually appealing output of these enhancement modules do \\textit{not} necessarily generate high accuracy for deep detectors. More recently, some multi-task learning methods jointly learn underwater detection and image enhancement, accessing promising improvements. Typically, these methods invoke huge architecture and expensive computations, rendering inefficient inference. Definitely, underwater object detection and image enhancement are two interrelated tasks. Leveraging information coming from the two tasks can benefit each task. Based on these factual opinions, we propose a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks. DPNet with one shared module and two task subnets learns from the two different tasks, seeking a shared representation. The shared representation provides more structural details for image enhancement and rich content information for object detection. Finally, we derive a cooperative training strategy to optimize parameters for DPNet. Extensive experiments on real-world and synthetic underwater datasets demonstrate that our method outputs visually favoring images and higher detection accuracy.",
                "date": "2023-07-07",
                "DOI": "10.48550/arXiv.2307.03536",
                "url": "http://arxiv.org/abs/2307.03536",
                "accessDate": "2026-01-03T14:47:19Z",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2307.03536 [cs]\nTLDR: This work proposes a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks.\nCitation Key: fu_joint-perceptual_2023",
                "repository": "arXiv",
                "archiveID": "arXiv:2307.03536",
                "creators": [
                  {
                    "firstName": "Chenping",
                    "lastName": "Fu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wanqi",
                    "lastName": "Yuan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiewen",
                    "lastName": "Xiao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Risheng",
                    "lastName": "Liu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xin",
                    "lastName": "Fan",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:47:19Z",
                "dateModified": "2026-01-27T10:22:18Z"
              }
            },
            "filePath": "attachments/FBSSAXXH/Snapshot",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "Computer Science - Computer Vision and Pattern Recognition",
            "type": 1
          },
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 145,
          "key": "8838AH6P",
          "itemType": "preprint",
          "title": "LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "8838AH6P",
            "version": 3,
            "itemType": "preprint",
            "title": "LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection",
            "abstractNote": "In this paper, we present a light-weight detection transformer, LW-DETR, which outperforms YOLOs for real-time object detection. The architecture is a simple stack of a ViT encoder, a projector, and a shallow DETR decoder. Our approach leverages recent advanced techniques, such as training-effective techniques, e.g., improved loss and pretraining, and interleaved window and global attentions for reducing the ViT encoder complexity. We improve the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduce window-major feature map organization for improving the efficiency of interleaved attention computation. Experimental results demonstrate that the proposed approach is superior over existing real-time detectors, e.g., YOLO and its variants, on COCO and other benchmark datasets. Code and models are available at (https://github.com/Atten4Vis/LW-DETR).",
            "date": "2024-06-05",
            "DOI": "10.48550/arXiv.2406.03459",
            "url": "http://arxiv.org/abs/2406.03459",
            "accessDate": "2026-01-03T14:42:50Z",
            "shortTitle": "LW-DETR",
            "libraryCatalog": "arXiv.org",
            "extra": "arXiv:2406.03459 [cs]\nTLDR: This paper improves the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduces window-major feature map organization for improving the efficiency of interleaved attention computation.\nCitation Key: chen_lwdetr-transformer_2024",
            "repository": "arXiv",
            "archiveID": "arXiv:2406.03459",
            "creators": [
              {
                "firstName": "Qiang",
                "lastName": "Chen",
                "creatorType": "author"
              },
              {
                "firstName": "Xiangbo",
                "lastName": "Su",
                "creatorType": "author"
              },
              {
                "firstName": "Xinyu",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Jian",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Jiahui",
                "lastName": "Chen",
                "creatorType": "author"
              },
              {
                "firstName": "Yunpeng",
                "lastName": "Shen",
                "creatorType": "author"
              },
              {
                "firstName": "Chuchu",
                "lastName": "Han",
                "creatorType": "author"
              },
              {
                "firstName": "Ziliang",
                "lastName": "Chen",
                "creatorType": "author"
              },
              {
                "firstName": "Weixiang",
                "lastName": "Xu",
                "creatorType": "author"
              },
              {
                "firstName": "Fanrong",
                "lastName": "Li",
                "creatorType": "author"
              },
              {
                "firstName": "Shan",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Kun",
                "lastName": "Yao",
                "creatorType": "author"
              },
              {
                "firstName": "Errui",
                "lastName": "Ding",
                "creatorType": "author"
              },
              {
                "firstName": "Gang",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Jingdong",
                "lastName": "Wang",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "Computer Science - Computer Vision and Pattern Recognition",
                "type": 1
              },
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:42:50Z",
            "dateModified": "2026-01-27T10:22:13Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 180,
              "key": "5H6H4BMS",
              "itemType": "attachment",
              "title": "Chen 等 - 2024 - LW-DETR A Transformer Replacement to YOLO for Real-Time Detection.md",
              "libraryID": 1,
              "parentItemID": 145,
              "data": {
                "key": "5H6H4BMS",
                "version": 95,
                "itemType": "attachment",
                "title": "Chen 等 - 2024 - LW-DETR A Transformer Replacement to YOLO for Real-Time Detection.md",
                "parentItem": "8838AH6P",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/5H6H4BMS/Chen 等 - 2024 - LW-DETR A Transformer Replacement to YOLO for Real-Time Detection.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T11:27:58Z",
                "dateModified": "2026-01-16T11:27:58Z"
              }
            },
            "parent": {
              "id": 145,
              "key": "8838AH6P",
              "itemType": "preprint",
              "title": "LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "8838AH6P",
                "version": 3,
                "itemType": "preprint",
                "title": "LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection",
                "abstractNote": "In this paper, we present a light-weight detection transformer, LW-DETR, which outperforms YOLOs for real-time object detection. The architecture is a simple stack of a ViT encoder, a projector, and a shallow DETR decoder. Our approach leverages recent advanced techniques, such as training-effective techniques, e.g., improved loss and pretraining, and interleaved window and global attentions for reducing the ViT encoder complexity. We improve the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduce window-major feature map organization for improving the efficiency of interleaved attention computation. Experimental results demonstrate that the proposed approach is superior over existing real-time detectors, e.g., YOLO and its variants, on COCO and other benchmark datasets. Code and models are available at (https://github.com/Atten4Vis/LW-DETR).",
                "date": "2024-06-05",
                "DOI": "10.48550/arXiv.2406.03459",
                "url": "http://arxiv.org/abs/2406.03459",
                "accessDate": "2026-01-03T14:42:50Z",
                "shortTitle": "LW-DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2406.03459 [cs]\nTLDR: This paper improves the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduces window-major feature map organization for improving the efficiency of interleaved attention computation.\nCitation Key: chen_lwdetr-transformer_2024",
                "repository": "arXiv",
                "archiveID": "arXiv:2406.03459",
                "creators": [
                  {
                    "firstName": "Qiang",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiangbo",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xinyu",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiahui",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yunpeng",
                    "lastName": "Shen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Chuchu",
                    "lastName": "Han",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ziliang",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Weixiang",
                    "lastName": "Xu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Fanrong",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shan",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Kun",
                    "lastName": "Yao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Errui",
                    "lastName": "Ding",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Gang",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jingdong",
                    "lastName": "Wang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:42:50Z",
                "dateModified": "2026-01-27T10:22:13Z"
              }
            },
            "filePath": "attachments/5H6H4BMS/Chen 等 - 2024 - LW-DETR A Transformer Replacement to YOLO for Real-Time Detection.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 240,
              "key": "EWGE865Z",
              "itemType": "attachment",
              "title": "Chen 等 - 2024 - LW-DETR A Transformer Replacement to YOLO for Real-Time Detection.pdf",
              "libraryID": 1,
              "parentItemID": 145,
              "data": {
                "key": "EWGE865Z",
                "version": 68,
                "itemType": "attachment",
                "title": "Chen 等 - 2024 - LW-DETR A Transformer Replacement to YOLO for Real-Time Detection.pdf",
                "url": "http://arxiv.org/pdf/2406.03459v1",
                "accessDate": "2026-01-03T14:42:51Z",
                "parentItem": "8838AH6P",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/EWGE865Z/Chen 等 - 2024 - LW-DETR A Transformer Replacement to YOLO for Real-Time Detection.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:42:51Z",
                "dateModified": "2026-01-03T14:42:51Z"
              }
            },
            "parent": {
              "id": 145,
              "key": "8838AH6P",
              "itemType": "preprint",
              "title": "LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "8838AH6P",
                "version": 3,
                "itemType": "preprint",
                "title": "LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection",
                "abstractNote": "In this paper, we present a light-weight detection transformer, LW-DETR, which outperforms YOLOs for real-time object detection. The architecture is a simple stack of a ViT encoder, a projector, and a shallow DETR decoder. Our approach leverages recent advanced techniques, such as training-effective techniques, e.g., improved loss and pretraining, and interleaved window and global attentions for reducing the ViT encoder complexity. We improve the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduce window-major feature map organization for improving the efficiency of interleaved attention computation. Experimental results demonstrate that the proposed approach is superior over existing real-time detectors, e.g., YOLO and its variants, on COCO and other benchmark datasets. Code and models are available at (https://github.com/Atten4Vis/LW-DETR).",
                "date": "2024-06-05",
                "DOI": "10.48550/arXiv.2406.03459",
                "url": "http://arxiv.org/abs/2406.03459",
                "accessDate": "2026-01-03T14:42:50Z",
                "shortTitle": "LW-DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2406.03459 [cs]\nTLDR: This paper improves the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduces window-major feature map organization for improving the efficiency of interleaved attention computation.\nCitation Key: chen_lwdetr-transformer_2024",
                "repository": "arXiv",
                "archiveID": "arXiv:2406.03459",
                "creators": [
                  {
                    "firstName": "Qiang",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiangbo",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xinyu",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiahui",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yunpeng",
                    "lastName": "Shen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Chuchu",
                    "lastName": "Han",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ziliang",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Weixiang",
                    "lastName": "Xu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Fanrong",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shan",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Kun",
                    "lastName": "Yao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Errui",
                    "lastName": "Ding",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Gang",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jingdong",
                    "lastName": "Wang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:42:50Z",
                "dateModified": "2026-01-27T10:22:13Z"
              }
            },
            "filePath": "attachments/EWGE865Z/Chen 等 - 2024 - LW-DETR A Transformer Replacement to YOLO for Real-Time Detection.pdf",
            "mimeType": null
          },
          {
            "item": {
              "id": 239,
              "key": "RMW324UH",
              "itemType": "attachment",
              "title": "Snapshot",
              "libraryID": 1,
              "parentItemID": 145,
              "data": {
                "key": "RMW324UH",
                "version": 3,
                "itemType": "attachment",
                "title": "Snapshot",
                "url": "http://arxiv.org/abs/2406.03459",
                "accessDate": "2026-01-03T14:42:53Z",
                "parentItem": "8838AH6P",
                "linkMode": "imported_url",
                "contentType": "text/html",
                "charset": "utf-8",
                "filename": "2406.html",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:42:53Z",
                "dateModified": "2026-01-03T14:42:53Z",
                "path": "attachments/RMW324UH/Snapshot"
              }
            },
            "parent": {
              "id": 145,
              "key": "8838AH6P",
              "itemType": "preprint",
              "title": "LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "8838AH6P",
                "version": 3,
                "itemType": "preprint",
                "title": "LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection",
                "abstractNote": "In this paper, we present a light-weight detection transformer, LW-DETR, which outperforms YOLOs for real-time object detection. The architecture is a simple stack of a ViT encoder, a projector, and a shallow DETR decoder. Our approach leverages recent advanced techniques, such as training-effective techniques, e.g., improved loss and pretraining, and interleaved window and global attentions for reducing the ViT encoder complexity. We improve the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduce window-major feature map organization for improving the efficiency of interleaved attention computation. Experimental results demonstrate that the proposed approach is superior over existing real-time detectors, e.g., YOLO and its variants, on COCO and other benchmark datasets. Code and models are available at (https://github.com/Atten4Vis/LW-DETR).",
                "date": "2024-06-05",
                "DOI": "10.48550/arXiv.2406.03459",
                "url": "http://arxiv.org/abs/2406.03459",
                "accessDate": "2026-01-03T14:42:50Z",
                "shortTitle": "LW-DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2406.03459 [cs]\nTLDR: This paper improves the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduces window-major feature map organization for improving the efficiency of interleaved attention computation.\nCitation Key: chen_lwdetr-transformer_2024",
                "repository": "arXiv",
                "archiveID": "arXiv:2406.03459",
                "creators": [
                  {
                    "firstName": "Qiang",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiangbo",
                    "lastName": "Su",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xinyu",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jian",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jiahui",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yunpeng",
                    "lastName": "Shen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Chuchu",
                    "lastName": "Han",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ziliang",
                    "lastName": "Chen",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Weixiang",
                    "lastName": "Xu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Fanrong",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shan",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Kun",
                    "lastName": "Yao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Errui",
                    "lastName": "Ding",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Gang",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jingdong",
                    "lastName": "Wang",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:42:50Z",
                "dateModified": "2026-01-27T10:22:13Z"
              }
            },
            "filePath": "attachments/RMW324UH/Snapshot",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "Computer Science - Computer Vision and Pattern Recognition",
            "type": 1
          },
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 52,
          "key": "KSM65VAD",
          "itemType": "conferencePaper",
          "title": "MOTR: End-to-End Multiple-Object Tracking with Transformer",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "KSM65VAD",
            "version": 0,
            "itemType": "conferencePaper",
            "title": "MOTR: End-to-End Multiple-Object Tracking with Transformer",
            "abstractNote": "Temporal modeling of objects is a key challenge in multiple-object tracking (MOT). Existing methods track by associating detections through motion-based and appearance-based similarity heuristics. The post-processing nature of association prevents end-to-end exploitation of temporal variations in video sequence.",
            "date": "2022",
            "DOI": "10.1007/978-3-031-19812-0_38",
            "shortTitle": "MOTR",
            "language": "en",
            "libraryCatalog": "Springer Link",
            "extra": "TLDR: MOTR is proposed, which extends DETR and introduces track query to model the tracked instances in the entire video to enhance temporal relation modeling and serve as a stronger baseline for future research on temporal modeling and Transformer-based trackers.\nCitation Key: zeng_motr-endtoend_2022",
            "publisher": "Springer Nature Switzerland",
            "place": "Cham",
            "ISBN": "978-3-031-19812-0",
            "pages": "659-675",
            "proceedingsTitle": "Computer Vision – ECCV 2022",
            "creators": [
              {
                "firstName": "Fangao",
                "lastName": "Zeng",
                "creatorType": "author"
              },
              {
                "firstName": "Bin",
                "lastName": "Dong",
                "creatorType": "author"
              },
              {
                "firstName": "Yuang",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Tiancai",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Xiangyu",
                "lastName": "Zhang",
                "creatorType": "author"
              },
              {
                "firstName": "Yichen",
                "lastName": "Wei",
                "creatorType": "author"
              },
              {
                "firstName": "Shai",
                "lastName": "Avidan",
                "creatorType": "editor"
              },
              {
                "firstName": "Gabriel",
                "lastName": "Brostow",
                "creatorType": "editor"
              },
              {
                "firstName": "Moustapha",
                "lastName": "Cissé",
                "creatorType": "editor"
              },
              {
                "firstName": "Giovanni Maria",
                "lastName": "Farinella",
                "creatorType": "editor"
              },
              {
                "firstName": "Tal",
                "lastName": "Hassner",
                "creatorType": "editor"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              },
              {
                "tag": "End-to-End",
                "type": 1
              },
              {
                "tag": "Multiple-object tracking",
                "type": 1
              },
              {
                "tag": "Transformer",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:56:47Z",
            "dateModified": "2026-01-27T10:22:49Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 95,
              "key": "CB2UA9RT",
              "itemType": "attachment",
              "title": "Zeng 等 - 2022 - MOTR End-to-End Multiple-Object Tracking with Transformer.md",
              "libraryID": 1,
              "parentItemID": 52,
              "data": {
                "key": "CB2UA9RT",
                "version": 0,
                "itemType": "attachment",
                "title": "Zeng 等 - 2022 - MOTR End-to-End Multiple-Object Tracking with Transformer.md",
                "parentItem": "KSM65VAD",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/CB2UA9RT/Zeng 等 - 2022 - MOTR End-to-End Multiple-Object Tracking with Transformer.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:18:25Z",
                "dateModified": "2026-01-27T01:18:25Z"
              }
            },
            "parent": {
              "id": 52,
              "key": "KSM65VAD",
              "itemType": "conferencePaper",
              "title": "MOTR: End-to-End Multiple-Object Tracking with Transformer",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "KSM65VAD",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "MOTR: End-to-End Multiple-Object Tracking with Transformer",
                "abstractNote": "Temporal modeling of objects is a key challenge in multiple-object tracking (MOT). Existing methods track by associating detections through motion-based and appearance-based similarity heuristics. The post-processing nature of association prevents end-to-end exploitation of temporal variations in video sequence.",
                "date": "2022",
                "DOI": "10.1007/978-3-031-19812-0_38",
                "shortTitle": "MOTR",
                "language": "en",
                "libraryCatalog": "Springer Link",
                "extra": "TLDR: MOTR is proposed, which extends DETR and introduces track query to model the tracked instances in the entire video to enhance temporal relation modeling and serve as a stronger baseline for future research on temporal modeling and Transformer-based trackers.\nCitation Key: zeng_motr-endtoend_2022",
                "publisher": "Springer Nature Switzerland",
                "place": "Cham",
                "ISBN": "978-3-031-19812-0",
                "pages": "659-675",
                "proceedingsTitle": "Computer Vision – ECCV 2022",
                "creators": [
                  {
                    "firstName": "Fangao",
                    "lastName": "Zeng",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Bin",
                    "lastName": "Dong",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yuang",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tiancai",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiangyu",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yichen",
                    "lastName": "Wei",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shai",
                    "lastName": "Avidan",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Gabriel",
                    "lastName": "Brostow",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Moustapha",
                    "lastName": "Cissé",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Giovanni Maria",
                    "lastName": "Farinella",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Tal",
                    "lastName": "Hassner",
                    "creatorType": "editor"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  },
                  {
                    "tag": "End-to-End",
                    "type": 1
                  },
                  {
                    "tag": "Multiple-object tracking",
                    "type": 1
                  },
                  {
                    "tag": "Transformer",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:47Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/CB2UA9RT/Zeng 等 - 2022 - MOTR End-to-End Multiple-Object Tracking with Transformer.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 59,
              "key": "TFYBKXJL",
              "itemType": "attachment",
              "title": "Zeng 等 - 2022 - MOTR End-to-End Multiple-Object Tracking with Transformer.pdf",
              "libraryID": 1,
              "parentItemID": 52,
              "data": {
                "key": "TFYBKXJL",
                "version": 0,
                "itemType": "attachment",
                "title": "Zeng 等 - 2022 - MOTR End-to-End Multiple-Object Tracking with Transformer.pdf",
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-031-19812-0_38.pdf",
                "accessDate": "2026-01-27T00:56:53Z",
                "parentItem": "KSM65VAD",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/TFYBKXJL/Zeng 等 - 2022 - MOTR End-to-End Multiple-Object Tracking with Transformer.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:53Z",
                "dateModified": "2026-01-27T00:56:54Z"
              }
            },
            "parent": {
              "id": 52,
              "key": "KSM65VAD",
              "itemType": "conferencePaper",
              "title": "MOTR: End-to-End Multiple-Object Tracking with Transformer",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "KSM65VAD",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "MOTR: End-to-End Multiple-Object Tracking with Transformer",
                "abstractNote": "Temporal modeling of objects is a key challenge in multiple-object tracking (MOT). Existing methods track by associating detections through motion-based and appearance-based similarity heuristics. The post-processing nature of association prevents end-to-end exploitation of temporal variations in video sequence.",
                "date": "2022",
                "DOI": "10.1007/978-3-031-19812-0_38",
                "shortTitle": "MOTR",
                "language": "en",
                "libraryCatalog": "Springer Link",
                "extra": "TLDR: MOTR is proposed, which extends DETR and introduces track query to model the tracked instances in the entire video to enhance temporal relation modeling and serve as a stronger baseline for future research on temporal modeling and Transformer-based trackers.\nCitation Key: zeng_motr-endtoend_2022",
                "publisher": "Springer Nature Switzerland",
                "place": "Cham",
                "ISBN": "978-3-031-19812-0",
                "pages": "659-675",
                "proceedingsTitle": "Computer Vision – ECCV 2022",
                "creators": [
                  {
                    "firstName": "Fangao",
                    "lastName": "Zeng",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Bin",
                    "lastName": "Dong",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yuang",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tiancai",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Xiangyu",
                    "lastName": "Zhang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yichen",
                    "lastName": "Wei",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Shai",
                    "lastName": "Avidan",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Gabriel",
                    "lastName": "Brostow",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Moustapha",
                    "lastName": "Cissé",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Giovanni Maria",
                    "lastName": "Farinella",
                    "creatorType": "editor"
                  },
                  {
                    "firstName": "Tal",
                    "lastName": "Hassner",
                    "creatorType": "editor"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  },
                  {
                    "tag": "End-to-End",
                    "type": 1
                  },
                  {
                    "tag": "Multiple-object tracking",
                    "type": 1
                  },
                  {
                    "tag": "Transformer",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:47Z",
                "dateModified": "2026-01-27T10:22:49Z"
              }
            },
            "filePath": "attachments/TFYBKXJL/Zeng 等 - 2022 - MOTR End-to-End Multiple-Object Tracking with Transformer.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          },
          {
            "tag": "End-to-End",
            "type": 1
          },
          {
            "tag": "Multiple-object tracking",
            "type": 1
          },
          {
            "tag": "Transformer",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 57,
          "key": "RPRBE2QN",
          "itemType": "conferencePaper",
          "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "RPRBE2QN",
            "version": 0,
            "itemType": "conferencePaper",
            "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
            "date": "2022",
            "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html",
            "accessDate": "2026-01-27T00:56:52Z",
            "shortTitle": "Panoptic SegFormer",
            "language": "en",
            "libraryCatalog": "openaccess.thecvf.com",
            "extra": "Citation Key: li_panoptic-segformer_2022",
            "pages": "1280-1289",
            "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "creators": [
              {
                "firstName": "Zhiqi",
                "lastName": "Li",
                "creatorType": "author"
              },
              {
                "firstName": "Wenhai",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Enze",
                "lastName": "Xie",
                "creatorType": "author"
              },
              {
                "firstName": "Zhiding",
                "lastName": "Yu",
                "creatorType": "author"
              },
              {
                "firstName": "Anima",
                "lastName": "Anandkumar",
                "creatorType": "author"
              },
              {
                "firstName": "Jose M.",
                "lastName": "Alvarez",
                "creatorType": "author"
              },
              {
                "firstName": "Ping",
                "lastName": "Luo",
                "creatorType": "author"
              },
              {
                "firstName": "Tong",
                "lastName": "Lu",
                "creatorType": "author"
              }
            ],
            "tags": [],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:56:52Z",
            "dateModified": "2026-01-27T10:22:50Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 96,
              "key": "NWU22TPK",
              "itemType": "attachment",
              "title": "Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.md",
              "libraryID": 1,
              "parentItemID": 57,
              "data": {
                "key": "NWU22TPK",
                "version": 0,
                "itemType": "attachment",
                "title": "Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.md",
                "parentItem": "RPRBE2QN",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/NWU22TPK/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:18:30Z",
                "dateModified": "2026-01-27T01:18:30Z"
              }
            },
            "parent": {
              "id": 57,
              "key": "RPRBE2QN",
              "itemType": "conferencePaper",
              "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "RPRBE2QN",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:56:52Z",
                "shortTitle": "Panoptic SegFormer",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: li_panoptic-segformer_2022",
                "pages": "1280-1289",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Zhiqi",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wenhai",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Enze",
                    "lastName": "Xie",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zhiding",
                    "lastName": "Yu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Anima",
                    "lastName": "Anandkumar",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jose M.",
                    "lastName": "Alvarez",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ping",
                    "lastName": "Luo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tong",
                    "lastName": "Lu",
                    "creatorType": "author"
                  }
                ],
                "tags": [],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:52Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/NWU22TPK/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 247,
              "key": "WC7HIYMF",
              "itemType": "attachment",
              "title": "Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.dual.md",
              "libraryID": 1,
              "parentItemID": 57,
              "data": {
                "key": "WC7HIYMF",
                "version": 0,
                "itemType": "attachment",
                "title": "Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.dual.md",
                "parentItem": "RPRBE2QN",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/WC7HIYMF/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.dual.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T10:11:46Z",
                "dateModified": "2026-01-27T10:11:47Z"
              }
            },
            "parent": {
              "id": 57,
              "key": "RPRBE2QN",
              "itemType": "conferencePaper",
              "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "RPRBE2QN",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:56:52Z",
                "shortTitle": "Panoptic SegFormer",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: li_panoptic-segformer_2022",
                "pages": "1280-1289",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Zhiqi",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wenhai",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Enze",
                    "lastName": "Xie",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zhiding",
                    "lastName": "Yu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Anima",
                    "lastName": "Anandkumar",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jose M.",
                    "lastName": "Alvarez",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ping",
                    "lastName": "Luo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tong",
                    "lastName": "Lu",
                    "creatorType": "author"
                  }
                ],
                "tags": [],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:52Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/WC7HIYMF/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.dual.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 248,
              "key": "8L4FD5K5",
              "itemType": "attachment",
              "title": "Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.mono.md",
              "libraryID": 1,
              "parentItemID": 57,
              "data": {
                "key": "8L4FD5K5",
                "version": 0,
                "itemType": "attachment",
                "title": "Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.mono.md",
                "parentItem": "RPRBE2QN",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/8L4FD5K5/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.mono.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T10:11:52Z",
                "dateModified": "2026-01-27T10:11:53Z"
              }
            },
            "parent": {
              "id": 57,
              "key": "RPRBE2QN",
              "itemType": "conferencePaper",
              "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "RPRBE2QN",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:56:52Z",
                "shortTitle": "Panoptic SegFormer",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: li_panoptic-segformer_2022",
                "pages": "1280-1289",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Zhiqi",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wenhai",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Enze",
                    "lastName": "Xie",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zhiding",
                    "lastName": "Yu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Anima",
                    "lastName": "Anandkumar",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jose M.",
                    "lastName": "Alvarez",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ping",
                    "lastName": "Luo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tong",
                    "lastName": "Lu",
                    "creatorType": "author"
                  }
                ],
                "tags": [],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:52Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/8L4FD5K5/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.mono.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 61,
              "key": "SMFVBYXT",
              "itemType": "attachment",
              "title": "Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.pdf",
              "libraryID": 1,
              "parentItemID": 57,
              "data": {
                "key": "SMFVBYXT",
                "version": 0,
                "itemType": "attachment",
                "title": "Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.pdf",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.pdf",
                "accessDate": "2026-01-27T00:57:05Z",
                "parentItem": "RPRBE2QN",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/SMFVBYXT/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:57:05Z",
                "dateModified": "2026-01-27T00:57:19Z"
              }
            },
            "parent": {
              "id": 57,
              "key": "RPRBE2QN",
              "itemType": "conferencePaper",
              "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "RPRBE2QN",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:56:52Z",
                "shortTitle": "Panoptic SegFormer",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: li_panoptic-segformer_2022",
                "pages": "1280-1289",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Zhiqi",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wenhai",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Enze",
                    "lastName": "Xie",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zhiding",
                    "lastName": "Yu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Anima",
                    "lastName": "Anandkumar",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jose M.",
                    "lastName": "Alvarez",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ping",
                    "lastName": "Luo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tong",
                    "lastName": "Lu",
                    "creatorType": "author"
                  }
                ],
                "tags": [],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:52Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/SMFVBYXT/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.pdf",
            "mimeType": null
          },
          {
            "item": {
              "id": 246,
              "key": "J3PD2ISP",
              "itemType": "attachment",
              "title": "Panoptic SegFormer-deepseek-dual",
              "libraryID": 1,
              "parentItemID": 57,
              "data": {
                "key": "J3PD2ISP",
                "version": 0,
                "itemType": "attachment",
                "title": "Panoptic SegFormer-deepseek-dual",
                "parentItem": "RPRBE2QN",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/J3PD2ISP/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.dual.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T10:08:56Z",
                "dateModified": "2026-01-27T10:08:56Z"
              }
            },
            "parent": {
              "id": 57,
              "key": "RPRBE2QN",
              "itemType": "conferencePaper",
              "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "RPRBE2QN",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:56:52Z",
                "shortTitle": "Panoptic SegFormer",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: li_panoptic-segformer_2022",
                "pages": "1280-1289",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Zhiqi",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wenhai",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Enze",
                    "lastName": "Xie",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zhiding",
                    "lastName": "Yu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Anima",
                    "lastName": "Anandkumar",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jose M.",
                    "lastName": "Alvarez",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ping",
                    "lastName": "Luo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tong",
                    "lastName": "Lu",
                    "creatorType": "author"
                  }
                ],
                "tags": [],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:52Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/J3PD2ISP/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.dual.pdf",
            "mimeType": null
          },
          {
            "item": {
              "id": 244,
              "key": "YTWMBNNW",
              "itemType": "attachment",
              "title": "Panoptic SegFormer-deepseek-mono",
              "libraryID": 1,
              "parentItemID": 57,
              "data": {
                "key": "YTWMBNNW",
                "version": 0,
                "itemType": "attachment",
                "title": "Panoptic SegFormer-deepseek-mono",
                "parentItem": "RPRBE2QN",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/YTWMBNNW/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.mono.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T10:08:55Z",
                "dateModified": "2026-01-27T10:08:55Z"
              }
            },
            "parent": {
              "id": 57,
              "key": "RPRBE2QN",
              "itemType": "conferencePaper",
              "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "RPRBE2QN",
                "version": 0,
                "itemType": "conferencePaper",
                "title": "Panoptic SegFormer: Delving Deeper Into Panoptic Segmentation With Transformers",
                "date": "2022",
                "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html",
                "accessDate": "2026-01-27T00:56:52Z",
                "shortTitle": "Panoptic SegFormer",
                "language": "en",
                "libraryCatalog": "openaccess.thecvf.com",
                "extra": "Citation Key: li_panoptic-segformer_2022",
                "pages": "1280-1289",
                "conferenceName": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "creators": [
                  {
                    "firstName": "Zhiqi",
                    "lastName": "Li",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wenhai",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Enze",
                    "lastName": "Xie",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Zhiding",
                    "lastName": "Yu",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Anima",
                    "lastName": "Anandkumar",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Jose M.",
                    "lastName": "Alvarez",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ping",
                    "lastName": "Luo",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Tong",
                    "lastName": "Lu",
                    "creatorType": "author"
                  }
                ],
                "tags": [],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:56:52Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/YTWMBNNW/Li 等 - 2022 - Panoptic SegFormer Delving Deeper Into Panoptic Segmentation With Transformers.no_watermark.zh-CN.mono.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 76,
          "key": "VI9JURUB",
          "itemType": "journalArticle",
          "title": "Rethinking detection based table structure recognition for visually rich document images",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "VI9JURUB",
            "version": 0,
            "itemType": "journalArticle",
            "title": "Rethinking detection based table structure recognition for visually rich document images",
            "abstractNote": "Detection models have been extensively employed for the Table Structure Recognition (TSR) task, aiming to convert table images into structured formats by detecting table components such as Columns and Rows. However, prevailing detection-based TSR models usually cannot perform well regarding cell-level metrics, such as TEDS, and the reasons hindering their performance are not thoroughly explored. Therefore, we first examine the underlying reasons impeding these models’ performance and find that the key issues are the improper problem formulation, the mismatch issue of detection and TSR metrics, the inherent characteristics of detection models, and the influence of local and long-range feature extraction. Based on these findings, we propose a tailored Cascade R-CNN based solution by introducing a new problem formulation, tuning the proposal generation, and applying deformation convolution and the proposed Spatial Attention Module. The experimental results show that our proposed model can improve the base Cascade R-CNN model by 19.32%, 11.56%, and 14.77% on the SciTSR, FinTabNet, and PubTables1M datasets regarding the structure-only TEDS, achieving state-of-the-art performance, demonstrating that our findings can serve as a valuable guide for enhancing detection-based TSR models. Our code and pre-trained models are public available11https://github.com/uobinxiao/CascadeTSRDet..",
            "date": "2025-04-15",
            "DOI": "10.1016/j.eswa.2025.126461",
            "url": "https://www.sciencedirect.com/science/article/pii/S0957417425000831",
            "accessDate": "2026-01-27T00:59:24Z",
            "libraryCatalog": "ScienceDirect",
            "extra": "TLDR: The limitations of existing detection-based solutions are revisited, two-stage and transformer-based detection models are compared, and the key design aspects for the success of a two-stage detection model for the TSR task are identified, including the multi-class problem definition, the aspect ratio for anchor box generation, and the feature generation of the backbone network.\nCitation Key: xiao_rethinking-detection_2025",
            "volume": "269",
            "pages": "126461",
            "publicationTitle": "Expert Systems with Applications",
            "ISSN": "0957-4174",
            "journalAbbreviation": "Expert Systems with Applications",
            "creators": [
              {
                "firstName": "Bin",
                "lastName": "Xiao",
                "creatorType": "author"
              },
              {
                "firstName": "Murat",
                "lastName": "Simsek",
                "creatorType": "author"
              },
              {
                "firstName": "Burak",
                "lastName": "Kantarci",
                "creatorType": "author"
              },
              {
                "firstName": "Ala Abu",
                "lastName": "Alkheir",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "/unread",
                "type": 1
              },
              {
                "tag": "Object detection",
                "type": 1
              },
              {
                "tag": "Document processing",
                "type": 1
              },
              {
                "tag": "Information extraction",
                "type": 1
              },
              {
                "tag": "Table structure recognition",
                "type": 1
              },
              {
                "tag": "Visually rich document understanding",
                "type": 1
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:59:24Z",
            "dateModified": "2026-01-27T10:22:50Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 77,
              "key": "XD74FR7G",
              "itemType": "attachment",
              "title": "ScienceDirect Snapshot",
              "libraryID": 1,
              "parentItemID": 76,
              "data": {
                "key": "XD74FR7G",
                "version": 0,
                "itemType": "attachment",
                "title": "ScienceDirect Snapshot",
                "url": "https://www.sciencedirect.com/science/article/pii/S0957417425000831",
                "accessDate": "2026-01-27T00:59:33Z",
                "parentItem": "VI9JURUB",
                "linkMode": "imported_url",
                "contentType": "text/html",
                "charset": "utf-8",
                "filename": "S0957417425000831.html",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:59:33Z",
                "dateModified": "2026-01-27T00:59:33Z",
                "path": "attachments/XD74FR7G/S0957417425000831.html"
              }
            },
            "parent": {
              "id": 76,
              "key": "VI9JURUB",
              "itemType": "journalArticle",
              "title": "Rethinking detection based table structure recognition for visually rich document images",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "VI9JURUB",
                "version": 0,
                "itemType": "journalArticle",
                "title": "Rethinking detection based table structure recognition for visually rich document images",
                "abstractNote": "Detection models have been extensively employed for the Table Structure Recognition (TSR) task, aiming to convert table images into structured formats by detecting table components such as Columns and Rows. However, prevailing detection-based TSR models usually cannot perform well regarding cell-level metrics, such as TEDS, and the reasons hindering their performance are not thoroughly explored. Therefore, we first examine the underlying reasons impeding these models’ performance and find that the key issues are the improper problem formulation, the mismatch issue of detection and TSR metrics, the inherent characteristics of detection models, and the influence of local and long-range feature extraction. Based on these findings, we propose a tailored Cascade R-CNN based solution by introducing a new problem formulation, tuning the proposal generation, and applying deformation convolution and the proposed Spatial Attention Module. The experimental results show that our proposed model can improve the base Cascade R-CNN model by 19.32%, 11.56%, and 14.77% on the SciTSR, FinTabNet, and PubTables1M datasets regarding the structure-only TEDS, achieving state-of-the-art performance, demonstrating that our findings can serve as a valuable guide for enhancing detection-based TSR models. Our code and pre-trained models are public available11https://github.com/uobinxiao/CascadeTSRDet..",
                "date": "2025-04-15",
                "DOI": "10.1016/j.eswa.2025.126461",
                "url": "https://www.sciencedirect.com/science/article/pii/S0957417425000831",
                "accessDate": "2026-01-27T00:59:24Z",
                "libraryCatalog": "ScienceDirect",
                "extra": "TLDR: The limitations of existing detection-based solutions are revisited, two-stage and transformer-based detection models are compared, and the key design aspects for the success of a two-stage detection model for the TSR task are identified, including the multi-class problem definition, the aspect ratio for anchor box generation, and the feature generation of the backbone network.\nCitation Key: xiao_rethinking-detection_2025",
                "volume": "269",
                "pages": "126461",
                "publicationTitle": "Expert Systems with Applications",
                "ISSN": "0957-4174",
                "journalAbbreviation": "Expert Systems with Applications",
                "creators": [
                  {
                    "firstName": "Bin",
                    "lastName": "Xiao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Murat",
                    "lastName": "Simsek",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Burak",
                    "lastName": "Kantarci",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ala Abu",
                    "lastName": "Alkheir",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  },
                  {
                    "tag": "Object detection",
                    "type": 1
                  },
                  {
                    "tag": "Document processing",
                    "type": 1
                  },
                  {
                    "tag": "Information extraction",
                    "type": 1
                  },
                  {
                    "tag": "Table structure recognition",
                    "type": 1
                  },
                  {
                    "tag": "Visually rich document understanding",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:59:24Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/XD74FR7G/S0957417425000831.html",
            "mimeType": null
          },
          {
            "item": {
              "id": 97,
              "key": "LVBBEES6",
              "itemType": "attachment",
              "title": "Xiao 等 - 2025 - Rethinking detection based table structure recognition for visually rich document images.md",
              "libraryID": 1,
              "parentItemID": 76,
              "data": {
                "key": "LVBBEES6",
                "version": 0,
                "itemType": "attachment",
                "title": "Xiao 等 - 2025 - Rethinking detection based table structure recognition for visually rich document images.md",
                "parentItem": "VI9JURUB",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/LVBBEES6/Xiao 等 - 2025 - Rethinking detection based table structure recognition for visually rich document images.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T01:18:37Z",
                "dateModified": "2026-01-27T01:18:37Z"
              }
            },
            "parent": {
              "id": 76,
              "key": "VI9JURUB",
              "itemType": "journalArticle",
              "title": "Rethinking detection based table structure recognition for visually rich document images",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "VI9JURUB",
                "version": 0,
                "itemType": "journalArticle",
                "title": "Rethinking detection based table structure recognition for visually rich document images",
                "abstractNote": "Detection models have been extensively employed for the Table Structure Recognition (TSR) task, aiming to convert table images into structured formats by detecting table components such as Columns and Rows. However, prevailing detection-based TSR models usually cannot perform well regarding cell-level metrics, such as TEDS, and the reasons hindering their performance are not thoroughly explored. Therefore, we first examine the underlying reasons impeding these models’ performance and find that the key issues are the improper problem formulation, the mismatch issue of detection and TSR metrics, the inherent characteristics of detection models, and the influence of local and long-range feature extraction. Based on these findings, we propose a tailored Cascade R-CNN based solution by introducing a new problem formulation, tuning the proposal generation, and applying deformation convolution and the proposed Spatial Attention Module. The experimental results show that our proposed model can improve the base Cascade R-CNN model by 19.32%, 11.56%, and 14.77% on the SciTSR, FinTabNet, and PubTables1M datasets regarding the structure-only TEDS, achieving state-of-the-art performance, demonstrating that our findings can serve as a valuable guide for enhancing detection-based TSR models. Our code and pre-trained models are public available11https://github.com/uobinxiao/CascadeTSRDet..",
                "date": "2025-04-15",
                "DOI": "10.1016/j.eswa.2025.126461",
                "url": "https://www.sciencedirect.com/science/article/pii/S0957417425000831",
                "accessDate": "2026-01-27T00:59:24Z",
                "libraryCatalog": "ScienceDirect",
                "extra": "TLDR: The limitations of existing detection-based solutions are revisited, two-stage and transformer-based detection models are compared, and the key design aspects for the success of a two-stage detection model for the TSR task are identified, including the multi-class problem definition, the aspect ratio for anchor box generation, and the feature generation of the backbone network.\nCitation Key: xiao_rethinking-detection_2025",
                "volume": "269",
                "pages": "126461",
                "publicationTitle": "Expert Systems with Applications",
                "ISSN": "0957-4174",
                "journalAbbreviation": "Expert Systems with Applications",
                "creators": [
                  {
                    "firstName": "Bin",
                    "lastName": "Xiao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Murat",
                    "lastName": "Simsek",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Burak",
                    "lastName": "Kantarci",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ala Abu",
                    "lastName": "Alkheir",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  },
                  {
                    "tag": "Object detection",
                    "type": 1
                  },
                  {
                    "tag": "Document processing",
                    "type": 1
                  },
                  {
                    "tag": "Information extraction",
                    "type": 1
                  },
                  {
                    "tag": "Table structure recognition",
                    "type": 1
                  },
                  {
                    "tag": "Visually rich document understanding",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:59:24Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/LVBBEES6/Xiao 等 - 2025 - Rethinking detection based table structure recognition for visually rich document images.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 79,
              "key": "PJLC7YWA",
              "itemType": "attachment",
              "title": "Xiao 等 - 2025 - Rethinking detection based table structure recognition for visually rich document images.pdf",
              "libraryID": 1,
              "parentItemID": 76,
              "data": {
                "key": "PJLC7YWA",
                "version": 0,
                "itemType": "attachment",
                "title": "Xiao 等 - 2025 - Rethinking detection based table structure recognition for visually rich document images.pdf",
                "parentItem": "VI9JURUB",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/PJLC7YWA/Xiao 等 - 2025 - Rethinking detection based table structure recognition for visually rich document images.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:59:58Z",
                "dateModified": "2026-01-27T00:59:59Z"
              }
            },
            "parent": {
              "id": 76,
              "key": "VI9JURUB",
              "itemType": "journalArticle",
              "title": "Rethinking detection based table structure recognition for visually rich document images",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "VI9JURUB",
                "version": 0,
                "itemType": "journalArticle",
                "title": "Rethinking detection based table structure recognition for visually rich document images",
                "abstractNote": "Detection models have been extensively employed for the Table Structure Recognition (TSR) task, aiming to convert table images into structured formats by detecting table components such as Columns and Rows. However, prevailing detection-based TSR models usually cannot perform well regarding cell-level metrics, such as TEDS, and the reasons hindering their performance are not thoroughly explored. Therefore, we first examine the underlying reasons impeding these models’ performance and find that the key issues are the improper problem formulation, the mismatch issue of detection and TSR metrics, the inherent characteristics of detection models, and the influence of local and long-range feature extraction. Based on these findings, we propose a tailored Cascade R-CNN based solution by introducing a new problem formulation, tuning the proposal generation, and applying deformation convolution and the proposed Spatial Attention Module. The experimental results show that our proposed model can improve the base Cascade R-CNN model by 19.32%, 11.56%, and 14.77% on the SciTSR, FinTabNet, and PubTables1M datasets regarding the structure-only TEDS, achieving state-of-the-art performance, demonstrating that our findings can serve as a valuable guide for enhancing detection-based TSR models. Our code and pre-trained models are public available11https://github.com/uobinxiao/CascadeTSRDet..",
                "date": "2025-04-15",
                "DOI": "10.1016/j.eswa.2025.126461",
                "url": "https://www.sciencedirect.com/science/article/pii/S0957417425000831",
                "accessDate": "2026-01-27T00:59:24Z",
                "libraryCatalog": "ScienceDirect",
                "extra": "TLDR: The limitations of existing detection-based solutions are revisited, two-stage and transformer-based detection models are compared, and the key design aspects for the success of a two-stage detection model for the TSR task are identified, including the multi-class problem definition, the aspect ratio for anchor box generation, and the feature generation of the backbone network.\nCitation Key: xiao_rethinking-detection_2025",
                "volume": "269",
                "pages": "126461",
                "publicationTitle": "Expert Systems with Applications",
                "ISSN": "0957-4174",
                "journalAbbreviation": "Expert Systems with Applications",
                "creators": [
                  {
                    "firstName": "Bin",
                    "lastName": "Xiao",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Murat",
                    "lastName": "Simsek",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Burak",
                    "lastName": "Kantarci",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Ala Abu",
                    "lastName": "Alkheir",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "/unread",
                    "type": 1
                  },
                  {
                    "tag": "Object detection",
                    "type": 1
                  },
                  {
                    "tag": "Document processing",
                    "type": 1
                  },
                  {
                    "tag": "Information extraction",
                    "type": 1
                  },
                  {
                    "tag": "Table structure recognition",
                    "type": 1
                  },
                  {
                    "tag": "Visually rich document understanding",
                    "type": 1
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-27T00:59:24Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/PJLC7YWA/Xiao 等 - 2025 - Rethinking detection based table structure recognition for visually rich document images.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "/unread",
            "type": 1
          },
          {
            "tag": "Object detection",
            "type": 1
          },
          {
            "tag": "Document processing",
            "type": 1
          },
          {
            "tag": "Information extraction",
            "type": 1
          },
          {
            "tag": "Table structure recognition",
            "type": 1
          },
          {
            "tag": "Visually rich document understanding",
            "type": 1
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 141,
          "key": "SZ3GNWT9",
          "itemType": "preprint",
          "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "SZ3GNWT9",
            "version": 6,
            "itemType": "preprint",
            "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
            "abstractNote": "Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr",
            "date": "2025-11-12",
            "DOI": "10.48550/arXiv.2511.09554",
            "url": "http://arxiv.org/abs/2511.09554",
            "accessDate": "2026-01-03T14:43:56Z",
            "shortTitle": "RF-DETR",
            "libraryCatalog": "arXiv.org",
            "extra": "arXiv:2511.09554 [cs]\nTLDR: RF-DETR is introduced, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS) and revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains.\nCitation Key: robinson_rfdetr-neural_2025",
            "repository": "arXiv",
            "archiveID": "arXiv:2511.09554",
            "creators": [
              {
                "firstName": "Isaac",
                "lastName": "Robinson",
                "creatorType": "author"
              },
              {
                "firstName": "Peter",
                "lastName": "Robicheaux",
                "creatorType": "author"
              },
              {
                "firstName": "Matvei",
                "lastName": "Popov",
                "creatorType": "author"
              },
              {
                "firstName": "Deva",
                "lastName": "Ramanan",
                "creatorType": "author"
              },
              {
                "firstName": "Neehar",
                "lastName": "Peri",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "Computer Science - Computer Vision and Pattern Recognition",
                "type": 1
              },
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:43:57Z",
            "dateModified": "2026-01-27T10:22:24Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 175,
              "key": "6W7TFL5F",
              "itemType": "attachment",
              "title": "Robinson 等 - 2025 - RF-DETR Neural Architecture Search for Real-Time Detection Transformers.md",
              "libraryID": 1,
              "parentItemID": 141,
              "data": {
                "key": "6W7TFL5F",
                "version": 105,
                "itemType": "attachment",
                "title": "Robinson 等 - 2025 - RF-DETR Neural Architecture Search for Real-Time Detection Transformers.md",
                "parentItem": "SZ3GNWT9",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/6W7TFL5F/Robinson 等 - 2025 - RF-DETR Neural Architecture Search for Real-Time Detection Transformers.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T11:29:40Z",
                "dateModified": "2026-01-16T11:29:41Z"
              }
            },
            "parent": {
              "id": 141,
              "key": "SZ3GNWT9",
              "itemType": "preprint",
              "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "SZ3GNWT9",
                "version": 6,
                "itemType": "preprint",
                "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
                "abstractNote": "Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr",
                "date": "2025-11-12",
                "DOI": "10.48550/arXiv.2511.09554",
                "url": "http://arxiv.org/abs/2511.09554",
                "accessDate": "2026-01-03T14:43:56Z",
                "shortTitle": "RF-DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2511.09554 [cs]\nTLDR: RF-DETR is introduced, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS) and revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains.\nCitation Key: robinson_rfdetr-neural_2025",
                "repository": "arXiv",
                "archiveID": "arXiv:2511.09554",
                "creators": [
                  {
                    "firstName": "Isaac",
                    "lastName": "Robinson",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Peter",
                    "lastName": "Robicheaux",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Matvei",
                    "lastName": "Popov",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Deva",
                    "lastName": "Ramanan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Neehar",
                    "lastName": "Peri",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:57Z",
                "dateModified": "2026-01-27T10:22:24Z"
              }
            },
            "filePath": "attachments/6W7TFL5F/Robinson 等 - 2025 - RF-DETR Neural Architecture Search for Real-Time Detection Transformers.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 216,
              "key": "AJHT6UTC",
              "itemType": "attachment",
              "title": "Robinson 等 - 2025 - RF-DETR Neural Architecture Search for Real-Time Detection Transformers.pdf",
              "libraryID": 1,
              "parentItemID": 141,
              "data": {
                "key": "AJHT6UTC",
                "version": 69,
                "itemType": "attachment",
                "title": "Robinson 等 - 2025 - RF-DETR Neural Architecture Search for Real-Time Detection Transformers.pdf",
                "url": "http://arxiv.org/pdf/2511.09554v1",
                "accessDate": "2026-01-03T14:44:02Z",
                "parentItem": "SZ3GNWT9",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/AJHT6UTC/Robinson 等 - 2025 - RF-DETR Neural Architecture Search for Real-Time Detection Transformers.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:02Z",
                "dateModified": "2026-01-03T14:44:02Z"
              }
            },
            "parent": {
              "id": 141,
              "key": "SZ3GNWT9",
              "itemType": "preprint",
              "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "SZ3GNWT9",
                "version": 6,
                "itemType": "preprint",
                "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
                "abstractNote": "Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr",
                "date": "2025-11-12",
                "DOI": "10.48550/arXiv.2511.09554",
                "url": "http://arxiv.org/abs/2511.09554",
                "accessDate": "2026-01-03T14:43:56Z",
                "shortTitle": "RF-DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2511.09554 [cs]\nTLDR: RF-DETR is introduced, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS) and revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains.\nCitation Key: robinson_rfdetr-neural_2025",
                "repository": "arXiv",
                "archiveID": "arXiv:2511.09554",
                "creators": [
                  {
                    "firstName": "Isaac",
                    "lastName": "Robinson",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Peter",
                    "lastName": "Robicheaux",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Matvei",
                    "lastName": "Popov",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Deva",
                    "lastName": "Ramanan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Neehar",
                    "lastName": "Peri",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:57Z",
                "dateModified": "2026-01-27T10:22:24Z"
              }
            },
            "filePath": "attachments/AJHT6UTC/Robinson 等 - 2025 - RF-DETR Neural Architecture Search for Real-Time Detection Transformers.pdf",
            "mimeType": null
          },
          {
            "item": {
              "id": 217,
              "key": "29E5SNUB",
              "itemType": "attachment",
              "title": "Snapshot",
              "libraryID": 1,
              "parentItemID": 141,
              "data": {
                "key": "29E5SNUB",
                "version": 6,
                "itemType": "attachment",
                "title": "Snapshot",
                "url": "http://arxiv.org/abs/2511.09554",
                "accessDate": "2026-01-03T14:44:02Z",
                "parentItem": "SZ3GNWT9",
                "linkMode": "imported_url",
                "contentType": "text/html",
                "charset": "utf-8",
                "filename": "2511.html",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:02Z",
                "dateModified": "2026-01-03T14:44:02Z",
                "path": "attachments/29E5SNUB/Snapshot"
              }
            },
            "parent": {
              "id": 141,
              "key": "SZ3GNWT9",
              "itemType": "preprint",
              "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "SZ3GNWT9",
                "version": 6,
                "itemType": "preprint",
                "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
                "abstractNote": "Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr",
                "date": "2025-11-12",
                "DOI": "10.48550/arXiv.2511.09554",
                "url": "http://arxiv.org/abs/2511.09554",
                "accessDate": "2026-01-03T14:43:56Z",
                "shortTitle": "RF-DETR",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2511.09554 [cs]\nTLDR: RF-DETR is introduced, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS) and revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains.\nCitation Key: robinson_rfdetr-neural_2025",
                "repository": "arXiv",
                "archiveID": "arXiv:2511.09554",
                "creators": [
                  {
                    "firstName": "Isaac",
                    "lastName": "Robinson",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Peter",
                    "lastName": "Robicheaux",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Matvei",
                    "lastName": "Popov",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Deva",
                    "lastName": "Ramanan",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Neehar",
                    "lastName": "Peri",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:43:57Z",
                "dateModified": "2026-01-27T10:22:24Z"
              }
            },
            "filePath": "attachments/29E5SNUB/Snapshot",
            "mimeType": null
          }
        ],
        "notes": [
          {
            "id": 218,
            "key": "FE5YBUZB",
            "itemType": "note",
            "title": "Comment: Project Page: https://rfdetr.roboflow.com/",
            "libraryID": 1,
            "parentItemID": 141,
            "data": {
              "key": "FE5YBUZB",
              "version": 6,
              "itemType": "note",
              "parentItem": "SZ3GNWT9",
              "note": "Comment: Project Page: https://rfdetr.roboflow.com/",
              "tags": [],
              "relations": {},
              "dateAdded": "2026-01-03T14:43:57Z",
              "dateModified": "2026-01-03T14:43:57Z"
            },
            "note": "Comment: Project Page: https://rfdetr.roboflow.com/"
          }
        ],
        "tags": [
          {
            "tag": "Computer Science - Computer Vision and Pattern Recognition",
            "type": 1
          },
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 139,
          "key": "GHWYS7AF",
          "itemType": "preprint",
          "title": "RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "GHWYS7AF",
            "version": 7,
            "itemType": "preprint",
            "title": "RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision",
            "abstractNote": "RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the same latency. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. The code will be released at https://github.com/clxia12/RT-DETRv3.",
            "date": "2024-12-19",
            "DOI": "10.48550/arXiv.2409.08475",
            "url": "http://arxiv.org/abs/2409.08475",
            "accessDate": "2026-01-03T14:44:13Z",
            "shortTitle": "RT-DETRv3",
            "libraryCatalog": "arXiv.org",
            "extra": "arXiv:2409.08475 [cs]\nCitation Key: wang_rtdetrv3-realtime_2024",
            "repository": "arXiv",
            "archiveID": "arXiv:2409.08475",
            "creators": [
              {
                "firstName": "Shuo",
                "lastName": "Wang",
                "creatorType": "author"
              },
              {
                "firstName": "Chunlong",
                "lastName": "Xia",
                "creatorType": "author"
              },
              {
                "firstName": "Feng",
                "lastName": "Lv",
                "creatorType": "author"
              },
              {
                "firstName": "Yifeng",
                "lastName": "Shi",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "Computer Science - Computer Vision and Pattern Recognition",
                "type": 1
              },
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:44:13Z",
            "dateModified": "2026-01-27T10:22:50Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 211,
              "key": "9CBE2BEI",
              "itemType": "attachment",
              "title": "Snapshot",
              "libraryID": 1,
              "parentItemID": 139,
              "data": {
                "key": "9CBE2BEI",
                "version": 7,
                "itemType": "attachment",
                "title": "Snapshot",
                "url": "http://arxiv.org/abs/2409.08475",
                "accessDate": "2026-01-03T14:44:16Z",
                "parentItem": "GHWYS7AF",
                "linkMode": "imported_url",
                "contentType": "text/html",
                "charset": "utf-8",
                "filename": "2409.html",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:16Z",
                "dateModified": "2026-01-03T14:44:16Z",
                "path": "attachments/9CBE2BEI/Snapshot"
              }
            },
            "parent": {
              "id": 139,
              "key": "GHWYS7AF",
              "itemType": "preprint",
              "title": "RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "GHWYS7AF",
                "version": 7,
                "itemType": "preprint",
                "title": "RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision",
                "abstractNote": "RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the same latency. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. The code will be released at https://github.com/clxia12/RT-DETRv3.",
                "date": "2024-12-19",
                "DOI": "10.48550/arXiv.2409.08475",
                "url": "http://arxiv.org/abs/2409.08475",
                "accessDate": "2026-01-03T14:44:13Z",
                "shortTitle": "RT-DETRv3",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2409.08475 [cs]\nCitation Key: wang_rtdetrv3-realtime_2024",
                "repository": "arXiv",
                "archiveID": "arXiv:2409.08475",
                "creators": [
                  {
                    "firstName": "Shuo",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Chunlong",
                    "lastName": "Xia",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Feng",
                    "lastName": "Lv",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yifeng",
                    "lastName": "Shi",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:13Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/9CBE2BEI/Snapshot",
            "mimeType": null
          },
          {
            "item": {
              "id": 188,
              "key": "VHEIFPDN",
              "itemType": "attachment",
              "title": "Wang 等 - 2024 - RT-DETRv3 Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision.md",
              "libraryID": 1,
              "parentItemID": 139,
              "data": {
                "key": "VHEIFPDN",
                "version": 79,
                "itemType": "attachment",
                "title": "Wang 等 - 2024 - RT-DETRv3 Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision.md",
                "parentItem": "GHWYS7AF",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/VHEIFPDN/Wang 等 - 2024 - RT-DETRv3 Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T10:00:07Z",
                "dateModified": "2026-01-16T10:00:07Z"
              }
            },
            "parent": {
              "id": 139,
              "key": "GHWYS7AF",
              "itemType": "preprint",
              "title": "RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "GHWYS7AF",
                "version": 7,
                "itemType": "preprint",
                "title": "RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision",
                "abstractNote": "RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the same latency. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. The code will be released at https://github.com/clxia12/RT-DETRv3.",
                "date": "2024-12-19",
                "DOI": "10.48550/arXiv.2409.08475",
                "url": "http://arxiv.org/abs/2409.08475",
                "accessDate": "2026-01-03T14:44:13Z",
                "shortTitle": "RT-DETRv3",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2409.08475 [cs]\nCitation Key: wang_rtdetrv3-realtime_2024",
                "repository": "arXiv",
                "archiveID": "arXiv:2409.08475",
                "creators": [
                  {
                    "firstName": "Shuo",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Chunlong",
                    "lastName": "Xia",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Feng",
                    "lastName": "Lv",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yifeng",
                    "lastName": "Shi",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:13Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/VHEIFPDN/Wang 等 - 2024 - RT-DETRv3 Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 212,
              "key": "CVV3EPRH",
              "itemType": "attachment",
              "title": "Wang 等 - 2024 - RT-DETRv3 Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision.pdf",
              "libraryID": 1,
              "parentItemID": 139,
              "data": {
                "key": "CVV3EPRH",
                "version": 69,
                "itemType": "attachment",
                "title": "Wang 等 - 2024 - RT-DETRv3 Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision.pdf",
                "url": "http://arxiv.org/pdf/2409.08475v3",
                "accessDate": "2026-01-03T14:44:14Z",
                "parentItem": "GHWYS7AF",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/CVV3EPRH/Wang 等 - 2024 - RT-DETRv3 Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:14Z",
                "dateModified": "2026-01-03T14:44:15Z"
              }
            },
            "parent": {
              "id": 139,
              "key": "GHWYS7AF",
              "itemType": "preprint",
              "title": "RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "GHWYS7AF",
                "version": 7,
                "itemType": "preprint",
                "title": "RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision",
                "abstractNote": "RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the same latency. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. The code will be released at https://github.com/clxia12/RT-DETRv3.",
                "date": "2024-12-19",
                "DOI": "10.48550/arXiv.2409.08475",
                "url": "http://arxiv.org/abs/2409.08475",
                "accessDate": "2026-01-03T14:44:13Z",
                "shortTitle": "RT-DETRv3",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2409.08475 [cs]\nCitation Key: wang_rtdetrv3-realtime_2024",
                "repository": "arXiv",
                "archiveID": "arXiv:2409.08475",
                "creators": [
                  {
                    "firstName": "Shuo",
                    "lastName": "Wang",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Chunlong",
                    "lastName": "Xia",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Feng",
                    "lastName": "Lv",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Yifeng",
                    "lastName": "Shi",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:13Z",
                "dateModified": "2026-01-27T10:22:50Z"
              }
            },
            "filePath": "attachments/CVV3EPRH/Wang 等 - 2024 - RT-DETRv3 Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision.pdf",
            "mimeType": null
          }
        ],
        "notes": [],
        "tags": [
          {
            "tag": "Computer Science - Computer Vision and Pattern Recognition",
            "type": 1
          },
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      },
      {
        "item": {
          "id": 131,
          "key": "29IBKEUR",
          "itemType": "preprint",
          "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "29IBKEUR",
            "version": 16,
            "itemType": "preprint",
            "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
            "abstractNote": "DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency on high resolution feature maps. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves 10x faster convergence and improved performance. Deformable DETR uses the multiscale feature to ameliorate performance, however, the number of encoder tokens increases by 20x compared to DETR, and the computation cost of the encoder attention remains a bottleneck. In our preliminary experiment, we observe that the detection performance hardly deteriorates even if only a part of the encoder token is updated. Inspired by this observation, we propose Sparse DETR that selectively updates only the tokens expected to be referenced by the decoder, thus help the model effectively detect objects. In addition, we show that applying an auxiliary detection loss on the selected tokens in the encoder improves the performance while minimizing computational overhead. We validate that Sparse DETR achieves better performance than Deformable DETR even with only 10% encoder tokens on the COCO dataset. Albeit only the encoder tokens are sparsified, the total computation cost decreases by 38% and the frames per second (FPS) increases by 42% compared to Deformable DETR. Code is available at https://github.com/kakaobrain/sparse-detr",
            "date": "2022-03-04",
            "DOI": "10.48550/arXiv.2111.14330",
            "url": "http://arxiv.org/abs/2111.14330",
            "accessDate": "2026-01-03T14:44:05Z",
            "shortTitle": "Sparse DETR",
            "language": "en-US",
            "libraryCatalog": "arXiv.org",
            "extra": "arXiv:2111.14330 [cs]\nCitation Key: roh_sparse-detr_2022",
            "repository": "arXiv",
            "archiveID": "arXiv:2111.14330",
            "creators": [
              {
                "firstName": "Byungseok",
                "lastName": "Roh",
                "creatorType": "author"
              },
              {
                "firstName": "JaeWoong",
                "lastName": "Shin",
                "creatorType": "author"
              },
              {
                "firstName": "Wuhyun",
                "lastName": "Shin",
                "creatorType": "author"
              },
              {
                "firstName": "Saehoon",
                "lastName": "Kim",
                "creatorType": "author"
              }
            ],
            "tags": [
              {
                "tag": "Computer Science - Computer Vision and Pattern Recognition",
                "type": 1
              },
              {
                "tag": "Computer Science - Machine Learning",
                "type": 1
              },
              {
                "tag": "match_status:unmatched"
              }
            ],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-03T14:44:05Z",
            "dateModified": "2026-01-27T10:22:31Z"
          }
        },
        "attachments": [
          {
            "item": {
              "id": 190,
              "key": "MACVYVTN",
              "itemType": "attachment",
              "title": "Roh 等 - 2022 - Sparse DETR Efficient End-to-End Object Detection with Learnable Sparsity.md",
              "libraryID": 1,
              "parentItemID": 131,
              "data": {
                "key": "MACVYVTN",
                "version": 75,
                "itemType": "attachment",
                "title": "Roh 等 - 2022 - Sparse DETR Efficient End-to-End Object Detection with Learnable Sparsity.md",
                "parentItem": "29IBKEUR",
                "linkMode": "linked_file",
                "contentType": "text/plain",
                "charset": "windows-1252",
                "path": "attachments/MACVYVTN/Roh 等 - 2022 - Sparse DETR Efficient End-to-End Object Detection with Learnable Sparsity.md",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-16T09:58:24Z",
                "dateModified": "2026-01-16T09:58:24Z"
              }
            },
            "parent": {
              "id": 131,
              "key": "29IBKEUR",
              "itemType": "preprint",
              "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "29IBKEUR",
                "version": 16,
                "itemType": "preprint",
                "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
                "abstractNote": "DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency on high resolution feature maps. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves 10x faster convergence and improved performance. Deformable DETR uses the multiscale feature to ameliorate performance, however, the number of encoder tokens increases by 20x compared to DETR, and the computation cost of the encoder attention remains a bottleneck. In our preliminary experiment, we observe that the detection performance hardly deteriorates even if only a part of the encoder token is updated. Inspired by this observation, we propose Sparse DETR that selectively updates only the tokens expected to be referenced by the decoder, thus help the model effectively detect objects. In addition, we show that applying an auxiliary detection loss on the selected tokens in the encoder improves the performance while minimizing computational overhead. We validate that Sparse DETR achieves better performance than Deformable DETR even with only 10% encoder tokens on the COCO dataset. Albeit only the encoder tokens are sparsified, the total computation cost decreases by 38% and the frames per second (FPS) increases by 42% compared to Deformable DETR. Code is available at https://github.com/kakaobrain/sparse-detr",
                "date": "2022-03-04",
                "DOI": "10.48550/arXiv.2111.14330",
                "url": "http://arxiv.org/abs/2111.14330",
                "accessDate": "2026-01-03T14:44:05Z",
                "shortTitle": "Sparse DETR",
                "language": "en-US",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2111.14330 [cs]\nCitation Key: roh_sparse-detr_2022",
                "repository": "arXiv",
                "archiveID": "arXiv:2111.14330",
                "creators": [
                  {
                    "firstName": "Byungseok",
                    "lastName": "Roh",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "JaeWoong",
                    "lastName": "Shin",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wuhyun",
                    "lastName": "Shin",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Saehoon",
                    "lastName": "Kim",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "Computer Science - Machine Learning",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:05Z",
                "dateModified": "2026-01-27T10:22:31Z"
              }
            },
            "filePath": "attachments/MACVYVTN/Roh 等 - 2022 - Sparse DETR Efficient End-to-End Object Detection with Learnable Sparsity.md",
            "mimeType": null
          },
          {
            "item": {
              "id": 213,
              "key": "3K6IP2AG",
              "itemType": "attachment",
              "title": "Roh 等 - 2022 - Sparse DETR Efficient End-to-End Object Detection with Learnable Sparsity.pdf",
              "libraryID": 1,
              "parentItemID": 131,
              "data": {
                "key": "3K6IP2AG",
                "version": 68,
                "itemType": "attachment",
                "title": "Roh 等 - 2022 - Sparse DETR Efficient End-to-End Object Detection with Learnable Sparsity.pdf",
                "url": "http://arxiv.org/pdf/2111.14330v2",
                "accessDate": "2026-01-03T14:44:08Z",
                "parentItem": "29IBKEUR",
                "linkMode": "linked_file",
                "contentType": "application/pdf",
                "charset": "",
                "path": "attachments/3K6IP2AG/Roh 等 - 2022 - Sparse DETR Efficient End-to-End Object Detection with Learnable Sparsity.pdf",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:08Z",
                "dateModified": "2026-01-03T14:44:09Z"
              }
            },
            "parent": {
              "id": 131,
              "key": "29IBKEUR",
              "itemType": "preprint",
              "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "29IBKEUR",
                "version": 16,
                "itemType": "preprint",
                "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
                "abstractNote": "DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency on high resolution feature maps. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves 10x faster convergence and improved performance. Deformable DETR uses the multiscale feature to ameliorate performance, however, the number of encoder tokens increases by 20x compared to DETR, and the computation cost of the encoder attention remains a bottleneck. In our preliminary experiment, we observe that the detection performance hardly deteriorates even if only a part of the encoder token is updated. Inspired by this observation, we propose Sparse DETR that selectively updates only the tokens expected to be referenced by the decoder, thus help the model effectively detect objects. In addition, we show that applying an auxiliary detection loss on the selected tokens in the encoder improves the performance while minimizing computational overhead. We validate that Sparse DETR achieves better performance than Deformable DETR even with only 10% encoder tokens on the COCO dataset. Albeit only the encoder tokens are sparsified, the total computation cost decreases by 38% and the frames per second (FPS) increases by 42% compared to Deformable DETR. Code is available at https://github.com/kakaobrain/sparse-detr",
                "date": "2022-03-04",
                "DOI": "10.48550/arXiv.2111.14330",
                "url": "http://arxiv.org/abs/2111.14330",
                "accessDate": "2026-01-03T14:44:05Z",
                "shortTitle": "Sparse DETR",
                "language": "en-US",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2111.14330 [cs]\nCitation Key: roh_sparse-detr_2022",
                "repository": "arXiv",
                "archiveID": "arXiv:2111.14330",
                "creators": [
                  {
                    "firstName": "Byungseok",
                    "lastName": "Roh",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "JaeWoong",
                    "lastName": "Shin",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wuhyun",
                    "lastName": "Shin",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Saehoon",
                    "lastName": "Kim",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "Computer Science - Machine Learning",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:05Z",
                "dateModified": "2026-01-27T10:22:31Z"
              }
            },
            "filePath": "attachments/3K6IP2AG/Roh 等 - 2022 - Sparse DETR Efficient End-to-End Object Detection with Learnable Sparsity.pdf",
            "mimeType": null
          },
          {
            "item": {
              "id": 214,
              "key": "5CJU6NFF",
              "itemType": "attachment",
              "title": "Snapshot",
              "libraryID": 1,
              "parentItemID": 131,
              "data": {
                "key": "5CJU6NFF",
                "version": 7,
                "itemType": "attachment",
                "title": "Snapshot",
                "url": "http://arxiv.org/abs/2111.14330",
                "accessDate": "2026-01-03T14:44:08Z",
                "parentItem": "29IBKEUR",
                "linkMode": "imported_url",
                "contentType": "text/html",
                "charset": "utf-8",
                "filename": "2111.html",
                "tags": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:08Z",
                "dateModified": "2026-01-03T14:44:08Z",
                "path": "attachments/5CJU6NFF/Snapshot"
              }
            },
            "parent": {
              "id": 131,
              "key": "29IBKEUR",
              "itemType": "preprint",
              "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
              "libraryID": 1,
              "parentItemID": null,
              "data": {
                "key": "29IBKEUR",
                "version": 16,
                "itemType": "preprint",
                "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
                "abstractNote": "DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency on high resolution feature maps. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves 10x faster convergence and improved performance. Deformable DETR uses the multiscale feature to ameliorate performance, however, the number of encoder tokens increases by 20x compared to DETR, and the computation cost of the encoder attention remains a bottleneck. In our preliminary experiment, we observe that the detection performance hardly deteriorates even if only a part of the encoder token is updated. Inspired by this observation, we propose Sparse DETR that selectively updates only the tokens expected to be referenced by the decoder, thus help the model effectively detect objects. In addition, we show that applying an auxiliary detection loss on the selected tokens in the encoder improves the performance while minimizing computational overhead. We validate that Sparse DETR achieves better performance than Deformable DETR even with only 10% encoder tokens on the COCO dataset. Albeit only the encoder tokens are sparsified, the total computation cost decreases by 38% and the frames per second (FPS) increases by 42% compared to Deformable DETR. Code is available at https://github.com/kakaobrain/sparse-detr",
                "date": "2022-03-04",
                "DOI": "10.48550/arXiv.2111.14330",
                "url": "http://arxiv.org/abs/2111.14330",
                "accessDate": "2026-01-03T14:44:05Z",
                "shortTitle": "Sparse DETR",
                "language": "en-US",
                "libraryCatalog": "arXiv.org",
                "extra": "arXiv:2111.14330 [cs]\nCitation Key: roh_sparse-detr_2022",
                "repository": "arXiv",
                "archiveID": "arXiv:2111.14330",
                "creators": [
                  {
                    "firstName": "Byungseok",
                    "lastName": "Roh",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "JaeWoong",
                    "lastName": "Shin",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Wuhyun",
                    "lastName": "Shin",
                    "creatorType": "author"
                  },
                  {
                    "firstName": "Saehoon",
                    "lastName": "Kim",
                    "creatorType": "author"
                  }
                ],
                "tags": [
                  {
                    "tag": "Computer Science - Computer Vision and Pattern Recognition",
                    "type": 1
                  },
                  {
                    "tag": "Computer Science - Machine Learning",
                    "type": 1
                  },
                  {
                    "tag": "match_status:unmatched"
                  }
                ],
                "collections": [],
                "relations": {},
                "dateAdded": "2026-01-03T14:44:05Z",
                "dateModified": "2026-01-27T10:22:31Z"
              }
            },
            "filePath": "attachments/5CJU6NFF/Snapshot",
            "mimeType": null
          }
        ],
        "notes": [
          {
            "id": 215,
            "key": "PMTT358H",
            "itemType": "note",
            "title": "Comment: ICLR 2022. Code is available at https://github.com/kakaobrain/sparse-detr",
            "libraryID": 1,
            "parentItemID": 131,
            "data": {
              "key": "PMTT358H",
              "version": 7,
              "itemType": "note",
              "parentItem": "29IBKEUR",
              "note": "Comment: ICLR 2022. Code is available at https://github.com/kakaobrain/sparse-detr",
              "tags": [],
              "relations": {},
              "dateAdded": "2026-01-03T14:44:05Z",
              "dateModified": "2026-01-03T14:44:05Z"
            },
            "note": "Comment: ICLR 2022. Code is available at https://github.com/kakaobrain/sparse-detr"
          }
        ],
        "tags": [
          {
            "tag": "Computer Science - Computer Vision and Pattern Recognition",
            "type": 1
          },
          {
            "tag": "Computer Science - Machine Learning",
            "type": 1
          },
          {
            "tag": "match_status:unmatched"
          }
        ],
        "collections": [],
        "children": []
      }
    ],
    "children": [],
    "attachments": [],
    "notes": [
      {
        "item": {
          "id": 130,
          "key": "7MMHBDHI",
          "itemType": "note",
          "title": "Welcome to Better Notes",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "7MMHBDHI",
            "version": 58,
            "itemType": "note",
            "note": "<h1>Welcome to Better Notes</h1>\n<p>This note is created by the Better Notes user guide.\nYou can always run the user guide again from menu Help -&gt; Better Notes User Guide.</p>\n<h2>📝 Introduction</h2>\n<blockquote>\n<p>Everything about note management. All in Zotero.</p>\n</blockquote>\n<p>Better Notes (BN) is a plugin for <a href=\"https://zotero.org\">Zotero</a>.</p>\n<p>BN streamlines your workflows of:</p>\n<ul>\n<li><span>paper reading</span></li>\n<li><span>annotating</span></li>\n<li><span>note taking</span></li>\n<li><span>metadata analyzing</span></li>\n<li><span>knowledge exporting</span></li>\n<li><span>AI writing assistant</span></li>\n</ul>\n<p>and:</p>\n<ul>\n<li><span>works out of the box</span></li>\n<li><span>highly customizable</span></li>\n<li><span>all in Zotero</span></li>\n</ul>\n<h2>🚀 Get Started</h2>\n<p>See the <a href=\"https://github.com/windingwind/zotero-better-notes?tab=readme-ov-file#-quick-start\">Quick Start Guide</a> to get started.</p>\n<h2>📚 Resources</h2>\n<p>You can find more information in the following links:</p>\n<ul>\n<li><a href=\"https://github.com/windingwind/zotero-better-notes\">Documentation</a></li>\n<li><a href=\"https://github.com/windingwind/zotero-better-notes/issues\">Issues</a></li>\n<li><a href=\"https://github.com/windingwind/zotero-better-notes/discussions\">Discussions</a></li>\n<li><a href=\"https://github.com/windingwind/zotero-better-notes?tab=readme-ov-file#-api\">API</a></li>\n</ul>",
            "tags": [],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-13T05:26:29Z",
            "dateModified": "2026-01-13T05:26:29Z"
          }
        },
        "parent": null,
        "tags": [],
        "collections": []
      },
      {
        "item": {
          "id": 127,
          "key": "MR6GQ85R",
          "itemType": "note",
          "title": "Welcome to Better Notes",
          "libraryID": 1,
          "parentItemID": null,
          "data": {
            "key": "MR6GQ85R",
            "version": 138,
            "itemType": "note",
            "note": "<h1>Welcome to Better Notes</h1>\n<p>This note is created by the Better Notes user guide.\nYou can always run the user guide again from menu Help -&gt; Better Notes User Guide.</p>\n<h2>📝 Introduction</h2>\n<blockquote>\n<p>Everything about note management. All in Zotero.</p>\n</blockquote>\n<p>Better Notes (BN) is a plugin for <a href=\"https://zotero.org\">Zotero</a>.</p>\n<p>BN streamlines your workflows of:</p>\n<ul>\n<li><span>paper reading</span></li>\n<li><span>annotating</span></li>\n<li><span>note taking</span></li>\n<li><span>metadata analyzing</span></li>\n<li><span>knowledge exporting</span></li>\n<li><span>AI writing assistant</span></li>\n</ul>\n<p>and:</p>\n<ul>\n<li><span>works out of the box</span></li>\n<li><span>highly customizable</span></li>\n<li><span>all in Zotero</span></li>\n</ul>\n<h2>🚀 Get Started</h2>\n<p>See the <a href=\"https://github.com/windingwind/zotero-better-notes?tab=readme-ov-file#-quick-start\">Quick Start Guide</a> to get started.</p>\n<h2>📚 Resources</h2>\n<p>You can find more information in the following links:</p>\n<ul>\n<li><a href=\"https://github.com/windingwind/zotero-better-notes\">Documentation</a></li>\n<li><a href=\"https://github.com/windingwind/zotero-better-notes/issues\">Issues</a></li>\n<li><a href=\"https://github.com/windingwind/zotero-better-notes/discussions\">Discussions</a></li>\n<li><a href=\"https://github.com/windingwind/zotero-better-notes?tab=readme-ov-file#-api\">API</a></li>\n</ul>",
            "tags": [],
            "collections": [],
            "relations": {},
            "dateAdded": "2026-01-27T00:28:59Z",
            "dateModified": "2026-01-27T00:28:59Z"
          }
        },
        "parent": null,
        "tags": [],
        "collections": []
      }
    ]
  },
  "summary": {
    "parentCount": 21,
    "childCount": 0,
    "attachmentCount": 0,
    "noteCount": 2
  },
  "warnings": [],
  "sampledAt": "2026-01-27T11:39:22.420Z"
}