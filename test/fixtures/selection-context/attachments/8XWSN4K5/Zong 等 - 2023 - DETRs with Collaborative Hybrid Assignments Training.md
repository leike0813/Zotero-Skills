# DETRs with Collaborative Hybrid Assignments Training

Zhuofan ZongGuanglu Song Yu Liu\* SenseTime Research

{zongzhuofan,liuyuisanai}@gmail.com songguanglu@sensetime.com

# Abstract

In this paper, we provide the observation that too few queries assigned as positive samples in DETR with oneto-one set matching leads to sparse supervision on the encoder's output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this,we present a novel collaborative hybrid assignments training scheme, namely Co-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS and Faster RCNN.In addition,we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference,these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR,Deformable-DETR,and DINO-DeformableDETR.The state-of-the-art DINO-Deformable-DETR with Swin-L can be improved from $5 8 . 5 \%$ to $5 9 . 5 \%$ AP on CoCO val. Surprisingly， incorporated with ViT-L backbone, we achieve $6 6 . 0 \%$ AP on COCO test-dev and $6 7 . 9 \%$ AP on LVIS val, outperforming previous methods by clear margins with much fewer model sizes. Codes are available at https://github.com/Sense-X/Co-DETR.

# 1. Introduction

Object detection is a fundamental task in computer vision,which requires us to localize the object and classify its category. The seminal R-CNN families [11,27,14] and a series of variants [30,43,36] such as ATSS [40],RetinaNet [21],FCOS [31],and PAA [17] lead to the significant breakthrough of object detection task. One-to-many label assignment is the core scheme of them, where each groundtruth box is assigned to multiple coordinates in the detector's output as the supervised target cooperated with proposals [11,27],anchors [21] or window centers [31].Despite their promising performance,these detectors heavily rely on many hand-designed components like a non-maximum suppression procedure or anchor generation [1]. To conduct a more flexible end-to-end detector,DEtection TRansformer (DETR)[1] is proposed to view the object detection as a set prediction problem and introduce the one-to-one set matching scheme based on a transformer encoder-decoder architecture.In this manner, each ground-truth box will only be assigned to one specific query,and multiple handdesigned components that encode prior knowledge are no longer needed. This approach introduces a flexible detection pipeline and encourages many DETR variants to further improve it. However, the performance of the vanilla end-to-end object detector is still inferior to the traditional detectors with one-to-many label assignments.

![](Images_KU3GJDWR/32e1eb1b0e139d2bba52ea2c7382461e002e94542ebd0687708da983b9254ad9.jpg)  
Figure 1: $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR outperforms other methods with the same ResNet-50 backbone by large margins on COCO va l.

![](Images_KU3GJDWR/3b50f2d1ce505b67e0f3ba374b828ffe6fce75c9165528d839bf17c198e0e9e2.jpg)  
Figure 2: IoF-IoB curves for the feature discriminability score in the encoder and attention discriminability score in the decoder.

In this paper, we try to make DETR-based detectors superior to conventional detectors while maintaining their end-to-end merit. To address this challenge,we focus on the intuitive drawback of one-to-one set matching that it explores less positive queries. This will lead to severe ineficient training issues. We detailedly analyze this from two aspects,the latent representation generated by the encoder and the attention learning in the decoder. We first compare the discriminability score of the latent features between the Deformable-DETR [42] and the one-to-many label assignment method where we simply replace the decoder with the ATSS head. The feature $l ^ { 2 }$ -norm in each spatial coordinate is utilized to represent the_ discriminability score. Given the encoder's output $\mathcal { F } \in \mathbb { R } ^ { C \times H \times W }$ , we can obtain the discriminability score map $S \in \mathbb { R } ^ { 1 \times H \times W }$ . The object can be better detected when the scores in the corresponding area are higher. As shown in Figure 2,we demonstrate the IoF-IoB curve (IoF: intersection over foreground, IoB: intersection over background) by applying different thresholds on the discriminability scores (details in Section 3.4). The higher IoF-IoB curve in ATSS indicates that it’s easier to distinguish the foreground and background. We further visualize the discriminability score map $s$ in Figure 3. It's obvious that the features in some salient areas are fully activated in the one-to-many label assignment method but less explored in one-to-one set matching.For the exploration of decoder training,we also demonstrate the IoF-IoB curve of the cross-attention score in the decoder based on the Deformable-DETR and the Group-DETR[5] which introduces more positive queries into the decoder. The illustration in Figure 2 shows that too few positive queries also influence attention learning and increasing more positive queries in the decoder can slightly alleviate this.

This significant observation motivates us to present a simple but effective method,a collaborative hybrid assignment training scheme ( $\scriptstyle { \mathcal { C } } 0$ -DETR).The key insight of ${ \mathit { C } } _ { 0 } .$ - DETR is to use versatile one-to-many label assignments to improve the training efficiency and effectiveness of both the encoder and decoder. More specifically，we integrate the auxiliary heads with the output of the transformer encoder. These heads can be supervised by versatile one-to-many label assignments such as ATSS [4O],FCOS [31],and Faster

![](Images_KU3GJDWR/70598c4e35d300cfad4628ffa6b2fa39b2c465ccc31ef6546c95ac4658be825d.jpg)  
Figure 3: Visualizations of discriminability scores in the encoder.

RCNN [27]. Different label assignments enrich the supervisions on the encoder's output which forces it to be discriminative enough to support the training convergence of these heads. To further improve the training efficiency of the decoder,we elaborately encode the coordinates of positive samples in these auxiliary heads,including the positive anchors and positive proposals. They are sent to the original decoder as multiple groups of positive queries to predict the pre-assigned categories and bounding boxes. Positive coordinates in each auxiliary head serve as an independent group that is isolated from the other groups. Versatile oneto-many label assignments can introduce lavish (positive query,ground-truth） pairs to improve the decoder's training efficiency. Note that, only the original decoder is used during inference, thus the proposed training scheme only introduces extra overheads during training.

We conduct extensive experiments to evaluate the efficiency and effectiveness of the proposed method. Illustrated in Figure 3, $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR greatly alleviates the poorly encoder's feature learning in one-to-one set matching. As a plug-and-play approach,we easily combine it with different DETR variants, including DAB-DETR[23],DeformableDETR [42],and DINO-Deformable-DETR [38]. As shown in Figure 1, $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR achieves faster training convergence and even higher performance. Specifically, we improve the basic Deformable-DETR by $5 . 8 \%$ AP in 12-epoch training and $3 . 2 \%$ AP in 36-epoch training. The state-of-theart DINO-Deformable-DETR with Swin-L [25] can still be improved from $5 8 . 5 \%$ to $5 9 . 5 \%$ AP on COCO val. Surprisingly, incorporated with ViT-L [8] backbone,we achieve $6 6 . 0 \%$ AP on COCO test-dev and $6 7 . 9 \%$ AP on LVIS val,establishing the new state-of-the-art detector with much fewer model sizes.

# 2.Related Works

One-to-many label assignment. For one-to-many label assignment in object detection,multiple box candidates can be assigned to the same ground-truth box as positive samples in the training phase.In classic anchor-based detectors, such as Faster-RCNN [27] and RetinaNet [21],the sample selection is guided by the predefined IoU threshold and matching IoU between anchors and annotated boxes. The anchor-free FCOS [31] leverages the center priors and assigns spatial locations near the center of each bounding box as positives.Moreover, the adaptive mechanism is incorporated into one-to-many label assignments to overcome the limitation of fixed label assignments. ATSS [40] performs adaptive anchor selection by the statistical dynamic IoU values of top- $k$ closest anchors.PAA[17] adaptively separates anchors into positive and negative samples in a probabilistic manner. In this paper, we propose a collaborative hybrid assignment scheme to improve encoder representations via auxiliary heads with one-to-many label assignments.

![](Images_KU3GJDWR/bf36682c3acd3a813960b2226455b9b285ccd369922f20ec866abe515676a25e.jpg)  
Figure 4: Framework of our Collaborative Hybrid Asignment Training.The auxiliary branches are discarded during evaluation.

One-to-one set matching. The pioneering transformerbased detector, DETR[1], incorporates the one-to-one set matching scheme into object detection and performs fully end-to-end object detection. The one-to-one set matching strategy first calculates the global matching cost via Hungarian matching and assigns only one positive sample with the minimum matching cost for each ground-truth box. DNDETR[18] demonstrates the slow convergence results from the instability of one-to-one set matching, thus introducing denoising training to eliminate this issue.DINO [38] inherits the advanced query formulation of DAB-DETR [23] and incorporates an improved contrastive denoising technique to achieve state-of-the-art performance.Group-DETR [5] constructs group-wise one-to-many label assignment to exploit multiple positive object queries,which is similar to the hybrid matching scheme in $\mathcal { H }$ -DETR[16]. In contrast with the above follow-up works, we present a new perspective of collaborative optimization for one-to-one set matching.

# 3.Method

# 3.1. Overview

Following the standard DETR protocol, the input image is fed into the backbone and encoder to generate latent features.Multiple predefined object queries interact with them in the decoder via cross-attention afterwards.We introduce $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR to improve the feature learning in the encoder and the attention learning in the decoder via the collaborative hybrid assignments training scheme and the customized positive queries generation. We will detailedly describe these modules and give insights why they can work well.

# 3.2. Collaborative Hybrid Assignments Training

To alleviate the sparse supervision on the encoder's output caused by the fewer positive queries in the decoder, we incorporate versatile auxiliary heads with different one-tomany label assignment paradigms,e.g.,ATSS,and Faster R-CNN. Different label assignments enrich the supervisions on the encoder's output which forces it to be discriminative enough to support the training convergence of these heads.Specifically, given the encoder's latent feature $\mathcal { F }$ ， we firstly transform it to the feature pyramid $\{ \mathcal { F } _ { 1 } , \cdot \cdot \cdot , \mathcal { F } _ { J } \}$ via the multi-scale adapter where $J$ indicates feature map with $2 ^ { 2 + J }$ downsampling stride. Similar to ViTDet [20], the feature pyramid is constructed by a single feature map in the single-scale encoder, while we use bilinear interpolation and $3 \times 3$ convolution for upsampling. For instance, with the single-scale feature from the encoder, we successively apply downsampling ( $3 \times 3$ convolution with stride 2) or upsampling operations to produce a feature pyramid.As for the multi-scale encoder, we only downsample the coarsest feature in the multi-scale encoder features $\mathcal { F }$ to build the feature pyramid. Defined $K$ collaborative heads with corresponding label assignment manners $\mathcal { A } _ { k }$ ，for the $i$ -th collaborative head, $\{ \mathcal { F } _ { 1 } , \cdot \cdot \cdot , \mathcal { F } _ { J } \}$ is sent to it to obtain the predictions $\hat { { \bf P } } _ { i }$ . At the $i$ -th head, $A _ { i }$ is used to compute the supervised targets for the positive and negative samples in $\mathbf { P } _ { i }$ . Denoted $\mathbf { G }$ as the ground-truth set, this procedure can be formulated as:

$$
\mathbf { P } _ { i } ^ { \{ p o s \} } , \mathbf { B } _ { i } ^ { \{ p o s \} } , \mathbf { P } _ { i } ^ { \{ n e g \} } = \mathcal { A } _ { i } ( \hat { \mathbf { P } } _ { i } , \mathbf { G } ) ,
$$

<table><tr><td rowspan=2 colspan=1>Head i</td><td rowspan=2 colspan=1>Loss Li</td><td rowspan=1 colspan=3>Assignment Ai</td></tr><tr><td rowspan=1 colspan=1>{pos},{neg} Generation</td><td rowspan=1 colspan=1>Pi Generation</td><td rowspan=1 colspan=1>BPosGeneration</td></tr><tr><td rowspan=1 colspan=1>Faster-RCNN [27]</td><td rowspan=1 colspan=1>cls: CE loss,reg: GIoU loss</td><td rowspan=1 colspan=1>{pos}: IoU(proposal, gt)&gt;0.5{neg}: IoU(proposal, gt)&lt;0.5</td><td rowspan=1 colspan=1>{pos}: gt labels, offset(proposal, gt){neg}: gt labels</td><td rowspan=1 colspan=1>positive proposals(x1,y1,x2,y2）</td></tr><tr><td rowspan=1 colspan=1>ATSS [40]</td><td rowspan=1 colspan=1>cls: Focal lossreg: GIoU,BCE loss</td><td rowspan=1 colspan=1>{pos}:IoU(anchor, gt)&gt;(mean+std){neg}: IoU(anchor, gt)&lt;(mean+std)</td><td rowspan=1 colspan=1>{pos}: gt labels, offset(anchor, gt), centerness{neg}: gt labels</td><td rowspan=1 colspan=1>positive anchors(x1,y1,x2,y2)</td></tr><tr><td rowspan=1 colspan=1>RetinaNet [21]</td><td rowspan=1 colspan=1>cls: Focal lossreg: GIoU Loss</td><td rowspan=1 colspan=1>{pos}: IoU(anchor, gt）&gt;0.5{neg}: IoU(anchor, gt)&lt;0.4</td><td rowspan=1 colspan=1>{pos}: gt labels, offset(anchor, gt){neg}: gt labels</td><td rowspan=1 colspan=1>positive anchors(x1,y1,x2,y2)</td></tr><tr><td rowspan=1 colspan=1>FCOS [31]</td><td rowspan=1 colspan=1>cls: Focal Lossreg: GIoU,BCE loss</td><td rowspan=1 colspan=1>{pos}:points inside gt center area{neg}: points outside gt center area</td><td rowspan=1 colspan=1>{pos}: gt labels, Itrb distance, centerness{neg}: gt labels</td><td rowspan=1 colspan=1>FCOS point (cx,cy)W=h=8×22+j</td></tr></table>

Table 1:Detailed information of auxiliary heads.The auxiliary heads include Faster-RCNN[27],ATSS[40],RetinaNet[21],andFCOS[31].Ifnot otherwise specified,we follw the original implementations,e.g.,anchor generation.

where $\{ p o s \}$ and $\{ n e g \}$ indicate the pair set of $( j$ ，positive coordinates or negative coordinates in ${ \mathcal { F } } _ { j }$ ）determined by $A _ { i }$ $j$ means the feature index in $\{ \mathcal { F } _ { 1 } , \cdot \cdot \cdot , \mathcal { F } _ { J } \}$ $\mathbf { B } _ { i } ^ { \{ p o s \} }$ is the set of spatial positivecoordinates.P{pos) and P{neg} are the supervised targets in the corresponding coordinates, including the categories and regressed offsets. To be specific,we describe the detailed information about each variable in Table 1. The loss functions can be defined as:

$$
\mathcal { L } _ { i } ^ { e n c } = \mathcal { L } _ { i } ( \hat { \mathbf { P } } _ { i } ^ { \{ p o s \} } , \mathbf { P } _ { i } ^ { \{ p o s \} } ) + \mathcal { L } _ { i } ( \hat { \mathbf { P } } _ { i } ^ { \{ n e g \} } , \mathbf { P } _ { i } ^ { \{ n e g \} } ) ,
$$

Note that the regression loss is discarded for negative samples.The training objective of the optimization for $K$ auxiliary heads is formulated as follows:

$$
\mathcal { L } ^ { e n c } = \sum _ { i = 1 } ^ { K } \mathcal { L } _ { i } ^ { e n c }
$$

# 3.3. Customized Positive Queries Generation

In the one-to-one set matching paradigm,each groundtruth box will only be assigned to one specific query as the supervised target. Too few positive queries lead to inefficient cross-attention learning in the transformer decoder as shown in Figure 2. To alleviate this, we elaborately generate sufficient customized positive queries according to the label assignment $A _ { i }$ in each auxiliary head. Specifically, given the positive coordinates set $\mathbf { B } _ { i } ^ { \{ p o s \} } \in \bar { \mathbb { R } } ^ { \bar { M } _ { i } \times 4 }$ in the $i$ -th auxiliary head,where $M _ { i }$ is the number of positive samples, the extra customized positive queries Qi ∈ RMi ×C can be generated by:

$$
\begin{array} { r } { \mathbf { Q } _ { i } = \operatorname { L i n e a r } ( \operatorname { P E } ( \mathbf { B } _ { i } ^ { \{ p o s \} } ) ) + \operatorname { L i n e a r } ( \operatorname { E } ( \{ \mathcal { F } _ { * } \} , \{ p o s \} ) ) . } \end{array}
$$

where $\mathrm { P E } ( \cdot )$ stands for positional encodings and we select the corresponding features from $\operatorname { E } ( \cdot )$ according to the index pair ( $j$ , positive coordinates or negative coordinates in $\mathcal { F } _ { j }$ ).

As a result, there are $K + 1$ groups of queries that contribute to a single one-to-one set matching branch and $K$ branches with one-to-many label assignments during training.The auxiliary one-to-many label assignment branches share the same parameters with $L$ decoders layers in the original main branch.All the queries in the auxiliary branch are regarded as positive queries, thus the matching process is discarded. To be specific,the loss of the $l$ -th decoder layer in the $i$ -th auxiliary branch can be formulated as:

$$
\mathcal { L } _ { i , l } ^ { d e c } = \widetilde { \mathcal { L } } ( \widetilde { \mathbf { P } } _ { i , l } , \mathbf { P } _ { i } ^ { \{ p o s \} } ) .
$$

$\widetilde { \mathbf { P } } _ { i , l }$ refers to the output predictions of the $l$ -th decoder layer in the $i$ -th auxiliary branch. Finally,the training objective for $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETRis:

$$
\mathcal { L } ^ { g l o b a l } = \sum _ { l = 1 } ^ { L } ( \widetilde { \mathcal { L } } _ { l } ^ { d e c } + \lambda _ { 1 } \sum _ { i = 1 } ^ { K } \mathcal { L } _ { i , l } ^ { d e c } + \lambda _ { 2 } \mathcal { L } ^ { e n c } ) ,
$$

where Ldec stands for the loss in the original one-to-one set matching branch [1], $\lambda _ { 1 }$ and $\lambda _ { 2 }$ are the coefficient balancing the losses.

# 3.4. Why Co-DETR works

$\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR leads to evident improvement to the DETRbased detectors. In the following,we try to investigate its effectiveness qualitatively and quantitatively. We conduct detailed analysis based on Deformable-DETR with ResNet50 [15] backbone using the 36-epoch setting.

Enrich the encoder's supervisions.Intuitively, too few positive queries lead to sparse supervisions as only one query is supervised by regression loss for each ground-truth. The positive samples in one-to-many label assignment manners receive more localization supervisions to help enhance the latent feature learning. To further explore how the sparse supervisions impede the model training，we detailedly investigate the latent features produced by the encoder. We introduce the IoF-IoB curve to quantize the discriminability score of the encoder's output. Specifically，given the latent feature $\mathcal { F }$ of the encoder, inspired by the feature visualization in Figure 3,we compute the IoF (intersection over foreground) and IoB (intersection over background).Given the encoder's feature $\mathcal { F } _ { j } \in \mathbb { R } ^ { C \times H _ { j } \times W _ { j } }$ at level $j$ ,we first calculate the $l ^ { 2 }$ -norm $\widehat { \mathcal { F } } _ { j } \in \mathbb { R } ^ { 1 \times H _ { j } \times W _ { j } }$ and resize it to the image size $H \times W$ .The discriminability score ${ \mathcal { D } } ( { \mathcal { F } } )$ is computed by averaging the scores from all levels:

![](Images_KU3GJDWR/122fb3cc4eedb20d9eba58a28e31df1626cfdaf26be32910a451032429558142.jpg)  
Figure 5: The instability (IS)[18] of Deformable-DETR and $\scriptstyle { \mathcal { C } } _ { 0 }$ -Deformable-DETR on COCO dataset.

$$
\mathcal { D } ( \mathcal { F } ) = \frac { 1 } { J } \sum _ { j = 1 } ^ { J } \frac { \widehat { \mathcal { F } } _ { j } } { m a x ( \widehat { \mathcal { F } } _ { j } ) } ,
$$

where the resize operation is omitted. We visualize the discriminability scores of ATSS,Deformable-DETR,and our $\scriptstyle { \mathcal { C } } _ { 0 }$ -Deformable-DETR in Figure 3. Compared with Deformable-DETR,both ATSS and $\scriptstyle { \mathcal { C } } _ { 0 } .$ -Deformable-DETR own stronger ability to distinguish the areas of key objects, while Deformable-DETR is almost disturbed by the background. Consequently,we define the indicators for foreground and background as $\mathbb { 1 } ( \mathcal { D } ( \mathcal { F } ) > S ) \in \mathbb { R } ^ { H \times W }$ and $\mathbb { 1 } ( \mathcal { D } ( \mathcal { F } ) < S ) \in \mathbb { R } ^ { H \times W }$ ,respectively. $S$ is a predefined score thresh, $\mathbb { 1 } ( x )$ is 1if $x$ is true and O otherwise.As for the mask of foreground $\mathcal { M } ^ { f g } \in \mathbb { R } ^ { H \times W }$ , the element $\mathcal { M } _ { h , w } ^ { f g }$ is 1 if the point $( h , w )$ is inside the foreground and O otherwise.The area of intersection over foreground (IoF) $\boldsymbol { \mathcal { T } ^ { f g } }$ can be computed as:

$$
\mathcal { T } ^ { f g } = \frac { \sum _ { h = 1 } ^ { H } \sum _ { w = 1 } ^ { W } \bigr ( \mathbb { 1 } \bigl ( \mathcal { D } ( \mathcal { F } _ { h , w } ) > S \bigr ) \cdot \mathcal { M } _ { h , w } ^ { f g } \bigr ) } { \sum _ { h = 1 } ^ { H } \sum _ { w = 1 } ^ { W } \mathcal { M } _ { h , w } ^ { f g } } .
$$

Concretely, we compute the area of intersection over background areas (IoB) in a similar way and plot the curve IoF and IoB by varying $S$ in Figure 2. Obviously,ATSS and $C \mathbf { o }$ -Deformable-DETR obtain higher IoF values than both Deformable-DETR and Group-DETR under the same IoB values,which demonstrates the encoder representations benefit from the one-to-many label assignment.

Improve the cross-attention learning by reducing the instability of Hungarian matching. Hungarian matching is the core scheme in one-to-one set matching. Cross-attention is an important operation to help the positive queries encode abundant object information. It requires suficient training to achieve this. We observe that the Hungarian matching introduces uncontrollable instability since the ground-truth assigned to a specific positive query in the same image is changing during the training process. Following [18],we present the comparison of instability in Figure 5,where we find our approach contributes to a more stable matching process.Furthermore,in order to quantify how well crossattention is being optimized,we also calculate the IoF-IoB curve for attention score. Similar to the feature discriminability score computation,we set different thresholds for attention score to get multiple IoF-IoB pairs. The comparisons between Deformable-DETR, Group-DETR,and $\scriptstyle { \mathcal { C } } 0 \cdot$ Deformable-DETR can be viewed in Figure 2.We find that the IoF-IoB curves of DETRs with more positive queries are generally above Deformable-DETR,which is consistent with our motivation.

# 3.5.Comparison with other methods

Differences between our method and other counterparts.Group-DETR, $\mathcal { H }$ -DETR,and SQR[2] perform oneto-many assignments by one-to-one matching with duplicate groups and repeated ground-truth boxes. $\scriptstyle { \mathcal { C } } _ { 0 } .$ -DETR explicitly assigns multiple spatial coordinates as positives for each ground truth. Accordingly, these dense supervision signals are directly applied to the latent feature map to enable it more discriminative.By contrast, Group-DETR, $\varkappa$ - DETR,and SQR lack this mechanism. Although more positive queries are introduced in these counterparts,the oneto-many assignments implemented by Hungarian Matching still suffer from the instability issues of one-to-one matching. Our method benefits from the stability of off-theshelf one-to-many assignments and inherits their specific matching manner between positive queries and ground-truth boxes.Group-DETR and $\mathcal { H }$ -DETR fail to reveal the complementarities between one-to-one matching and traditional one-to-many assignment.To our best knowledge,we are the first to give the quantitative and qualitative analysis on the detectors with the traditional one-to-many assignment and one-to-one matching.This helps us better understand their differences and complementarities so that we can naturally improve the DETR's learning ability by leveraging off-theshelf one-to-many assignment designs without requiring additional specialized one-to-many design experience.

No negative queries are introduced in the decoder. Duplicate object queries inevitably bring large amounts of negative queries for the decoder and a significant increase in GPU memory. However, our method only processes the positive coordinates in the decoder, thus consuming less memory as shown in Table 7.

# 4. Experiments

# 4.1. Setup

Datasets and Evaluation Metrics. Our experiments are conducted on the MS COCO 2017 dataset [22] and LVIS vl.0 dataset [12]. The COCO dataset consists of 115K labeled images for training and 5K images for validation. We report the detection results by default on the val subset. The results of our largest model evaluated on the test-dev (2OK images) are also reported. LVIS v1.0 is a large-scale and long-tail dataset with 12O3 categories for large vocabulary instance segmentation. To verify the scalability of $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR,we further apply it to a large-scale object detection benchmark, namely Objects365 [29]. There are 1.7M labeled images used for training and 8OK images for validation in the Objects365 dataset.All results follow the standard mean Average Precision(AP) under IoU thresholds ranging from O.5 to O.95 at different object scales.

Table 2: Results of plain baselines on COCO val.   

<table><tr><td rowspan=1 colspan=1>Method</td><td rowspan=1 colspan=1>K</td><td rowspan=1 colspan=1>#epochs</td><td rowspan=1 colspan=1>AP</td></tr><tr><td rowspan=1 colspan=1>Conditional DETR-C5 [26]Conditional DETR-C5 [26]Conditional DETR-C5 [26]</td><td rowspan=1 colspan=1>012</td><td rowspan=1 colspan=1>363636</td><td rowspan=1 colspan=1>39.441.5(+2.1)41.8(+2.4)</td></tr><tr><td rowspan=1 colspan=1>DAB-DETR-C5 [23]DAB-DETR-C5 [23]DAB-DETR-C5 [23]</td><td rowspan=1 colspan=1>012</td><td rowspan=1 colspan=1>363636</td><td rowspan=1 colspan=1>41.243.1(+1.9)43.5(+2.3)</td></tr><tr><td rowspan=1 colspan=1>Deformable-DETR [42]Deformable-DETR [42]Deformable-DETR [42]</td><td rowspan=1 colspan=1>012</td><td rowspan=1 colspan=1>121212</td><td rowspan=1 colspan=1>37.142.3(+5.2)42.9(+5.8)</td></tr><tr><td rowspan=2 colspan=1>Deformable-DETR [42]Deformable-DETR [42]Deformable-DETR [42]</td><td rowspan=1 colspan=1>01</td><td rowspan=1 colspan=1>3636</td><td rowspan=2 colspan=1>43.346.8(+3.5)46.5(+3.2)</td></tr><tr><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>36</td></tr></table>

Implementation Details. We incorporate our $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR into the current DETR-like pipelines and keep the training setting consistent with the baselines. We adopt ATSS and Faster-RCNN as the auxiliary heads for $K = 2$ and only keep ATSS for $K = 1$ . More details about our auxiliary heads can be found in the supplementary materials. We choose the number of learnable object queries to 300 and set $\{ \lambda _ { 1 } , \lambda _ { 2 } \}$ to $\lbrace 1 . 0 , 2 . 0 \rbrace$ by default. For $\scriptstyle { \mathcal { C } } _ { 0 }$ -DINODeformable-DETR $^ { + + }$ , we use large-scale jitter with copypaste [10].

# 4.2.Main Results

In this section,we empirically analyze the effectiveness and generalization ability of $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETRon differentDETR variants in Table 2 and Table 3. All results are reproduced using mmdetection [4]. We first apply the collaborative hybrid assignments training to single-scale DETRs with C5 features.Surprisingly, both Conditional-DETR and DAB-DETR obtain $2 . 4 \%$ and $2 . 3 \%$ AP gains over the baselines with a long training schedule. For DeformableDETR with multi-scale features, the detection performance is significantly boosted from $3 7 . 1 \%$ to $4 2 . 9 \%$ AP. The overall improvements $( + 3 . 2 \%$ AP) still hold when the training time is increased to 36 epochs.Moreover, we conduct experiments on the improved Deformable-DETR (denoted as Deformable-DETR $^ { + + }$ ） following [16],where a $+ 2 . 4 \%$ AP gain is observed. The state-of-the-art DINO-DeformableDETR equipped with our method can achieve $5 1 . 2 \%$ AP, which is $+ 1 . 8 \%$ AP higher than the competitive baseline.

Table 3:Results of strong baselines on COCO val．Methods with $\dagger$ use 5 feature levels. $^ \ddag$ refers to Swin-L backbone.   

<table><tr><td rowspan=1 colspan=1>Method</td><td rowspan=1 colspan=1>K</td><td rowspan=1 colspan=1>#epochs</td><td rowspan=1 colspan=1>AP</td></tr><tr><td rowspan=1 colspan=1>Deformable-DETR++ [42]Deformable-DETR++ [42]Deformable-DETR++ [42]</td><td rowspan=1 colspan=1>012</td><td rowspan=1 colspan=1>121212</td><td rowspan=1 colspan=1>47.148.7(+1.6)49.5(+2.4)</td></tr><tr><td rowspan=1 colspan=1>DINO-Deformable-DETR† [38]DINO-Deformable-DETR† [38]DINO-Deformable-DETR† [38]</td><td rowspan=1 colspan=1>012</td><td rowspan=1 colspan=1>121212</td><td rowspan=1 colspan=1>49.451.0(+1.6)51.2(+1.8)</td></tr><tr><td rowspan=1 colspan=1>Deformable-DETR++ [42]Deformable-DETR++* [42]Deformable-DETR++* [42]</td><td rowspan=1 colspan=1>012</td><td rowspan=1 colspan=1>121212</td><td rowspan=1 colspan=1>55.256.4(+1.2)56.9(+1.7)</td></tr><tr><td rowspan=3 colspan=1>DINO-Deformable-DETRt‡ [38]DINO-Deformable-DETR†‡ [38]DINO-Deformable-DETRt‡ [38]</td><td rowspan=1 colspan=1>0</td><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>58.5</td></tr><tr><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>59.3(+0.8)</td></tr><tr><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>59.5(+1.0)</td></tr></table>

We further scale up the backbone capacity from ResNet50 to Swin-L [25] based on two state-of-the-art baselines.As presented in Table 3, $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETRachieves $5 6 . 9 \%$ AP and surpasses the Deformable- $\mathrm { . D E T R + + }$ baseline by a large margin $( + 1 . 7 \%$ AP). The performance of DINODeformable-DETR with Swin-L can still be boosted from $5 8 . 5 \%$ to $5 9 . 5 \%$ AP.

# 4.3. Comparisons with the state-of-the-art

We apply our method with $K \ = \ 2$ to Deformable$\mathrm { D E T R + + }$ and DINO.Besides, the quality focal loss [19] and NMS are adopted for our $\scriptstyle { \mathcal { C } } _ { 0 }$ -DINO-Deformable-DETR. We report the comparisons on COCO val in Table 4. Compared with other competitive counterparts,our method converges much faster. For example, $\scriptstyle { \mathcal { C } } _ { 0 }$ -DINO-DeformableDETR readily achieves $5 2 . 1 \%$ AP when using only 12 epochs with ResNet-50 backbone. Our method with SwinL can obtain $5 8 . 9 \%$ AP for $1 \times$ scheduler, even surpassing other state-of-the-art frameworks on $3 \times$ scheduler. More importantly, our best model $\scriptstyle { \mathcal { C } } _ { 0 }$ -DINO-Deformable$\mathrm { D E T R + + }$ achieves $5 4 . 8 \%$ AP with ResNet-50 and $6 0 . 7 \%$ AP with Swin-L under 36-epoch training, outperforming all existing detectors with the same backbone by clear margins.

To further explore the scalability of our method, we extend the backbone capacity to 3O4 million parameters. This large-scale backbone ViT-L [7] is pre-trained using a selfsupervised learning method (EVA-O2 [8]). We first pre-train $\scriptstyle { \mathcal { C } } _ { 0 }$ -DINO-Deformable-DETR with ViT-L on Objects365 for 26 epochs, then fine-tune it on the COCO dataset for 12 epochs.In the fine-tuning stage,the input resolution is randomly selected between $4 8 0 \times 2 4 0 0$ and $1 5 3 6 \times 2 4 0 0$ The detailed settings are available in supplementary materials. Our results are evaluated with test-time augmentation. Table 5 presents the state-of-the-art comparisons on the COCO test-dev benchmark. With much fewer model sizes (304M parameters), $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR sets a new record of $6 6 . 0 \%$ AP on COCO test-dev, outperforming the previous best model InternImage-G [33] by $+ 0 . 5 \%$ AP.

Table 4: Comparison to the state-of-the-art DETR variants on COCO val.   

<table><tr><td>Method</td><td>Backbone</td><td>Multi-scale</td><td>#query</td><td>#epochs</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td>Conditional-DETR [26]</td><td>R50</td><td>X</td><td>300</td><td>108</td><td>43.0</td><td>64.0</td><td>45.7</td><td>22.7</td><td>46.7</td><td>61.5</td></tr><tr><td>Anchor-DETR [34]</td><td>R50</td><td>X</td><td>300</td><td>50</td><td>42.1</td><td>63.1</td><td>44.9</td><td>22.3</td><td>46.2</td><td>60.0</td></tr><tr><td>DAB-DETR [23]</td><td>R50</td><td>X</td><td>900</td><td>50</td><td>45.7</td><td>66.2</td><td>49.0</td><td>26.1</td><td>49.4</td><td>63.1</td></tr><tr><td>AdaMixer [9]</td><td>R50</td><td>√</td><td>300</td><td>36</td><td>47.0</td><td>66.0</td><td>51.1</td><td>30.1</td><td>50.2</td><td>61.8</td></tr><tr><td>Deformable-DETR [42]</td><td>R50</td><td>√</td><td>300</td><td>50</td><td>46.9</td><td>65.6</td><td>51.0</td><td>29.6</td><td>50.1</td><td>61.6</td></tr><tr><td>DN-Deformable-DETR[18]</td><td>R50</td><td>√</td><td>300</td><td>50</td><td>48.6</td><td>67.4</td><td>52.7</td><td>31.0</td><td>52.0</td><td>63.7</td></tr><tr><td>DINO-Deformable-DETR† [38]</td><td>R50</td><td>√</td><td>900</td><td>12</td><td>49.4</td><td>66.9</td><td>53.8</td><td>32.3</td><td>52.5</td><td>63.9</td></tr><tr><td>DINO-Deformable-DETR† [38]</td><td>R50</td><td>√</td><td>900</td><td>36</td><td>51.2</td><td>69.0</td><td>55.8</td><td>35.0</td><td>54.3</td><td>65.3</td></tr><tr><td>DINO-Deformable-DETR† [38]</td><td>Swin-L (IN-22K)</td><td>√</td><td>900</td><td>36</td><td>58.5</td><td>77.0</td><td>64.1</td><td>41.5</td><td>62.3</td><td>74.0</td></tr><tr><td>Group-DINO-Deformable-DETR [5]</td><td>Swin-L (IN-22K)</td><td>√</td><td>900</td><td>36</td><td>58.4</td><td>-</td><td>-</td><td>41.0</td><td>62.5</td><td>73.9</td></tr><tr><td>H-Deformable-DETR[16]</td><td>R50</td><td>√</td><td>300</td><td>12</td><td>48.7</td><td>66.4</td><td>52.9</td><td>31.2</td><td>51.5</td><td>63.5</td></tr><tr><td>H-Deformable-DETR [16]</td><td>Swin-L (IN-22K)</td><td>√</td><td>900</td><td>36</td><td>57.9</td><td>76.8</td><td>63.6</td><td>42.4</td><td>61.9</td><td>73.4</td></tr><tr><td>Co-Deformable-DETR</td><td>R50</td><td>√</td><td>300</td><td>12</td><td>49.5</td><td>67.6</td><td>54.3</td><td>32.4</td><td>52.7</td><td>63.7</td></tr><tr><td>Co-Deformable-DETR</td><td>Swin-L (IN-22K)</td><td>√</td><td>900</td><td>36</td><td>58.5</td><td>77.1</td><td>64.5</td><td>42.4</td><td>62.4</td><td>74.0</td></tr><tr><td>Co-DINO-Deformable-DETRt</td><td>R50</td><td>√</td><td>900</td><td>12</td><td>52.1</td><td>69.4</td><td>57.1</td><td>35.4</td><td>55.4</td><td>65.9</td></tr><tr><td>Co-DINO-Deformable-DETRt</td><td>Swin-L (IN-22K)</td><td>√</td><td>900</td><td>12</td><td>58.9</td><td>76.9</td><td>64.8</td><td>42.6</td><td>62.7</td><td>75.1</td></tr><tr><td>Co-DINO-Deformable-DETRt</td><td>Swin-L (IN-22K)</td><td>√</td><td>900</td><td>24</td><td>59.8</td><td>77.7</td><td>65.5</td><td>43.6</td><td>63.5</td><td>75.5</td></tr><tr><td>Co-DINO-Deformable-DETRt</td><td> Swin-L (IN-22K)</td><td>√</td><td>900</td><td>36</td><td>60.0</td><td>77.7</td><td>66.1</td><td>44.6</td><td>63.9</td><td>75.7</td></tr><tr><td>Co-DINO-Deformable-DETR++†</td><td>R50</td><td>√</td><td>900</td><td>12</td><td> 52.1</td><td>69.3</td><td>57.3</td><td>35.4</td><td>55.5</td><td>67.2</td></tr><tr><td>Co-DINO-Deformable-DETR++†</td><td>R50</td><td>√</td><td>900</td><td>36</td><td> 54.8</td><td>72.5</td><td>60.1</td><td>38.3</td><td>58.4</td><td>69.6</td></tr><tr><td> Co-DINO-Deformable-DETR++†</td><td> Swin-L (IN-22K)</td><td>√</td><td>900</td><td>12</td><td> 59.3</td><td>77.3</td><td>64.9</td><td>43.3</td><td>63.3</td><td>75.5</td></tr><tr><td> Co-DINO-Deformable-DETR++†</td><td>Swin-L (IN-22K)</td><td>√</td><td>900</td><td>24</td><td>60.4</td><td>78.3</td><td>66.4</td><td>44.6</td><td>64.2</td><td>76.5</td></tr><tr><td>Co-DINO-Deformable-DETR++†</td><td> Swin-L (IN-22K)</td><td>√</td><td>900</td><td>36</td><td>60.7</td><td>78.5</td><td>66.7</td><td>45.1</td><td>64.7</td><td>76.4</td></tr></table>

+:5 feature levels.

Table 5: Comparison to the state-of-the-art frameworks on COCO.   

<table><tr><td>Method</td><td>Backbone</td><td>enc. #params</td><td>val APbor</td><td>test-dev APbor</td></tr><tr><td>HTC++ [3]</td><td>SwinV2-G [24]</td><td>3.0B</td><td>62.5</td><td>63.1</td></tr><tr><td>DINO [38]</td><td>Swin-L [25]</td><td>218M</td><td>63.2</td><td>63.3</td></tr><tr><td>BEIT3 [32]</td><td>ViT-g [7]</td><td>1.9B</td><td>-</td><td>63.7</td></tr><tr><td>FD [35]</td><td>SwinV2-G [24]</td><td>3.0B</td><td>-</td><td>64.2</td></tr><tr><td>DINO [38]</td><td>FocalNet-H [37]</td><td>746M</td><td>64.2</td><td>64.3</td></tr><tr><td>Group DETRv2 [6]</td><td>ViT-H [7]</td><td>629M</td><td>-</td><td>64.5</td></tr><tr><td>EVA-02 [8]</td><td>ViT-L [7]</td><td>304M</td><td>64.1</td><td>64.5</td></tr><tr><td>DINO [38]</td><td>InternImage-G [33]</td><td>3.0B</td><td>65.3</td><td>65.5</td></tr><tr><td>Co-DETR</td><td>ViT-L [7]</td><td>304M</td><td>65.9</td><td>66.0</td></tr></table>

We also demonstrate the best results of $\scriptstyle { \mathcal { C } } _ { 0 } .$ -DETRon the long-tailed LVIS detection dataset. In particular, we use the same $\scriptstyle { \mathcal { C } } _ { 0 }$ -DINO-Deformable-DETR $^ { + + }$ as the model on COCO but choose FedLoss [41] as the classification loss to remedy the impact of unbalanced data distribution.

Table 6: Comparison to the state-of-the-art frameworks on LVIS.   

<table><tr><td rowspan=1 colspan=1>Method</td><td rowspan=1 colspan=1>Backbone</td><td rowspan=1 colspan=1>enc.#params</td><td rowspan=1 colspan=1>valApbox</td><td rowspan=1 colspan=1>minivalApbox</td></tr><tr><td rowspan=1 colspan=1>H-DETR [16]</td><td rowspan=1 colspan=1>Swin-L [25]</td><td rowspan=1 colspan=1>218M</td><td rowspan=3 colspan=1>47.951.253.4</td><td rowspan=6 colspan=1>--59.865.8-</td></tr><tr><td rowspan=1 colspan=1>ViTDet [20]</td><td rowspan=1 colspan=1>ViT-L [7]</td><td rowspan=2 colspan=1>307M632M</td><td rowspan=1 colspan=1>307M</td></tr><tr><td rowspan=1 colspan=1>ViTDet [20]</td><td rowspan=1 colspan=1>ViT-H[7]</td></tr><tr><td rowspan=1 colspan=1>GLIPv2 [39]</td><td rowspan=1 colspan=1>Swin-H[25]</td><td rowspan=1 colspan=1>637M</td><td rowspan=1 colspan=1>-</td></tr><tr><td rowspan=2 colspan=1>DINO [38]EVA-02 [8]</td><td rowspan=2 colspan=1>InternImage-G [33]ViT-L [7]</td><td rowspan=1 colspan=1>3.0B</td><td rowspan=2 colspan=1>63.265.2</td></tr><tr><td rowspan=1 colspan=1>304M</td></tr><tr><td rowspan=1 colspan=1>Co-DETR</td><td rowspan=1 colspan=1>Swin-L [25]</td><td rowspan=1 colspan=1>218M</td><td rowspan=1 colspan=1>56.9</td><td rowspan=2 colspan=1>62.371.9</td></tr><tr><td rowspan=1 colspan=1>Co-DETR</td><td rowspan=1 colspan=1>ViT-L [7]</td><td rowspan=1 colspan=1>304M</td><td rowspan=1 colspan=1>67.9</td></tr></table>

Here,we only apply bounding boxes supervision and report the object detection results. The comparisons are available in Table 6. $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR with Swin-L yields $5 6 . 9 \%$ and $6 2 . 3 \%$ AP on LVIS $\mathtt { v a l }$ and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [39] by $+ 3 . 5 \%$ and $+ 2 . 5 \%$ AP,respectively. We further finetune the Objects365 pretrained $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR on this dataset. Without elaborate test-time augmentation,our approach achieves the best detection performance of $6 7 . 9 \%$ and $7 1 . 9 \%$ AP on LVIS val and minival. Compared to the 3-billion parameter InternImage-G with test-time augmentation,we obtain $+ 4 . 7 \%$ and $+ 6 . 1 \%$ AP gains on LVIS val and minival while reducing the model size to 1/10.

Table 7: Experimental results of $K$ varying from 1 to 6.   

<table><tr><td rowspan=1 colspan=1>Method</td><td rowspan=1 colspan=1>K</td><td rowspan=1 colspan=1>Auxiliaryhead</td><td rowspan=1 colspan=1>Memory(MB)</td><td rowspan=1 colspan=1>GPUhours</td><td rowspan=1 colspan=1>AP</td></tr><tr><td rowspan=1 colspan=1>Deformable-DETR++</td><td rowspan=1 colspan=1>0</td><td rowspan=1 colspan=1>-</td><td rowspan=1 colspan=1>12808</td><td rowspan=1 colspan=1>70</td><td rowspan=1 colspan=1>47.1</td></tr><tr><td rowspan=1 colspan=1>H-Deformable-DETR</td><td rowspan=1 colspan=1>0</td><td rowspan=1 colspan=1>-</td><td rowspan=1 colspan=1>15307</td><td rowspan=1 colspan=1>104</td><td rowspan=1 colspan=1>48.4</td></tr><tr><td rowspan=1 colspan=1>Deformable-DETR++</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>ATSS</td><td rowspan=1 colspan=1>13947</td><td rowspan=1 colspan=1>86</td><td rowspan=1 colspan=1>48.7</td></tr><tr><td rowspan=1 colspan=1>Deformable-DETR++</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>ATSS +PAA</td><td rowspan=1 colspan=1>14629</td><td rowspan=1 colspan=1>124</td><td rowspan=1 colspan=1>49.0</td></tr><tr><td rowspan=1 colspan=1>Deformable-DETR++</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>ATSS + Faster-RCNN</td><td rowspan=1 colspan=1>14387</td><td rowspan=1 colspan=1>120</td><td rowspan=1 colspan=1>49.5</td></tr><tr><td rowspan=1 colspan=1>Deformable-DETR++</td><td rowspan=1 colspan=1>3</td><td rowspan=1 colspan=1>ATSS +Faster-RCNN+ PAA</td><td rowspan=1 colspan=1>15263</td><td rowspan=1 colspan=1>150</td><td rowspan=1 colspan=1>49.5</td></tr><tr><td rowspan=1 colspan=1>Deformable-DETR++</td><td rowspan=1 colspan=1>6</td><td rowspan=1 colspan=1>ATSS + Faster-RCNN+ PAA+RetinaNet+ FCOS + GFL</td><td rowspan=1 colspan=1>19385</td><td rowspan=1 colspan=1>280</td><td rowspan=1 colspan=1>48.9</td></tr></table>

<table><tr><td rowspan=1 colspan=1>Auxiliary head</td><td rowspan=1 colspan=1>#epochs</td><td rowspan=1 colspan=1>AP</td><td rowspan=1 colspan=1>AP50</td><td rowspan=1 colspan=1>AP75</td></tr><tr><td rowspan=1 colspan=1>Baseline</td><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>43.3</td><td rowspan=1 colspan=1>62.3</td><td rowspan=1 colspan=1>47.1</td></tr><tr><td rowspan=6 colspan=1>RetinaNet [21]Faster-RCNN [27]Mask-RCNN [14]FCOS [31]PAA [17]GFL [19]</td><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>46.1</td><td rowspan=1 colspan=1>64.2</td><td rowspan=1 colspan=1>50.1</td></tr><tr><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>46.3</td><td rowspan=1 colspan=1>64.7</td><td rowspan=1 colspan=1>50.5</td></tr><tr><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>46.5</td><td rowspan=1 colspan=1>65.0</td><td rowspan=1 colspan=1>50.6</td></tr><tr><td rowspan=2 colspan=1>3636</td><td rowspan=1 colspan=1>46.5</td><td rowspan=1 colspan=1>64.8</td><td rowspan=2 colspan=1>50.750.7</td></tr><tr><td rowspan=1 colspan=1>46.5</td><td rowspan=1 colspan=1>64.6</td></tr><tr><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>46.5</td><td rowspan=1 colspan=1>65.0</td><td rowspan=1 colspan=1>51.0</td></tr><tr><td rowspan=1 colspan=1> ATSS [40]</td><td rowspan=1 colspan=1>36</td><td rowspan=1 colspan=1>46.8</td><td rowspan=1 colspan=1>65.1</td><td rowspan=1 colspan=1> 51.5</td></tr></table>

Table 8: Performance of our approach with various auxiliary oneto-many heads on COCO val.

# 4.4. Ablation Studies

Unless stated otherwise,all experiments for ablations are conducted on Deformable-DETR with a ResNet-50 backbone. We choose the number of auxiliary heads $K$ to 1 by default and set the total batch size to 32.More ablations and analyses can be found in the supplementary materials.

Criteria for choosing auxiliary heads. We further delve into the criteria for choosing auxiliary heads in Table 7 and 8.The results in Table 8 reveal that any auxiliary head with one-to-many label assignments consistently improves the baseline and ATSS achieves the best performance.We find the accuracy continues to increase as $K$ increases when choosing $K$ smaller than 3.It is worth noting that performance degradation occurs when $K = 6$ ，and we speculate the severe conflicts among auxiliary heads cause this. If the feature learning is inconsistent across the auxiliary heads, the continuous improvement as $K$ becomes larger will be destroyed.We also analyze the optimization consistency of multiple heads next and in the supplementary materials. In summary, we can choose any head as the auxiliary head and we regard ATSS and Faster-RCNN as the common practice to achieve the best performance when $K \leq 2$ . We do not use too many different heads,e.g.,6 different heads to avoid optimization conflicts.

Conflicts analysis. The conflicts emerge when the same spatial coordinate is assigned to different foreground boxes or treated as background in different auxiliary heads and can confuse the training of the detector. We first define the distance between head $H _ { i }$ and head $H _ { j }$ ， and the average distance of $H _ { i }$ to measure the optimization conflicts as:

![](Images_KU3GJDWR/4086f98bb0f1f49585091063f3b328bd2206d61dd4fe6a46488f2d040dd4263b.jpg)  
Figure 6: The distance when varying $K$ from 1 to 6.

$$
\mathcal { S } _ { i , j } = \frac { 1 } { \vert \mathbf { D } \vert } \sum _ { \mathbf { I } \in \mathbf { D } } \mathrm { K L } ( \mathcal { C } ( H _ { i } ( \mathbf { I } ) ) , \mathcal { C } ( H _ { j } ( \mathbf { I } ) ) ,
$$

$$
\mathcal { S } _ { i } = \frac { 1 } { 2 ( K - 1 ) } \sum _ { j \neq i } ^ { K } ( S _ { i , j } + S _ { j , i } ) ,
$$

where KL, D,I, $\mathcal { C }$ refer to KL divergence, dataset, the input image,and class activation maps (CAM) [28]. As illustrated in Figure 6,we compute the average distances among auxiliary heads for $K > 1$ and the distance between the DETR head and the single auxiliary head for $K = 1$ .We find the distance metric is insignificant for each auxiliary head when $K = 1$ and this observation is consistent with our results in Table 8: the DETR head can be collaboratively improved with any head when $K = 1$ .When $K$ is increased to 2, the distance metrics increase slightly and our method achieves the best performance as shown in Table 7. The distance surges when $K$ is increased from 3 and 6,indicating severe optimization conflicts among these auxiliary heads lead to a decrease in performance.However, the baseline with 6 ATSS achieves $4 9 . 5 \%$ AP and can be decreased to $4 8 . 9 \%$ AP by replacing ATSS with 6 various heads. Accordingly, we speculate too many diverse auxiliary heads,e.g.,more than 3 different heads, exacerbate the conflicts.In summary, optimization conflicts are influenced by the number of various auxiliary heads and the relations among these heads.

Should the added heads be different? Collaborative training with two ATSS heads( $( 4 9 . 2 \%$ AP） still improves the model with one ATSS head ( $4 8 . 7 \%$ AP) as ATSS is complementary to the DETR head in our analysis.Besides,introducing a diverse and complementary auxiliary head rather than the same one as the original head, e.g., Faster-RCNN, can bring better gains $( 4 9 . 5 \%$ AP). Note that this is not contradictory to above conclusion; instead, we can obtain the best performance with few different heads( $K \leq 2$ ）asthe conflicts are insignificant, but we are faced with severe conflicts when using many different heads ( $K > 3 ,$ .

Table 9: “aux head’ denotes training with an auxiliary head and “pos queries”means the customized positive queries generation.   

<table><tr><td rowspan=1 colspan=1>aux head</td><td rowspan=1 colspan=1>pos queries</td><td rowspan=1 colspan=1>#epochs</td><td rowspan=1 colspan=1>AP</td><td rowspan=1 colspan=1>AP50</td><td rowspan=1 colspan=1>AP75</td></tr><tr><td rowspan=1 colspan=1>X</td><td rowspan=1 colspan=1>X</td><td rowspan=1 colspan=1>1236</td><td rowspan=1 colspan=1>37.143.3</td><td rowspan=1 colspan=1>55.562.3</td><td rowspan=1 colspan=1>40.047.1</td></tr><tr><td rowspan=1 colspan=1>√</td><td rowspan=1 colspan=1>X</td><td rowspan=1 colspan=1>1236</td><td rowspan=1 colspan=1>41.6(+4.5)46.2(+2.9)</td><td rowspan=1 colspan=1>59.864.7</td><td rowspan=1 colspan=1>45.650.9</td></tr><tr><td rowspan=1 colspan=1>X</td><td rowspan=1 colspan=1>√</td><td rowspan=1 colspan=1>1236</td><td rowspan=1 colspan=1>40.5(+3.4)45.3(+2.0)</td><td rowspan=1 colspan=1>58.863.5</td><td rowspan=1 colspan=1>44.449.8</td></tr><tr><td rowspan=1 colspan=1>√</td><td rowspan=1 colspan=1>√</td><td rowspan=1 colspan=1>1236</td><td rowspan=1 colspan=1>42.3(+5.2)46.8(+3.5)</td><td rowspan=1 colspan=1>60.565.1</td><td rowspan=1 colspan=1>46.151.5</td></tr></table>

<table><tr><td>Method</td><td>K</td><td>#epochs</td><td>GPU hours</td><td>AP</td></tr><tr><td>Deformable-DETR</td><td>1</td><td>36</td><td>288</td><td>46.8</td></tr><tr><td>Deformable-DETR</td><td>0</td><td>50</td><td>333</td><td>44.5</td></tr><tr><td>Deformable-DETR</td><td>0</td><td>100</td><td>667</td><td>46.0</td></tr><tr><td>Deformable-DETR</td><td>0</td><td>150</td><td>1000</td><td>45.9</td></tr></table>

Table 10: Comparison to baselines with longer schedule.

The effect of each component. We perform a componentwise ablation to thoroughly analyze the effect of each component in Table 9. Incorporating the auxiliary head yields significant gains since the dense spatial supervision enables the encoder features more discriminative. Alternatively, introducing customized positive queries also contributes remarkably to the final results,while improving the training efficiency of the one-to-one set matching. Both techniques can accelerate convergence and improve performance.In summary,we observe the overall improvements stem from more discriminative features for the encoder and more efficient attention learning for the decoder.

Comparisons to the longer training schedule. As presented in Table 1O,we find Deformable-DETR can not benefit from longer training as the performance saturates. On the contrary, $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR greatly accelerates the convergence as well as increasing the peak performance.

Performance of auxiliary branches. Surprisingly, we observe $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR also brings consistent gains for auxiliary heads in Table 11. This implies our training paradigm contributes to more discriminative encoder representations, which improves the performances of both decoder and auxiliary heads.

Difference in distribution of original and customized positive queries. We visualize the positions of original positive queries and customized positive queries in Figure 7a. We only show one object (green box) per image. Positive queries assigned by Hungarian Matching in the decoder are marked in red. We mark positive queries extracted from

Table 11: Collaborative training consistently improves performances of all branches on Deformable-DETR $^ { + + }$ with ResNet-50.   

<table><tr><td>Branch</td><td>NMS</td><td>K=0</td><td>K=1</td><td>K=2</td></tr><tr><td>Deformable-DETR++</td><td>X</td><td>47.1</td><td>48.7(+1.6)</td><td>49.5(+2.4)</td></tr><tr><td>ATSS</td><td>√</td><td>46.8</td><td>47.4(+0.6)</td><td>48.0(+1.2)</td></tr><tr><td>Faster-RCNN</td><td>√</td><td>45.9</td><td>-</td><td>46.7(+0.8)</td></tr></table>

![](Images_KU3GJDWR/1962cc1ab0a0a9f4b28f28560e278db3cd8143e4e8c71b496d52ffd4824ff32b.jpg)  
Figure 7: Distribution of original and customized queries.

Faster-RCNN and ATSS in blue and orange, respectively. These customized queries are distributed around the center region of the instance and provide sufficient supervision signals for the detector.

Does distribution difference lead to instability? We compute the average distance between original and customized queries in Figure 7b. The average distance between original negative queries and customized positive queries is significantly larger than the distance between original and customized positive queries.As this distribution gap between original and customized queries is marginal, there is no instability encountered during training.

# 5. Conclusions

In this paper, we present a novel collaborative hybrid assignments training scheme,namely $\scriptstyle { \mathcal { C } } _ { 0 }$ -DETR,to learn more efficient and effective DETR-based detectors from versatile label assignment manners.This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments.In addition,we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in decoder. Extensive experiments on COCO dataset demonstrate the effciency and effectiveness of $\scriptstyle { \mathcal { C } } _ { 0 } .$ DETR. Surprisingly, incorporated with ViT-L backbone, we achieve $6 6 . 0 \%$ AP on COCO test-dev and $6 7 . 9 \%$ AP on LVIS val,establishing the new state-of-the-art detector with much fewer model sizes.

# References

[1] Nicolas Carion,Francisco Massa,Gabriel Synnaeve,Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers.ArXiv, abs/2005.12872,2020.   
[2] Fangyi Chen,Han Zhang,Kai Hu, Yu-kai Huang, Chenchen Zhu, and Marios Savvides.Enhanced training of querybased object detection via selective query recollection. arXiv preprint arXiv:2212.07593,2022.   
[3] Kai Chen,Jiangmiao Pang, Jiaqi Wang,Yu Xiong, Xiaoxiao Li,Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi,Wanli Ouyang,et al.Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974- 4983,2019.   
[4] Kai Chen,Jiaqi Wang,Jiangmiao Pang,Yuhang Cao,Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155,2019.   
[5] Qiang Chen,Xiaokang Chen, Gang Zeng，and Jingdong Wang.Group detr: Fast training convergence with decoupled one-to-many label assignment.arXiv preprint arXiv:2207.13085,2022.   
[6] Qiang Chen, Jian Wang,Chuchu Han, Shan Zhang, Zexian Li, Xiaokang Chen, Jiahui Chen, Xiaodi Wang, Shuming Han, Gang Zhang, et al. Group detr v2: Strong object detector with encoder-decoder pretraining. arXiv preprint arXiv:2211.03594,2022.   
[7] Alexey Dosovitskiy,Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas_ Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.ArXiv,abs/2010.11929,2021.   
[8] Yuxin Fang, Quan Sun, Xinggang Wang,Tiejun Huang, Xinlong Wang,and Yue Cao. Eva-O2: A visual representation for neon genesis.arXiv preprint arXiv:2303.11331,2023.   
[9] Ziteng Gao,Limin Wang,Bing Han，and Sheng Guo. Adamixer: A fast-converging query-based object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 5364-5373,2022.   
[10] Golnaz Ghiasi, Yin Cui, Aravind Srinivas,Rui Qian,TsungYi Lin,Ekin D Cubuk,Quoc V Le,and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2918-2928,2021.   
[11]Ross Girshick.Fast r-cnn.In Proceedings of the IEEE international conference on computer vision, pages 1440-1448, 2015.   
[12] Agrim Gupta,Piotr Dollar,and Ross Girshick.Lvis:A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356-5364,2019.   
[13]KaimingHe,Xinlei Chen,Saining Xie,Yanghao Li,Piotr Dollar,and Ross Girshick.Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000- 16009,2022.   
[14] Kaiming He,Georgia Gkioxari, Piotr Dollar, and Ross Girshick.Mask r-cnn.In Proceedings of the IEEE international conference on computer vision,pages 2961-2969,2017.   
[15] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2O16 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778,2016.   
[16] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin,Lei Sun, Chao Zhang,and Han Hu. Detrs with hybrid matching. arXiv preprint arXiv:2207.13080,2022.   
[17] Kang Kim and Hee Seok Lee.Probabilistic anchor assignment with iou prediction for object detection.In European Conference on Computer Vision, pages 355-371. Springer, 2020.   
[18] Feng Li, Hao Zhang, Shilong Liu, Jian Guo,Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13619-13627,2022.   
[19] Xiang Li,Wenhai Wang,Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang,and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense objectdetection. Advances in Neural Information Processing Systems,33:21002-21012,2020.   
[20] Yanghao Li,Hanzi Mao,Ross Girshick,and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527,2022.   
[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick,Kaiming He,and Piotr Dollar. Focal loss for dense object detection.In Proceedings of the IEEE international conference on computer vision, pages 2980-2988,2017.   
[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona,Deva Ramanan, Piotr Dollar,and CLawrence Zitnick.Microsoft coco: Common objects in context.In European conference on computer vision,pages 740-755. Springer, 2014.   
[23] Shilong Liu,Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su,Jun Zhu,and Lei Zhang．Dab-detr: Dynamic anchor boxes are bettr queries for detr.arXiv preprint arXiv:2201.12329,2022.   
[24] Ze Liu,Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei,Jia Ning,Yue Cao,Zheng Zhang,Li Dong,etal. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12009-12019,2022.   
[25] Ze Liu, Yutong Lin, Yue Cao,Han Hu, Yixuan Wei, Zheng Zhang，S.Lin,and B.Guo. Swin transformer: Hierarchical vision transformer using shifted windows. ArXiv, abs/2103.14030,2021.   
[26] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan,Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence.In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3651-3660,2021.   
[27] Shaoqing Ren,Kaiming He,Ross Girshick,and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems,28,2015.   
[28] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam，Devi Parikh,and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618-626, 2017.   
[29] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li,and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision,pages 8430-8439,2019.   
[30] Guanglu Song，Yu Liu,and Xiaogang Wang.Revisiting the sibling head in object detector. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11563-11572, 2020.   
[31] Zhi Tian,Chunhua Shen,Hao Chen,and Tong He.Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9627-9636,2019.   
[32] Wenhui Wang,Hangbo Bao,Li Dong,Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language:Beit pretraining for all vision and visionlanguage tasks. arXiv preprint arXiv:2208.10442, 2022.   
[33] Wenhai Wang, Jifeng Dai,and Zhe Chen.Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778, 2022.   
[34] Yingming Wang, Xiangyu Zhang,Tong Yang,and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the AAAI conference on artificial intelligence,volume 36, pages 2567-2575,2022.   
[35] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao,Dong Chen,and Baining Guo． Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.   
[36] Zeyue Xue, Jianming Liang, Guanglu Song, Zhuofan Zong, Liang Chen, Yu Liu,and Ping Luo.Large-batch optimization for dense visual predictions.In Advances in Neural Information Processing Systems,2022.   
[37] Jianwei Yang, Chunyuan Li,and Jianfeng Gao. Focal modulation networks. arXiv preprint arXiv:2203.11926,2022.   
[38] Hao Zhang,Feng Li, Shilong Liu,Lei Zhang,Hang Su, Jun Zhu,Lionel M Ni,and Heung-Yeung Shum. Dino:Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605,2022.   
[39] Haotian Zhang,Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen,Liunian Li, Xiyang Dai,Lijuan Wang,Lu Yuan, JenqNeng Hwang,and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. Advances in Neural Information Processing Systems,35:36067-36080,2022.   
[40] Shifeng Zhang,Cheng Chi, Yongqiang Yao, Zhen Lei,and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759-9768, 2020.   
[41] Xingyi Zhou,Vladlen Koltun，and Philipp Krähenbuhl. Probabilistictwo-stagedetection. arXivpreprint arXiv:2103.07461,2021.   
[42] Xizhou Zhu,Weijie Su, Lewei Lu,Bin Li, Xiaogang Wang,and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection.arXiv preprint arXiv:2010.04159,2020.   
[43] Zhuofan Zong, Qianggang Cao,and Biao Leng.Rcnet: Reverse feature pyramid and cross-scale shift network for object detection.In Proceedings of the 29th ACMInternational Conference on Multimedia,pages 5637-5645,2021.