# DETRs Beat YOLOs on Real-time Object Detection

Yian Zhao $^ { 1 , 2 }$ 十 Wenyu LvlttShangliang $\mathrm { { X u ^ { 1 } } }$ Jinman Weil Guanzhong Wang1 Qingqing Dang1YiLiulJie Chen²区

1Baidu Inc,Beijing, China²School of Electronic and Computer Enginering,Peking University, Shenzhen, China

zhaoyian@stu.pku.edu.cnlvwenyu01@baidu.comjiechen2019@pku.edu.cn

# Abstract

The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS.Recently, end-to-end Transformer-based detectors(DETRs) have provided an alternative to eliminating NMS.Nevertheless,the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy.In addition,RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 /R101 achieves $5 3 . 1 \% / 5 4 . 3 \% A F$ Pon COCO and 108 /74 FPS on T4 GPU,outperforming previously advanced YOLOs in both speed and accuracy. Furthermore,RT-DETR-R50 outperforms DINO-R50 by $2 . 2 \%$ AP in accuracy and about 21 times in FPS.After pre-training with Objects365,RTDETR-R50 /R101 achieves $5 5 . 3 \% / 5 6 . 2 \% A$ P. The project page: https://zhao-yian.github.io/RTDETR.

# 1. Introduction

Real-time object detection is an important area of research and has a wide range of applications, such as object tracking [4O],video surveillance [26],and autonomous driving [2], etc. Existing real-time detectors generally adopt the CNN-based architecture, the most famous of which is the YOLO detectors [1, 9-11,14, 15,24,28,35,37] due to their reasonable trade-off between speed and accuracy. However, these detectors typically require Non-Maximum Suppression (NMS) for post-processing,which not only slows down the inference speed but also introduces hyperparameters that cause instability in both the speed and accuracy. Moreover, considering that different scenarios place different emphasis on recall and accuracy, it is necessary to carefully select the appropriate NMS thresholds,which hinders the development of real-time detectors.

![](Images_4GZM2DR5/44b1333dcccd6b84f5b2e8035b9a9df3a7c8196efc79f148547db0353108f0b1.jpg)  
Figure 1. Compared to previously advanced real-time object detectors,our RT-DETR achieves state-of-the-art performance.

Recently， the end-to-end Transformer-based detectors (DETRs) [4, 16,22,25,33,36, 41, 42] have received extensive attention from the academia due to their streamlined architecture and elimination of hand-crafted components. However, their high computational cost prevents them from meeting real-time detection requirements,so the NMS-free architecture does not demonstrate an inference speed advantage.This inspires us to explore whether DETRs can be extended to real-time scenarios and outperform the advanced YOLO detectors in both speed and accuracy, eliminating the delay caused by NMS for real-time object detection.

To achieve the above goal, we rethink DETRs and conduct detailed analysis of key components to reduce unnecessary computational redundancy and further improve accuracy. For the former, we observe that although the introduction of multi-scale features is beneficial in accelerating the training convergence [42], it leads to a significant increase in the length of the sequence feed into the encoder. The high computational cost caused by the interaction of multi-scale features makes the Transformer encoder the computational bottleneck. Therefore, implementing the real-time DETR requires a redesign of the encoder. And for the latter, previous works [39,41, 42] show that the hard-to-optimize object queries hinder the performance of DETRs and propose the query selection schemes to replace the vanilla learnable embeddings with encoder features.However,we observe that the current query selection directly adopt classification scores for selection,ignoring the fact that the detector are required to simultaneously model the category and location of objects,both of which determine the quality of the features.This inevitably results in encoder features with low localization confidence being selected as initial queries, thus leading to a considerable level of uncertainty and hurting the performance of DETRs.We view query initialization as a breakthrough to further improve performance.

In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. To expeditiously process multi-scale features,we design an efficient hybrid encoder to replace the vanilla Transformer encoder, which significantly improves inference speed by decoupling the intra-scale interaction and cross-scale fusion of features with different scales. To avoid encoder features with low localization confidence being selected as object queries,we propose the uncertainty-minimal query selection,which provides highquality initial queries to the decoder by explicitly optimizing the uncertainty, thereby increasing the accuracy. Furthermore,RT-DETR supports flexible speed tuning to accommodate various real-time scenarios without retraining, thanks to the multi-layer decoder architecture of DETR.

RT-DETR achieves an ideal trade-off between the speed and accuracy. Specifically,RT-DETR-R5O achieves $5 3 . 1 \%$ AP on COCO val2 017 and 108 FPS on T4 GPU, while RTDETR-R101 achieves $5 4 . 3 \%$ AP and 74 FPS, outperforming $L$ and $X$ models of previously advanced YOLO detectors in both speed and accuracy,Figure 1. We also develop scaled RT-DETRs by scaling the encoder and decoder with smaller backbones,which outperform the lighter YOLO detectors ( $S$ and $M$ models). Furthermore,RT-DETR-R50 outperforms DINO-Deformable-DETR-R50 by $2 . 2 \%$ AP $( 5 3 . 1 \%$ AP vs $5 0 . 9 \%$ AP) in accuracy and by about 21 times in FPS (108 FPS vs 5 FPS), significantly improves accuracy and speed of DETRs. After pre-training with Objects365 [32], RTDETR-R50 /R101 achieves $5 5 . 3 \% / 5 6 . 2 \%$ AP, resulting in surprising performance improvements.More experimental results are provided in the Appendix.

The main contributions are summarized as: (i). We propose the first real-time end-to-end object detector called RTDETR,which not only outperforms the previously advanced YOLO detectors in both speed and accuracy but also eliminates the negative impact caused by NMS post-processing on real-time object detection; (ii).We quantitatively analyze the impact of NMS on the speed and accuracy of YOLO detectors,and establish an end-to-end speed benchmark to test the end-to-end inference speed of real-time detectors; (iii). The proposed RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to accommodate various scenarios without retraining.

# 2. Related Work

# 2.1. Real-time Object Detectors

YOLOv1 [29] is the first CNN-based one-stage object detector to achieve true real-time object detection. Through years of continuous development, the YOLO detectors have outperformed other one-stage object detectors [2O,23] and become the synonymous with the real-time object detector. YOLO detectors can be classified into two categories: anchor-based [1,10,14,24,27,28,34,35] and anchorfree [9,11,15,37], which achieve a reasonable trade-off between speed and accuracy and are widely used in various practical scenarios.These advanced real-time detectors produce numerous overlapping boxes and require NMS postprocessing,which slows down their speed.

# 2.2. End-to-end Object Detectors

End-to-end object detectors are well-known for their streamlined pipelines. Carion et al.[4] first propose the end-toend detector based on Transformer called DETR,which has attracted extensive attention due to its distinctive features. Particularly,DETR eliminates the hand-crafted anchor and NMS components. Instead, it employs bipartite matching and directly predicts the one-to-one object set. Despite its obvious advantages,DETR suffers from several problems: slow training convergence, high computational cost, and hard-to-optimize queries.Many DETR variants have been proposed to address these issues. Accelerating convergence. Deformable-DETR [42] accelerates training convergence with multi-scale features by enhancing the efficiency of the attention mechanism.DAB-DETR[22] and DN-DETR[16] further improve performance by introducing the iterative refinement scheme and denoising training. Group-DETR [5] introduces group-wise one-to-many assignment. Reducing computational cost. Efficient DETR [39] and Sparse DETR [31] reduce the computational cost by reducing the number of encoder and decoder layers or the number of updated queries.Lite DETR[17] enhances the efficiency of encoder by reducing the update frequency of low-level features in an interleaved way. Optimizing query initialization. Conditional DETR [25] and Anchor DETR [36] decrease the optimization difficulty of the queries.Zhu et al.[42] propose the query selection for two-stage DETR,and DINO [41] suggests the mixed query selection to help beter initialize queries. Current DETRs are still computationally intensive and are not designed to detect in real time.Our RT-DETR vigorously explores computational cost reduction and attempts to optimize query initialization, outperforming state-of-the-art real-time detectors.

![](Images_4GZM2DR5/67131013f416008916be05d2211db376ffacc44430805f46a093e3fbd4d3055c.jpg)  
Figure 2. The number of boxes at different confidence thresholds.

# 3. End-to-end Speed of Detectors

# 3.1. Analysis of NMS

NMS is a widely used post-processing algorithm in object detection, employed to eliminate overlapping output boxes. Two thresholds are required in NMS: confidence threshold and IoU threshold. Specifically, the boxes with scores below the confidence threshold are directly filtered out, and whenever the IoU of any two boxes exceeds the IoU threshold, the box with the lower score will be discarded. This process is performed iteratively until all boxes of every category have been processed. Thus, the execution time of NMS primarily depends on the number of boxes and two thresholds.To verify this observation,we leverage YOLOv5 [10] (anchor-based) and YOLOv8 [11] (anchor-free) for analysis.

We first count the number of boxes remaining after filtering the output boxes with different confidence thresholds on the same input. We sample values from O.OO1 to 0.25 as confidence thresholds to count the number of remaining boxes of the two detectors and plot them on a bar graph, which intuitively reflects that NMS is sensitive to its hyperparameters,Figure 2.As the confidence threshold increases, more prediction boxes are fltered out,and the number of remaining boxes that need to calculate IoU decreases,thus reducing the execution time of NMS.

Furthermore, we use YOLOv8 to evaluate the accuracy on the COCO val2O17 and test the execution time of the NMS operation under different hyperparameters.Note that the NMS operation we adopt refers to the TensorRT efficientNMSPlugin，which involves multiple kernels,including EfficientNMSFilter,RadixSort, EfficientNMS,etc.,and we only report the execution time of the EfficientNMS kernel. We test the speed on T4 GPU with TensorRT FP16,and the input and preprocessing remain consistent. The hyperparameters and the corresponding results are shown in Table 1． From the results,we can conclude that the execution time of the Ef ficient NMS kernel increases as the confidence threshold decreases or the IoU threshold increases.The reason is that the high confidence threshold directly filters out more prediction boxes,whereas the high IoU threshold filters out fewer prediction boxes in each round of screening.We also visualize the predictions of YOLOv8 with different NMS thresholds in Appendix. The results show that inappropriate confidence thresholds lead to significant false positives or false negatives by the detector. With a confidence threshold of 0.001 and an IoU threshold of O.7,YOLOv8 achieves the best AP results,but the corresponding NMS time is at a higher level. Considering that YOLO detectors typically report the model speed and exclude the NMS time, thus an end-to-end speed benchmark needs to be established.

Table 1.The effect of IoU threshold and confidence threshold on accuracy and NMS execution time.   

<table><tr><td>IoU thr. (Conf=0.001)</td><td>AP (%)</td><td>NMS (ms)</td><td>Conf thr. (IoU=0.7)</td><td>AP (%)</td><td>NMS (ms)</td></tr><tr><td>0.5</td><td>52.1</td><td>2.24</td><td>0.001</td><td>52.9</td><td>2.36</td></tr><tr><td>0.6</td><td>52.6</td><td>2.29</td><td>0.01</td><td>52.4</td><td>1.73</td></tr><tr><td>0.8</td><td>52.8</td><td>2.46</td><td>0.05</td><td>51.2</td><td>1.06</td></tr></table>

# 3.2.End-to-end Speed Benchmark

To enable a fair comparison of the end-to-end speed of various real-time detectors,we establish an end-to-end speed benchmark. Considering that the execution time of NMS is influenced by the input,it is necessary to choose a benchmark dataset and calculate the average execution time across multiple images.We choose COCO val2 O17 [19] as the benchmark dataset and append the NMS post-processing plugin of TensorRT for YOLO detectors as mentioned above. Specifically,we test the average inference time of the detector according to the NMS thresholds of the corresponding accuracy taken on the benchmark dataset,excluding $\ I / { \cal O }$ and Memo ryCopy operations. We utilize the benchmark to test the end-to-end speed of anchor-based detectors YOLOv5 [1O] and YOLOv7 [35],as well as anchor-free detectors PP-YOLOE [37], YOLOv6 [15] and YOLOv8 [11] on T4 GPU with TensorRT FP16. According to the results (cf. Table 2),we conclude that anchor-free detectors outperform anchor-based detectors with equivalent accuracy for YOLO detectors because the former require less NMS time than the latter.The reason is that anchor-based detectors produce more prediction boxes than anchor-free detectors (three times more in our tested detectors).

![](Images_4GZM2DR5/5d16201449dccb9255951f83a3494ebe490b3698113b7e8b590fd2904cddfff3.jpg)  
Figure3.Theencoderstructure foreachvariant.SSErepresents thesingle-scaleTransformerencoder,MSErepresentste multi-scale Transformerencoder,and CSFrepresents cross-scalefusion. AIFIand CCFFare the two modules designedintoour hybridencoder.

# 4. TheReal-timeDETR

# 4.1. Model Overview

RT-DETR consists of a backbone,an eficient hybrid encoder, and a Transformer decoder with auxiliary prediction heads. The overview of RT-DETR is illustrated in Figure 4. Specifically, we feed the features from the last three stages of the backbone $\{ \pmb { S } _ { 3 } , \pmb { S } _ { 4 } , \pmb { S } _ { 5 } \}$ into the encoder. The efficient hybrid encoder transforms multi-scale features into a sequence of image features through intra-scale feature interaction and cross-scale feature fusion (cf. Sec.4.2). Subsequently, the uncertainty-minimal query selection is employed to select a fixed number of encoder features to serve as initial object queries for the decoder (cf. Sec.4.3). Finally, the decoder with auxiliary prediction heads iteratively optimizes object queries to generate categories and boxes.

# 4.2.Efficient Hybrid Encoder

Computational bottleneck analysis.The introduction of multi-scale features accelerates training convergence and improves performance [42]. However, although the deformable attention reduces the computational cost, the sharply increased sequence length still causes the encoder to become the computational bottleneck.As reported in Lin et al.[18], the encoder accounts for $4 9 \%$ of the GFLOPs but contributes only $1 1 \%$ of the AP in Deformable-DETR.To overcome this bottleneck, we first analyze the computational redundancy present in the multi-scale Transformer encoder. Intuitively, high-level features that contain rich semantic information about objects are extracted from low-level features,making it redundant to perform feature interaction on the concatenated multi-scale features.Therefore,we design a set of variants with different types of the encoder to prove that the simultaneous intra-scale and cross-scale feature interaction is inefficient,Figure 3.Specially,we use DINO-Deformable-R50 with the smaller size data reader and lighter decoder used in RT-DETR for experiments and first remove the multi-scale Transformer encoder in DINO-Deformable-R5O as variant A. Then,different types of the encoder are inserted to produce a series of variants based on A, elaborated as follows (Detailed indicators of each variant are referred to in Table 3):

· $\mathbf A \to \mathbf B$ : Variant B inserts a single-scale Transformer encoder into A,which uses one layer of Transformer block. The multi-scale features share the encoder for intra-scale feature interaction and then concatenate as output.   
· $\mathbf B \to \mathbf C$ :Variant C introduces cross-scale feature fusion based on B and feeds the concatenated features into the multi-scale Transformer encoder to perform simultaneous intra-scale and cross-scale feature interaction.   
· $\mathbf { C }  \mathbf { D }$ : Variant D decouples intra-scale interaction and cross-scale fusion byutilizing the single-scale Transformer encoder for the former and a PANet-style [21] structure for the latter.   
· $\mathrm { D }  \mathrm { E }$ : Variant E enhances the intra-scale interaction and cross-scale fusion based on D,adopting an efficient hybrid encoder designed by us.

Hybrid design.Based on the above analysis,we rethink the structure of the encoder and propose an efficient hybrid encoder,consisting of two modules,namely the Attentionbased Intra-scale Feature Interaction (AIFI) and the CNNbased Cross-scale Feature Fusion (CCFF). Specifically, AIFI further reduces the computational cost based on variant D by performing the intra-scale interaction only on $\pmb { S } _ { 5 }$ with the single-scale Transformer encoder. The reason is that applying the self-attention operation to high-level features with richer semantic concepts captures the connection between conceptual entities,which facilitates the localization and recognition of objects by subsequent modules.However, the intra-scale interactions of lower-level features are unnecessary due to the lack of semantic concepts and the risk of duplication and confusion with high-level feature interactions.To verify this opinion,we perform the intra-scale interaction only on $\pmb { S } _ { 5 }$ in variant $\mathbf { D }$ ,and the experimental results are reported in Table 3 (see row $\mathrm { D } _ { { \pmb S } _ { 5 } }$ ). Compared to

![](Images_4GZM2DR5/830c3d8be34234e30d2e038a3e7071013b5eefa9e5f516a29d2180cafc1c897c.jpg)  
Figure4.OverviewofRT-DETR.Wefeedthefeaturesfromthelasttreestagesof thebackboneintotheencoder.Theeffcienthybrid encodertransformsmulti-scalefeaturesitoasequeneofimagefaturesthrough theAtentionbasedIntra-scaleFeatureInteractio(F) andtheCNN-basedCrossaleFeatureFusion (CCF).Then,teuncertainty-miimalqueryselectionselectsafxednumberofencoder features toserveasinitalojectqueriesfortedecoder.Finalltedecoderwithxiliarypredictionadsiterativelyoptisobject queries to generate categories and boxes.

![](Images_4GZM2DR5/97a004bc757dcb803a13580e26d4738a8949a350c288a1c26415868f7d430eab.jpg)  
Figure 5.The fusion block in CCFF.

D, $\mathrm { D } _ { { \pmb S } _ { 5 } }$ not only significantly reduces latency ( $3 5 \%$ faster), but also improves accuracy ( $0 . 4 \%$ AP higher). CCFF is optimized based on the cross-scale fusion module,which inserts several fusion blocks consisting of convolutional layers into the fusion path.The role of the fusion block is to fuse two adjacent scale features into a new feature,and its structure is illustrated in Figure 5. The fusion block contains two $1 \times 1$ convolutions to adjust the number of channels, $N$ RepBlocks composed of RepConv [7] are used for feature fusion,and the two-path outputs are fused by element-wise add.We formulate the calculation of the hybrid encoder as:

$$
\begin{array} { r l } & { \pmb { \mathcal { Q } } = \pmb { \mathcal { K } } = \pmb { \mathcal { V } } = \mathtt { F l a t t e n } ( \pmb { \mathcal { S } } _ { 5 } ) , } \\ & { \pmb { \mathcal { F } } _ { 5 } = \mathtt { R e s h a p e } \left( \mathtt { A I F I } ( \pmb { \mathcal { Q } } , \pmb { \mathcal { K } } , \pmb { \nu } ) \right) , } \\ & { \pmb { \mathcal { O } } = \mathrm { C C F F } ( \{ \pmb { \mathcal { S } } _ { 3 } , \pmb { \mathcal { S } } _ { 4 } , \pmb { \mathcal { F } } _ { 5 } \} ) , } \end{array}
$$

where Re shape represents restoring the shape of the flattened feature to the same shape as $\pmb { S } _ { 5 }$ .

# 4.3. Uncertainty-minimal Query Selection

To reduce the difficulty of optimizing object queries in DETR, several subsequent works [39, 41, 42] propose query selection schemes,which have in common that they use the confidence score to select the top $K$ features from the encoder to initialize object queries (or just position queries).

The confidence score represents the likelihood that the feature includes foreground objects.Nevertheless, the detector are required to simultaneously model the category and location of objects,both of which determine the quality of the features.Hence,the performance score of the feature is a latent variable that is jointly correlated with both classification and localization. Based on the analysis, the current query selection lead to a considerable level of uncertainty in the selected features,resulting in sub-optimal initialization for the decoder and hindering the performance of the detector.

To address this problem, we propose the uncertainty minimal query selection scheme,which explicitly constructs and optimizes the epistemic uncertainty to model the joint latent variable of encoder features, thereby providing high-quality queries for the decoder. Specifically, the feature uncertainty $\mathcal { U }$ is defined as the discrepancy between the predicted distributions of localization $\mathcal { P }$ and classification $\mathcal { C }$ in Eq. (2). To minimize the uncertainty of the queries,we integrate the uncertainty into the loss function for the gradient-based optimization in Eq. (3).

$$
\mathcal { U } ( \hat { \pmb { \mathscr { X } } } ) = \| \mathcal { P } ( \hat { \pmb { \mathscr { X } } } ) - \mathcal { C } ( \hat { \pmb { \mathscr { X } } } ) \| , \hat { \pmb { \mathscr { X } } } \in \mathbb { R } ^ { D }
$$

$$
\mathcal { L } ( \hat { \pmb { x } } , \hat { \pmb { y } } , \pmb { y } ) = \mathcal { L } _ { b o x } ( \hat { \bf b } , \mathbf { b } ) + \mathcal { L } _ { c l s } ( \mathcal { U } ( \hat { \pmb { x } } ) , \hat { \mathbf { c } } , \mathbf { c } )
$$

where $\hat { y }$ and $_ { \mathscr { y } }$ denote the prediction and ground truth, $\hat { \mathcal { \mathbf { y } } } = \{ \hat { \mathbf { c } } , \hat { \mathbf { b } } \}$ ,c and $\hat { \textbf { b } }$ represent the category and bounding box respectively, $\hat { x }$ represent the encoder feature.

Effectiveness analysis.To analyze the effectiveness of the uncertainty-minimal query selection,we visualize the classification scores and IoU scores of the selected features on COCO val2 O17, Figure 6. We draw the scatterplot with classification scores greater than O.5. The purple and green dots represent the selected features from the model trained with uncertainty-minimal query selection and vanilla query selection,respectively. The closer the dot is to the top right of the figure,the higher the quality of the corresponding feature,i.e., the more likely the predicted category and box are to describe the true object. The top and right density curves reflect the number of dots for two types.

![](Images_4GZM2DR5/f29111ed6580146a1deedfbfee6495c763a6a7939e0863ee97a011ce70667914.jpg)  
Figure 6.Classification and IoU scores of the selected encoder features.Purple and Green dots represent the selected features from model trained with uncertainty-minimal query selection and vanilla query selection,respectively.

The most striking feature of the scatterplot is that the purple dots are concentrated in the top right of the figure,while the green dots are concentrated in the bottom right. This shows that uncertainty-minimal query selection produces more high-quality encoder features.Furthermore,we perform quantitative analysis on two query selection schemes. There are $1 3 8 \%$ more purple dots than green dots,i.e., more green dots with a classification score less than or equal to 0.5,which can be considered low-quality features.And there are $1 2 0 \%$ more purple dots than green dots with both scores greater than O.5.The same conclusion can be drawn from the density curves,where the gap between purple and green is most evident in the top right of the figure. Quantitative results further demonstrate that the uncertainty-minimal query selection provides more features with accurate classification and precise location for queries,thereby improving the accuracy of the detector (cf. Sec.5.3).

# 4.4. Scaled RT-DETR

Since real-time detectors typically provide models at different scales to accommodate different scenarios,RT-DETR also supports flexible scaling. Specifically, for the hybrid encoder, we control the width by adjusting the embedding dimension and the number of channels,and the depth by adjusting the number of Transformer layers and RepBlocks.

The width and depth of the decoder can be controlled by manipulating the number of object queries and decoder layers.Furthermore, the speed of RT-DETR supports flexible adjustment by adjusting the number of decoder layers. We observe that removing a few decoder layers at the end has minimal effect on accuracy, but greatly enhances inference speed (cf. Sec. 5.4). We compare the RT-DETR equipped with ResNet50 and ResNet101 [12,13] to the $L$ and $X$ models of YOLO detectors.Lighter RT-DETRs can be designed by applying other smaller (e.g.,ResNet18/34) or scalable (e.g., CSPResNet [37]) backbones with scaled encoder and decoder. We compare the scaled RT-DETRs with the lighter ( $S$ and $M$ ） YOLO detectors in Appendix,which outperform all $S$ and $M$ models in both speed and accuracy.

# 5. Experiments

# 5.1. Comparison with SOTA

Table 2 compares RT-DETR with current real-time (YOLOs) and end-to-end (DETRs) detectors,where only the $L$ and $X$ models of the YOLO detector are compared,and the $S$ and $M$ models are compared in Appendix. Our RT-DETR and YOLO detectors share a common input size of (640, 640),and other DETRs use an input size of (800,1333). The FPS is reported on T4 GPU with TensorRT FP16,and for YOLO detectors using oficial pre-trained models according to the end-to-end speed benchmark proposed in Sec.3.2. Our RT-DETR-R50 achieves $5 3 . 1 \%$ AP and 1O8 FPS,while RTDETR-R101 achieves $5 4 . 3 \%$ AP and 74 FPS,outperforming state-of-the-art YOLO detectors of similar scale and DETRs with the same backbone in both speed and accuracy. The experimental settings are shown in Appendix.

Comparison with real-time detectors. We compare the end-to-end speed (cf. Sec. 3.2) and accuracy of RTDETR with YOLO detectors.We compare RT-DETR with YOLOv5 [10], PP-YOLOE [37], YOLOv6v3.0 [15] (hereinafter referred to as YOLOv6)，YOLOv7 [35] and YOLOv8 [11]. Compared to YOLOv5-L/PP-YOLOE-L / YOLOv6-L,RT-DETR-R50 improves accuracy by $4 . 1 \% /$ $1 . 7 \% / 0 . 3 \%$ AP, increases FPS by $1 0 0 . 0 \% / 1 4 . 9 \% / 9 . 1 \%$ ， and reduces the number of parameters by $8 . 7 \% / \ 1 9 . 2 \%$ $\ : I \ : 2 8 . 8 \%$ Compared to YOLOv5-X / PP-YOLOE-X, RTDETR-R1O1 improves accuracy by $3 . 6 \% / 2 . 0 \%$ ,increases FPS by $7 2 . 1 \% / \ 2 3 . 3 \%$ ，and reduces the number of parameters by $1 1 . 6 \% / 2 2 . 4 \%$ . Compared to $\mathrm { Y O L O v } 7 { \mathrm { - } } \mathrm { L } \ /$ YOLOv8-L, RT-DETR-R50 improves accuracy by $1 . 9 \% $ $0 . 2 \%$ AP and increases FPS by $9 6 . 4 \% / 5 2 . 1 \%$ . Compared to YOLOv7-X / YOLOv8-X, RT-DETR-R101 improves accuracy by $1 . 4 \% / 0 . 4 \%$ AP and increases FPS by $6 4 . 4 \% /$ $4 8 . 0 \%$ .This shows that our RT-DETR achieves state-of-theart real-time detection performance.

Comparison with end-to-end detectors. We also compare RT-DETR with existing DETRs using the same backbone.

<table><tr><td>Model</td><td>Backbone</td><td></td><td>#Epochs #Params (M)GFLOPs</td><td></td><td>FPSbs=1</td><td>APual</td><td>AP</td><td>APg</td><td>APal</td><td>APa</td><td>APal</td></tr><tr><td colspan="10"> Real-time Object Detectors</td></tr><tr><td>YOLOv5-L[10]</td><td></td><td>300</td><td>46</td><td>109</td><td>54</td><td>49.0</td><td>67.3</td><td>-</td><td>-</td><td>■</td><td>-</td></tr><tr><td>YOLOv5-X[10]</td><td></td><td>300</td><td>86</td><td>205</td><td>43</td><td>50.7</td><td>68.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PPYOLOE-L [37]</td><td>■</td><td>300</td><td>52</td><td>110</td><td>94</td><td>51.4</td><td>68.9</td><td>55.6</td><td>31.4</td><td>55.3</td><td>66.1</td></tr><tr><td>PPYOLOE-X [37]</td><td></td><td>300</td><td>98</td><td>206</td><td>60</td><td>52.3</td><td>69.9</td><td>56.5</td><td>33.3</td><td>56.3</td><td>66.4</td></tr><tr><td>YOLOv6-L [15]</td><td>=</td><td>300</td><td>59</td><td>150</td><td>99</td><td>52.8</td><td>70.3</td><td>57.7</td><td>34.4</td><td>58.1</td><td>70.1</td></tr><tr><td>YOLOv7-L [35]</td><td></td><td>300</td><td>36</td><td>104</td><td>55</td><td>51.2</td><td>69.7</td><td>55.5</td><td>35.2</td><td>55.9</td><td>66.7</td></tr><tr><td>YOLOv7-X [35]</td><td></td><td>300</td><td>71</td><td>189</td><td>45</td><td>52.9</td><td>71.1</td><td>57.4</td><td>36.9</td><td>57.7</td><td>68.6</td></tr><tr><td>YOLOv8-L[11]</td><td></td><td>-</td><td>43</td><td>165</td><td>71</td><td>52.9</td><td>69.8</td><td>57.5</td><td>35.3</td><td>58.3</td><td>69.8</td></tr><tr><td>YOLOv8-X[11]</td><td></td><td>-</td><td>68</td><td>257</td><td>50</td><td>53.9</td><td>71.0</td><td>58.7</td><td>35.7</td><td>59.3</td><td>70.7</td></tr><tr><td colspan="10">End-to-end Object Detectors</td></tr><tr><td>DETR-DC5 [4]</td><td>R50</td><td>500</td><td>41</td><td>187</td><td>-</td><td>43.3</td><td>63.1</td><td>45.9</td><td>22.5</td><td>47.3</td><td>61.1</td></tr><tr><td>DETR-DC5 [4]</td><td>R101</td><td>500</td><td>60</td><td>253</td><td>■</td><td>44.9</td><td>64.7</td><td>47.7</td><td>23.7</td><td>49.5</td><td>62.3</td></tr><tr><td>Anchor-DETR-DC5 [36]</td><td>R50</td><td>50</td><td>39</td><td>172</td><td></td><td>44.2</td><td>64.7</td><td>47.5</td><td>24.7</td><td>48.2</td><td>60.6</td></tr><tr><td>Anchor-DETR-DC5 [36]</td><td>R101</td><td>50</td><td>-</td><td>-</td><td></td><td>45.1</td><td>65.7</td><td>48.8</td><td>25.8</td><td>49.4</td><td>61.6</td></tr><tr><td>Conditional-DETR-DC5 [25]</td><td>R50</td><td>108</td><td>44</td><td>195</td><td></td><td>45.1</td><td>65.4</td><td>48.5</td><td>25.3</td><td>49.0</td><td>62.2</td></tr><tr><td>Conditional-DETR-DC5[25]</td><td>R101</td><td>108</td><td>63</td><td>262</td><td></td><td>45.9</td><td>66.8</td><td>49.5</td><td>27.2</td><td>50.3</td><td>63.3</td></tr><tr><td>Efficient-DETR [39]</td><td>R50</td><td>36</td><td>35</td><td>210</td><td></td><td>45.1</td><td>63.1</td><td>49.1</td><td>28.3</td><td>48.4</td><td>59.0</td></tr><tr><td>Efficient-DETR [39]</td><td>R101</td><td>36</td><td>54</td><td>289</td><td></td><td>45.7</td><td>64.1</td><td>49.5</td><td>28.2</td><td>49.1</td><td>60.2</td></tr><tr><td>SMCA-DETR [8]</td><td>R50</td><td>108</td><td>40</td><td>152</td><td></td><td>45.6</td><td>65.5</td><td>49.1</td><td>25.9</td><td>49.3</td><td>62.6</td></tr><tr><td>SMCA-DETR [8]</td><td>R101</td><td>108</td><td>58</td><td>218</td><td></td><td>46.3</td><td>66.6</td><td>50.2</td><td>27.2</td><td>50.5</td><td>63.2</td></tr><tr><td>Deformable-DETR [42]</td><td>R50</td><td>50</td><td>40</td><td>173</td><td></td><td>46.2</td><td>65.2</td><td>50.0</td><td>28.8</td><td>49.2</td><td>61.7</td></tr><tr><td>DAB-Deformable-DETR [22]</td><td>R50</td><td>50</td><td>48</td><td>195</td><td></td><td>46.9</td><td>66.0</td><td>50.8</td><td>30.1</td><td>50.4</td><td>62.5</td></tr><tr><td>DAB-Deformable-DETR++ [22]</td><td>R50</td><td>50</td><td>47</td><td>-</td><td></td><td>48.7</td><td>67.2</td><td>53.0</td><td>31.4</td><td>51.6</td><td>63.9</td></tr><tr><td>DN-Deformable-DETR [16]</td><td>R50</td><td>50</td><td>48</td><td>195</td><td></td><td>48.6</td><td>67.4</td><td>52.7</td><td>31.0</td><td>52.0</td><td>63.7</td></tr><tr><td>DN-Deformable-DETR++ [16]</td><td>R50</td><td>50</td><td>47</td><td>-</td><td>■</td><td>49.5</td><td>67.6</td><td>53.8</td><td>31.3</td><td>52.6</td><td>65.4</td></tr><tr><td>DINO-Deformable-DETR [41]</td><td>R50</td><td>36</td><td>47</td><td>279</td><td>5</td><td>50.9</td><td>69.0</td><td>55.3</td><td>34.6</td><td>54.1</td><td>64.6</td></tr><tr><td colspan="10"> Real-time End-to-end Object Detector(ours)</td></tr><tr><td>RT-DETR</td><td>R50</td><td>72</td><td>42</td><td>136</td><td>108</td><td>53.1</td><td>71.3</td><td> 57.7</td><td>34.8</td><td>58.0</td><td>70.0</td></tr><tr><td>RT-DETR</td><td>R101</td><td>72</td><td>76</td><td>259</td><td>74</td><td>54.3</td><td>72.7</td><td>58.6</td><td>36.0</td><td>58.8</td><td>72.1</td></tr></table>

Table 2.Comparison with SOTA (only $L$ and $X$ models of YOLO detectors,see Appendix for the comparison with $S$ and $M$ models). We do notesttheseedofotherDETRs,exceptforINO-Deformable-DETR[41]forcomparison,astheyarenotreal-tiedtector.Our RT-DETR outperforms the state-of-the-art YOLO detectors and DETRs in both speed and accuracy.

We test the speed of DINO-Deformable-DETR [41] according to the settings of the corresponding accuracy taken on COCO val2O17 for comparison, i.e., the speed is tested with TensorRT FP16 and the input size is (80O, 1333). Table 2 shows that RT-DETR outperforms all DETRs with the same backbone in both speed and accuracy. Compared to DINO-Deformable-DETR-R50,RT-DETR-R50 improves the accuracy by $2 . 2 \%$ AP and the speed by 21 times (108 FPS vs 5 FPS), both of which are significantly improved.

# 5.2. Ablation Study on Hybrid Encoder

We evaluate the indicators of the variants designed in Sec.4.2, including AP (trained with $1 \times$ configuration), the number of parameters,and the latency, Table 3. Compared to baseline A, variant B improves accuracy by $1 . 9 \%$ AP and increases the latency by $5 4 \%$ . This proves that the intrascale feature interaction is significant, but the single-scale Transformer encoder is computationally expensive.Variant C delivers a $0 . 7 \%$ AP improvement over B and increases the latency by $2 0 \%$ .This shows that the cross-scale feature fusion is also necessary but the multi-scale Transformer encoder requires higher computational cost. Variant D delivers a $0 . 8 \%$ AP improvement over C, but reduces latency by $8 \%$ ， suggesting that decoupling intra-scale interaction and crossscale fusion not only reduces computational cost but also improves accuracy. Compared to variant D, $D _ { { \pmb S } _ { 5 } }$ reduces the latency by $3 5 \%$ but delivers $0 . 4 \%$ AP improvement, demonstrating that intra-scale interactions of lower-level features are not required.Finally,variant E delivers $1 . 5 \%$ AP improvement over D.Despite a $2 0 \%$ increase in the number of parameters, the latency is reduced by $2 4 \%$ ,making the encoder more efficient. This shows that our hybrid encoder achieves a better trade-off between speed and accuracy.

# 5.3.Ablation Study on Query Selection

We conduct an ablation study on uncertainty-minimal query selection,and the results are reported on RT-DETR-R5O with $1 \times$ configuration, Table 4. The query selection in RT-DETR selects the top $K$ $K = 3 0 0$ ） encoder features according to the classification scores as the content queries,and the prediction boxes corresponding to the selected features are used as initial position queries. We compare the encoder features selected by the two query selection schemes on COCO val2 O17 and calculate the proportions of classi

Table 3.The indicators of the set of variants ilustrated in Figure 3.   

<table><tr><td>Variant</td><td>AP (%)</td><td>#Params (M)</td><td>Latency (ms)</td></tr><tr><td>A</td><td>43.0</td><td>31</td><td>7.2</td></tr><tr><td>B</td><td>44.9</td><td>32</td><td>11.1</td></tr><tr><td>C</td><td>45.6</td><td>32</td><td>13.3</td></tr><tr><td>D</td><td>46.4</td><td>35</td><td>12.2</td></tr><tr><td>Ds5</td><td>46.8</td><td>35</td><td>7.9</td></tr><tr><td>E</td><td>47.9</td><td>42</td><td>9.3</td></tr></table>

Table 4.Results of the ablation study on uncertainty-minimal query selection. $\mathbf { P r o p } _ { c l s }$ and $\mathbf { P r o p } _ { b o t h }$ represent the proportion of classification score and both scores greater than O.5 respectively.

fication scores greater than O.5 and both classification and IoU scores greater than O.5,respectively. The results show that the encoder features selected by uncertainty-minimal query selection not only increase the proportion of high classification scores ( $( 0 . 8 2 \%$ VS $0 . 3 5 \%$ ）but also provide more high-quality features $( 0 . 6 7 \%$ VS $0 . 3 0 \%$ ).We also evaluate the accuracy of the detectors trained with the two query selection schemes on COCO val 2 O17,where the uncertaintyminimal query selection achieves an improvement of $0 . 8 \%$ AP( $4 8 . 7 \%$ AP vs $4 7 . 9 \%$ AP).

# 5.4. Ablation Study on Decoder

Table 5 shows the inference latency and accuracy of each decoder layer of RT-DETR-R5O trained with different numbers of decoder layers.When the number of decoder layers is set to 6, the RT-DETR-R5O achieves the best accuracy $5 3 . 1 \%$ AP. Furthermore, we observe that the diference in accuracy between adjacent decoder layers gradually decreases as the index of the decoder layer increases. Taking the column RTDETR-R50-Det6 as an example, using 5-th decoder layer for inference only loses $0 . 1 \%$ AP $( 5 3 . 1 \%$ AP vs $5 3 . 0 \%$ AP) in accuracy, while reducing latency by $0 . 5 \mathrm { m s }$ (9.3 ms vs 8.8 ms). Therefore,RT-DETR supports flexible speed tuning by adjusting the number of decoder layers without retraining, thus improving its practicality.

Table 5.Results of the ablation study on decoder. ID indicates decoder layer index.Detk represents detector with $k$ decoder layers. All results are reported on RT-DETR-R5O with $6 \times$ configuration.   

<table><tr><td>Query selection</td><td>AP (%)</td><td>Propcls↑ (%）</td><td>Propboth↑ (%）</td></tr><tr><td>Vanilla</td><td>47.9</td><td>0.35</td><td>0.30</td></tr><tr><td>Uncertainty-minimal</td><td>48.7</td><td>0.82</td><td>0.67</td></tr></table>

# 6. Limitation and Discussion

Limitation. Although the proposed RT-DETR outperforms the state-of-the-art real-time detectors and end-to-end detectors with similar size in both speed and accuracy,it shares the same limitation as the other DETRs,i.e., the performance on small objects is still inferior than the strong real-time detectors.According to Table 2, RT-DETR-R50 is $0 . 5 \%$ AP lower than the highest $\mathsf { A P } _ { S } ^ { v a l }$ in the $L$ model(YOLOv8-L) and RTDETR-R101 is $0 . 9 \%$ AP lower than the highest $\mathsf { A P } _ { S } ^ { v a l }$ in the $X$ model (YOLOv7-X). We hope that this problem will be addressed in future work.

<table><tr><td rowspan="2">ID</td><td colspan="4">AP(%)</td><td rowspan="2">Latency (ms)</td></tr><tr><td>Det4</td><td>Det</td><td>Det6</td><td>Det7</td></tr><tr><td>7</td><td>=</td><td>=</td><td></td><td>52.6</td><td>9.6</td></tr><tr><td>6</td><td>=</td><td>=</td><td>53.1</td><td>52.6</td><td>9.3</td></tr><tr><td>5</td><td>1</td><td>52.9</td><td>53.0</td><td>52.5</td><td>8.8</td></tr><tr><td>4</td><td>52.7</td><td>52.7</td><td>52.7</td><td>52.1</td><td>8.3</td></tr><tr><td>3</td><td>52.4</td><td>52.3</td><td>52.4</td><td>51.5</td><td>7.9</td></tr><tr><td>2</td><td>51.6</td><td>51.3</td><td>51.3</td><td>50.6</td><td>7.5</td></tr><tr><td>1</td><td>49.6</td><td>48.8</td><td>49.1</td><td>48.3</td><td>7.0</td></tr></table>

Discussion. Existing large DETR models [3, 6, 30, 38, 41, 43] have demonstrated impressive performance on COCO te st-dev [19] leaderboard. The proposed RT-DETR at different scales preserves decoders homogeneous to other DETRs,which makes it possible to distill our lightweight detector with high accuracy pre-trained large DETR models. We believe that this is one of the advantages of RT-DETR over other real-time detectors and could be an interesting direction for future exploration.

# 7. Conclusion

In this work, we propose a real-time end-to-end detector, called RT-DETR, which successfully extends DETR to the real-time detection scenario and achieves state-of-the-art performance. RT-DETR includes two key enhancements: an efficient hybrid encoder that expeditiously processes multiscale features,and the uncertainty-minimal query selection that improves the quality of initial object queries.Furthermore,RT-DETR supports flexible speed tuning without retraining and eliminates the inconvenience caused by two NMS thresholds, facilitating its practical application. RTDETR,along with its model scaling strategy, broadens the technical approach to real-time object detection, offering new possibilities beyond YOLO for diverse real-time scenarios.We hope that RT-DETR can be put into practice.

Acknowledgements. This work was supported in part by the National Key R&D Program of China (No. 2022ZD0118201), Natural Science Foundation of China (No. 61972217, 32071459, 62176249, 62006133,62271465), and the Shenzhen Medical Research Funds in China (No. B2302037). Thanks to Chang Liu, Zhennan Wang and Kehan Li for helpful suggestions on writing and presentation.

# References

[1] Alexey Bochkovskiy, Chien-Yao Wang,and Hong-Yuan Mark Liao.Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934,2020. 1,2   
[2] Daniel Bogdoll, Maximilian Nitsche,and JMarius Zollner. Anomaly detection in autonomous driving: A survey. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4488-4499,2022. 1   
[3] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun Li,and Xiangyu Zhang. Reversible column networks. In International Conference on Learning Representations,2022. 8   
[4] Nicolas Carion,Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,and Sergey Zagoruyko. End-toend object detection with transformers.In European Conference on Computer Vision,pages 213-229.Springer, 2020.1, 2,7   
[5] Qiang Chen, Xiaokang Chen, Gang Zeng,and Jingdong Wang. Group detr: Fast training convergence with decoupled oneto-many label assignment. arXiv preprint arXiv:2207.13085, 2022.2   
[6] Qiang Chen,Jian Wang,Chuchu Han, Shan Zhang,Zexian Li, Xiaokang Chen, Jiahui Chen, Xiaodi Wang,Shuming Han, Gang Zhang, et al. Group detr v2: Strong object detector with encoder-decoder pretraining. arXiv preprint arXiv:2211.03594,2022. 8   
[7] Xiaohan Ding, Xiangyu Zhang,Ningning Ma, Jungong Han, Guiguang Ding,and Jian Sun. Repvgg: Making vgg-style convnets great again.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13733-13742, 2021. 5   
[8] Peng Gao,Minghang Zheng,Xiaogang Wang, Jifeng Dai, and Hongsheng Li. Fast convergence of detr with spatially modulated co-attention.In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3621- 3630,2021. 7   
[9] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li,and Jian Sun.Yolox: Exceeding yolo series in 2021.arXiv preprint arXiv:2107.08430,2021. 1,2   
[10] Jocher Glenn. Yolov5 release v7.0. https ://github. com/ultralytics/yolov5/tree/v7.0,2022. 2,3, 6,7   
[11] Jocher Glenn.Yolov8.https://github.com / ultralytics/ultralytics/tree/main, 2023. 1, 2, 3,6,7   
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren,and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770-778,2016.6   
[13] Tong He, Zhi Zhang,Hang Zhang, Zhongyue Zhang,Junyuan Xie,and Mu Li. Bag of tricks for image classification with convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 558-567,2019. 6   
[14] Xin Huang, Xinxin Wang, Wenyu Lv, Xiaying Bai, Xiang Long, Kaipeng Deng, Qingqing Dang, Shumin Han, Qiwen Liu,Xiaoguang Hu, et al. Pp-yolov2: A practical object detector. arXiv preprint arXiv:2i04.10419,2021.1, 2   
[15] Chuyi Li, Lulu Li, Yifei Geng,Hongliang Jiang,Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu,and Xiangxiang Chu. Yolov6 v3.0: A ful-scale reloading. arXiv preprint arXiv:2301.05586,2023. 1,2,3,6,7   
[16] Feng Li,Hao Zhang, Shilong Liu,Jian Guo,Lionel MNi,and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13619- 13627,2022. 1, 2,7   
[17] Feng Li, Ailing Zeng, Shilong Liu, Hao Zhang,Hongyang Li,Lei Zhang,and Lionel MNi. Lite detr: An interleaved multi-scale encoder for efficient detr. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18558-18567,2023.2   
[18] Junyu Lin, Xiaofeng Mao, Yuefeng Chen,Lei Xu, Yuan He,and Hui Xue. D' 2etr: Decoder-only detr with computationally efficient cross-scale attention. arXiv preprint arXiv:2203.00860,2022. 4   
[19] Tsung-YiLin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Dollar,and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740-755. Springer, 2014. 3, 8   
[20] Tsung-Yi Lin,Priya Goyal,Ross Girshick,Kaiming He,and Piotr Dollar.Focal loss for dense object detection.In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2980-2988,2017. 2   
[21] Shu Liu,Lu Qi, Haifang Qin,Jianping Shi,and Jiaya Jia. Path aggregation network for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8759-8768,2018. 4   
[22] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu,and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. In International Conference on Learning Representations, 2021. 1,2,7   
[23] Wei Liu,Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, ScottReed, Cheng-Yang Fu,and Alexander C Berg. Ssd: Single shot multibox detector. In European Conference on Computer Vision,pages 21-37. Springer,2016.2   
[24] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang, Qingqing Dang, Yuan Gao,Hui Shen, Jianguo Ren, Shumin Han,Errui Ding,et al.Pp-yolo: An effective and efficient implementation of object detector.arXiv preprint arXiv:2007.12099,2020. 1, 2   
[25] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan,Lei Sun,and Jingdong Wang. Conditional detr for fast training convergence.In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3651-3660,2021.1,3,7   
[26] Rashmika Nawaratne,Damminda Alahakoon, Daswin De Silva,and Xinghuo Yu. Spatiotemporal anomaly detection using deep learning for real-time video surveillance.IEEE Transactions on Industrial Informatics,16(1):393-402,2019. 1   
[27] Joseph Redmon and Ali Farhadi. Yolo9OoO:better, faster, stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7263-7271, 2017.2   
[28] Joseph Redmon and Ali Farhadi. Yolov3:An incremental improvement. arXiv preprint arXiv:1804.02767,2018.1, 2   
[29] Joseph Redmon, Santosh Divvala,Ross Girshick,and Ali Farhadi. You only look once: Unified,real-time object detection．In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 779-788, 2016.2   
[30] Tianhe Ren, Jianwei Yang, Shilong Liu, Ailing Zeng,Feng Li, Hao Zhang, Hongyang Li, Zhaoyang Zeng, and Lei Zhang. A strong and reproducible object detector with only public datasets. arXiv preprint arXiv:2304.13027,2023.8   
[31] Byungseok Roh,JaeWoong Shin, Wuhyun Shin,and Saehoon Kim. Sparse detr: Efficient end-to-end object detection with learnable sparsity. In International Conference on Learning Representations,2021. 2   
[32] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li,and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8430-8439,2019. 2   
[33] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu,Wei Zhan, Masayoshi Tomizuka,Lei Li, Zehuan Yuan, Changhu Wang,et al. Sparse r-cnn: End-to-end object detection with learnable proposals.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14454-14463,2021.1   
[34] Chien-Yao Wang, Alexey Bochkovskiy,and Hong-Yuan Mark Liao.Scaled-yolov4: Scaling cross stage partial network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13029-13038,2021.2   
[35] Chien-Yao Wang,Alexey Bochkovskiy,and Hong-Yuan Mark Liao.Yolov7: Trainable bag-of-freebies sets new state-ofthe-art for real-time object detectors.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7464-7475,2023. 1,2,3,6,7   
[36] Yingming Wang,Xiangyu Zhang, Tong Yang,and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings ofthe AAI Conference on Artificial Intelligence, pages 2567-2575,2022.1,3,7   
[37] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, et al. $\mathrm { P p }$ -yoloe:An evolved version of yolo. arXiv preprint arXiv:2203.16250,2022.1,2, 3,6,7   
[38] Jianwei Yang,Chunyuan Li, Xiyang Dai,and Jianfeng Gao. Focal modulation networks.Advances in Neural Information Processing Systems,35:4203-4217,2022. 8   
[39] Zhuyu Yao,Jiangbo Ai,Boxun Li,and Chi Zhang.Efficient detr: improving end-to-end object detector with dense prior. arXiv preprint arXiv:2104.01318,2021. 2,5,7   
[40] Fangao Zeng,Bin Dong,Yuang Zhang, Tiancai Wang, Xiangyu Zhang,and Yichen Wei.Motr: End-to-end multipleobject tracking with transformer. In European Conference on Computer Vision,pages 659-675. Springer,2022.1   
[41] Hao Zhang,Feng Li, Shilong Liu,Lei Zhang, Hang Su, Jun Zhu,Lionel Ni,and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.In International Conference on Learning Representations,2022.1,2,3,5,7, 8   
[42] Xizhou Zhu,Weijie Su,Lewei Lu,Bin Li, Xiaogang Wang, and Jifeng Dai.Deformable detr: Deformable transformers for end-to-end object detection.In International Conference onLearning Representations,2020.1,2,3,4,5,7   
[43] Zhuofan Zong,Guanglu Song,and Yu Liu. Detrs with collaborative hybrid assignments training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6748-6758,2023.8