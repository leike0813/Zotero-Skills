# Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers

Zhiqi $\mathrm { L i ^ { 1 } }$ , Wenhai Wang², Enze Xie³, Zhiding $\mathrm { Y u ^ { 4 } }$ ， Anima Anandkumar4.5, Jose M. Alvarez4, Ping Luo³, Tong Lu1 1Nanjing University ²Shanghai AI Laboratory The University of Hong Kong 4NVIDIA 5Caltech lzq $@$ smail.nju.edu.cn wangwenhai $@$ pjlab.org.cn xieenze $@$ hku.hk zhidingy $@$ nvidia.com aanandkumar@ nvidia.com josea $@$ nvidia.com pluo@cs.hku.hk lutong@nju.edu.cn

# Abstract

Panoptic segmentation involves a combination of joint semantic segmentation and instance segmentation,where image contents are divided into two types: things and stuff. We present Panoptic SegFormer, a general framework for panoptic segmentation with transformers. It contains three innovative components: an efcient deeply-supervised mask decoder, a query decoupling strategy， and an improved post-processing method. We also use Deformable DETR to effciently process multi-scale features,which is a fast and efficient version of DETR. Specifically， we supervise the attention modules in the mask decoder in a layer-wise manner. This deep supervision strategy lets the attention modules quickly focus on meaningful semantic regions. It improves performance and reduces the number of required training epochs by half compared to Deformable DETR. Our query decoupling strategy decouples the responsibilities of the query set and avoids mutual interference between things and stuff. In addition, our postprocessing strategy improves performance without additional costs by jointly considering classification and segmentation qualities to resolve conflicting mask overlaps. Our approach increases the accuracy $6 . 2 \%$ PQ over the baseline DETR model. Panoptic SegFormer achieves stateof-the-art results on COCO test-dev with $56 . 2 \%$ PQ. It also shows stronger zero-shot robustness over existing methods.

# 1. Introduction

Semantic segmentation and instance segmentation are two important and related vision tasks. Their underlying connections recently motivated panoptic segmentation as a unification of both the tasks [6]. In panoptic segmentation, image contents are divided into two types: things and stuff. Things refer to countable instances (e.g., person,car) and each instance has a unique id to distinguish it from the other instances. Stuff refers to the amorphous and uncountable regions (e.g., sky, grassland) and has no instance id [6].

![](Images_SMFVBYXT/c06bfb8dc2ed43f439647097294e5922d94ea84badd6793d9d6d66cb6a0928af.jpg)  
Figure 1. Comparison to the prior arts in panoptic segmentation methods on the COCO val2017 split. Panoptic SegFormer models outperform the other counterparts among different models. Panoptic SegFormer (PVTv2-B5 [5]） achieves $5 5 . 4 \%$ PQ,surpassing previous methods with significantly fewer parameters.

Recent works [1-3] attempt to employ transformers to handle both things and stuff through a query set. For example,DETR[1] simplifies the workflow of panoptic segmentation by adding a panoptic head on top of an end-to-end object detector. Unlike previous methods [6,7],DETR does not require additional handcrafted pipelines [8,9]. While being simple,DETR also causes some issues: (1) It requires a lengthy training process to converge; (2) Because the computational complexity of self-attention is squared with the length of the input sequence, the feature resolution of DETR is limited. So that it uses an FPN-style [1,1O] panoptic head to generate masks,which always suffer low-fidelity boundaries;(3) It handles things and stuff equally,yet representing them with bounding boxes,which may be suboptimal for stuff [2,3]. Although DETR achieves excellent performance on the object detection task,its superiority on panoptic segmentation has not been well demonstrated.In order to overcome the defects of DETR on panoptic segmentation,we propose a series of novel and effective strategies that improve the performance of transformer-based panoptic segmentation models by a large margin.

![](Images_SMFVBYXT/2cbe1e4ffcb22027c6478e52efc78573177d3cce15b167f51e83a85a526f1693.jpg)  
Figure2.Overviewof Panoptic SegFormer.Panoptic SegFormeriscomposedofbackbone,encoder,anddecoder.Thebackbonend the encoder output and refine multi-scale features. Inputs of the location decoder are $N _ { \mathrm { t h } }$ thing queries and the multi-scale features. We feed $N _ { \mathrm { t h } }$ thing queries from the location decoder and $N _ { \mathrm { s t } }$ stuff queries to the mask decoder. The location decoder aims to learn reference pointsofqueries,andthemaskdecoderpredictsthefialcategoryandmask.Detailsofthedecoderwillbeintroducedbelow.Weusea mask-wise merging method instead of the commonly used pixel-wise argmax method to perform inference.

Our approach. In this work, we propose Panoptic SegFormer, a concise and efective framework for panoptic segmentation with transformers. Our framework design is motivated by the following observations: 1) Deep supervision matters in learning high-qualities discriminative attention representations in the mask decoder. 2) Treating things and stuff with the same recipe[1] is suboptimal due to the different properties between things and stuff [6].3) Commonly used post-processing such as pixel-wise argmax [1-3] tends to generate false-positive results due to extreme anomalies.We overcome these challenges in Panoptic SegFormer framework as follows:

· We propose a mask decoder that utilizes multi-scale attention maps to generate high-fidelity masks.The mask decoder is deeply-supervised,promoting discriminative attention representations in the intermediate layers with better mask qualities and faster convergence. · We propose a query decoupling strategy that decomposes the query set into a thing query set to match things via bipartite matching and another stuff query set to process stuff with class-fixed assign. This strategy avoids mutual interference between things and stuff within each query and significantly improves the qualities of stuff segmentation. Kindly refer to Sec.3.3.1 and Fig.3 for more details. · We propose an improved post-processing method to generate results in panoptic format. Besides being more efficient than the widely used pixel-wise argmax method, our method contains a mask-wise merging strategy that considers both classification probability and predicted mask qualities. Our post-processing method alone renders a $1 . 3 \%$ PQ improvement to DETR [1].

We conduct extensive experiments on COCO [11] dataset. As shown in Fig.1, Panoptic SegFormer significantly surpasses priors arts such as MaskFormer [3] and K-Net [4] with much fewer parameters.With deformable attention [12] and our deeply-supervised mask decoder, our method requires much fewer training epochs than previous transformer-based methods (24 vs. $3 0 0 +$ ）[1,3]. In addition,our approach also achieves competitive performance with current methods [13,14] on the instance segmentation task.

# 2. Related Work

Panoptic Segmentation. Panoptic segmentation becomes a popular task for holistic scene understanding [6, 15-17]. The panoptic segmentation literature mainly treats this problem as a joint task of instance segmentation and semantic segmentation where things and stuff are handled separately [18,19]. Kirillov et al. [6] proposed the concept of and benchmark of panoptic segmentation together with a baseline that directly combines the outputs of individual instance segmentation and semantic segmentation models. Since then, models such as Panoptic FPN[7], UPSNet [9] and AUNet [2O] have improved the accuracy and reduced the computational overhead by combining instance segmentation and semantic segmentation into a single model. However, these methods approximate the target task by solving the surrogate sub-tasks, therefore introducing undesired model complexities and suboptimal performance.

Recently, efforts have been made to unify the framework of panoptic segmentation. Li et al. [21] proposed Panoptic FCN where the panoptic segmentation pipeline is simplified with a“top-down meets bottom-up” two-branch design similar to CondInst [22]. In their work, things and stuff are jointly modeled by an object/region-level kernel branch and an image-level feature branch. Several recent works represent things and stuf as queries and perform endto-end panoptic segmentation via transformers.DETR[1] predicts the bounding boxes of things and stuff and combines the attention maps of the transformer decoder and the feature maps of ResNet [23] to perform panoptic segmentation. Max-Deeplab [2] directly predicts object categories and masks through a dual-path transformer regardless of the category being things or stuff. On top of DETR, MaskFomer [3] used an additional pixel decoder to refine high spatial resolution features and generated the masks by multiplying queries and features from the pixel decoder. Due to the computational complexity of self attention [24], both DETR and MaskFormer use feature maps with limited spatial resolutions for panoptic segmentation,which hurts the performance and requires combining additional highresolution feature maps in final mask prediction.Unlike the methods mentioned above,our query decoupling strategy deals with things and stuff with separate query sets. Although thing and stuff queries are designed for different targets,they are processed by the mask decoder with the same workflow. Prediction results of these queries are in the same format so that we can process them in an equal manner during the post-processing procedure. One concurrent work [4] employs a similar line of thinking to use dynamic kernels to perform instance and semantic segmentation,and it aims to utilize unified kernels to handle various segmentation tasks. In contrast to it, we aim to delve deeper into the transformerbased panoptic segmentation. Due to the different nature of various tasks,whether a unified pipeline is suitable for these tasks is still an open problem.In this work,we utilize an additional location decoder to assist things to learn location clues and get better results.

End-to-end Object Detection. The recent popular endto-end object detection frameworks have inspired many other related works [13,25]. DETR [1] is arguably the most representative end-to-end object detector among these methods.DETR models the object detection task as a dictionary lookup problem with learnable queries and employs an encoder-decoder transformer to predict bounding boxes without extra post-processing.DETR greatly simplifies the conventional detection framework and removes many handcrafted components such as Non-Maximum Suppression (NMS) [26,27] and anchors [27]. Zhu et al.[12] proposed Deformable DETR,which further reduces the memory and computational cost through deformable attention layers.In this work,we adopt deformable attention [12] for the improved efficiency and convergence over DETR[1].

# 3. Methods

# 3.1. Overall Architecture

As illustrated in Fig.2,Panoptic SegFormer consists of three key modules: transformer encoder, location decoder, and mask decoder, where (1) the transformer encoder is applied to refine the multi-scale feature maps given by the backbone,(2) the location decoder is designed to capturing location clues of things,and (3) the mask decoder is for final classification and segmentation.

![](Images_SMFVBYXT/13a152ab8656c839477fd47d15d8d685457cddd9cc7f200f7aa5d32560fa3cf9.jpg)  
Figure 3.(a) Methods [1-3] adopt one query set to match things (purple squares)and stuff (green squares) jointly.(b) We use one thing query set (purple circles） to target things through bipartite matching and one stuff query set (green circles) to predict stuff by a class-fixed assign strategy. $\emptyset$ is assigned to not-matched queries.

Our architecture feeds an input image $X \in \mathbb { R } ^ { H \times W \times 3 }$ to the backbone network,and obtains the feature maps $C _ { 3 }$ ， $C _ { 4 }$ ， and $C _ { 5 }$ from the last three stages,of which the resolutions are $1 / 8 , 1 / 1 6$ and $1 / 3 2$ compared to the input image, respectively. We project the three feature maps to the ones with 256 channels by a fully-connected (FC) layer, and flatten them into feature tokens $C _ { 3 } ^ { \prime }$ ， $C _ { 4 } ^ { \prime }$ ,and $C _ { 5 } ^ { \prime }$ . Here, we define $L _ { i }$ as ${ \frac { H } { 2 ^ { i + 2 } } } \times { \frac { W } { 2 ^ { i + 2 } } }$ , and the shapes of $C _ { 3 } ^ { \prime }$ ， $C _ { 4 } ^ { \prime }$ ,and $C _ { 5 } ^ { \prime }$ are $L _ { 1 } \times 2 5 6$ ， $L _ { 2 } \times 2 5 6$ ,and $L _ { 3 } \times 2 5 6$ ,respectively. Next, using the concatenated feature tokens as input, the transformer encoder outputs the refined features of size $( L _ { 1 } + L _ { 2 } + L _ { 3 } ) { \times } 2 5 6$ After that, we use $N _ { \mathrm { t h } }$ and $N _ { \mathrm { s t } }$ randomly initialized things and stuff queries to describe things and stuff separately. Location decoder refines $N _ { \mathrm { t h } }$ thing queries by detecting the bounding boxes of things to capture location information. The mask decoder then takes both things and stuff queries as input and predicts mask and category at each layer.

During inference,we adopt a mask-wise merging strategy to convert the predicted masks from final mask decoder layer into the panoptic segmentation results,which will be introduced in detail in Sec.3.5.

# 3.2. Transformer Encoder

High-resolution and the multi-scale features maps are important for the segmentation tasks [7,21,28].Since the high computational cost of self-attention layer, previous transformer-based methods [1,3] can only process lowresolution feature maps (e.g., ResNet $C _ { 5 }$ )in their encoders, which limits the segmentation performance.Different from these methods,we employ the deformable attention [12] to implement our transformer encoder. Due to the low computational complexity of the deformable attention, our encoder can refine and involve positional encoding [24] to high-resolution and multi-scale feature maps $F$

# 3.3. Decoder

In this section, we introduce our query decoupling strategy firstly,and then we will explain the details of our location decoder and mask decoder.

# 3.3.1Query Decoupling Strategy

We argue that using one query set to handle both things and stuff equally is suboptimal. Since there many different properties between them, things and stuff is likely to interfere with each other and hurt the model performance, especially for $\mathrm { P Q } ^ { \mathrm { s t } }$ . To prevent things and stuff from interfering with each other, we apply a query decoupling strategy in Panoptic SegFormer, as shown in Fig. 3. Specifically, $N _ { \mathrm { t h } }$ thing queries are used to predict things results, and $N _ { \mathrm { s t } }$ stuff queries target stuff only. Using this form, things and stuff queries can share the same pipeline since they are in the same format. We can also customize private workflow for things or stuff according to the characteristics of different tasks.In this work,we use an additional location decoder to detect individual instances with thing queries,and this will assist in distinguishing between different instances [6]. Mask decoder accepts both thing queries and stuff queries and generates the final masks and categories. Note that, for thing queries,ground truths are assigned by bipartite matching strategy. For stuff,We use a class-fixed assign strategy,and each stuff query corresponds to one stuff category. Thing and stuf queries will output results in the same format,and we handle these results with a uniform postprocessing method.

# 3.3.2 Location Decoder

Location information plays an important role in distinguishing things with different instance ids in the panoptic segmentation task [22,28,29]. Inspired by this,we employ a location decoder to introduce the location information of things into the learnable queries. Specifically，given $N _ { \mathrm { t h } }$ randomly initialized thing queries and the refined feature tokens generated by transformer encoder, the decoder will output $N _ { \mathrm { t h } }$ location-aware queries.

In the training phase, we apply an auxiliary MLP head on top of location-aware queries to predict the bounding boxes and categories of the target object, We supervise the prediction results with a detection loss $\mathcal { L } _ { \mathrm { d e t } }$ . The MLP head is an auxiliary branch,which can be discarded during the inference phase.The location decoder follows Deformable DETR [12]. Notably, the location decoder can learn location information by predicting the mass centers of masks instead of bounding boxes. This box-free model can still achieve comparable results to our box-based model.

# 3.3.3Mask Decoder

As shown in Fig.2 (d),the mask decoder is proposed to predict the categories and masks according to the given queries. The queries $Q$ of the mask decoder are the locationaware thing queries from the location decoder or the classfixed stuff queries. The keys $K$ and values $V$ of the mask decoder are projected from the refined feature tokens $F$ from the transformer encoder. We first pass thing queries through the mask decoder,and then fetch the attntion map

$A \ \in \ \mathbb { R } ^ { N \times h \times ( L _ { 1 } + L _ { 2 } + L _ { 3 } ) }$ and the refined query $Q _ { \mathrm { r e f i n e } } \in$ $\mathbb { R } ^ { N \times 2 5 6 }$ from each decoder layer, where $N = N _ { \mathrm { t h } } + N _ { \mathrm { s t } }$ is the whole query number, $h$ is the number of attention heads, and $L _ { 1 } + L _ { 2 } + L _ { 3 }$ is the length of feature tokens $F$ .

Similar to methods [1,2], we directly perform classifcation through a FC layer on top of the refined query $Q _ { \mathrm { r e f i n e } }$ from each decoder layer. Each thing query needs to predict probabilities over all thing categories. Stuff query only predicts the probability of its corresponding stuff category.

At the same time, to predict the masks, we first split and reshape the attention maps $A$ into attention maps $A _ { 3 } , A _ { 4 }$ ， and $A _ { 5 }$ , which have the same spatial resolution as $C _ { 3 } , C _ { 4 }$ ， and $C _ { 5 }$ . This process can be formulated as:

$$
\begin{array} { r } { ( A _ { 3 } , A _ { 4 } , A _ { 5 } ) = \mathrm { S p l i t } ( A ) , \ : \ : \ : A _ { i } \in \mathbb { R } ^ { \frac { H } { 2 ^ { i + 2 } } \times \frac { W } { 2 ^ { i + 2 } } \times h } , } \end{array}
$$

where $\operatorname { S p l i t } ( \cdot )$ denotes the split and reshaping operation. After that, as illustrated in Eq. (2), we upsample these attention maps to the resolution of $H / 8 { \times } W / 8$ and concatenate them along the channel dimension,

$$
A _ { \mathrm { f u s e d } } = \operatorname { C o n c a t } ( A _ { 1 } , \operatorname { U p } _ { \times 2 } ( A _ { 2 } ) , \operatorname { U p } _ { \times 4 } ( A _ { 3 } ) ) ,
$$

where $\mathrm { U p } _ { \times 2 } ( \cdot )$ and $\mathrm { U p } _ { \times 4 } ( \cdot )$ mean the 2 times and 4 times bilinear interpolation operations,respectively. Concat(·) is the concatenation operation. Finally, based on the fused attention maps $A _ { \mathrm { f u s e d } }$ ,we predict the binary mask through a $1 \times 1$ convolution.

Previous literature [12] argues that the reason for slow convergence of DETR is that attention modules equally pay attention to all the pixels in the feature maps,and learning to focus on sparse meaningful locations requires plenty of effort. We use two key designs to solve this problem in our mask decoder:(1) Using an ultra-light FC head to generate masks from the attention maps,ensuring attention modules can be guided by ground truth mask to learn where to focus on. This FC head only contains 2OO parameters,which ensures the semantic information of attention maps is highly related to the mask. Intuitively, the ground truth mask is exactly the meaningful region on which we expect the attention module to focus.(2) We employ deep supervision in the mask decoder. Attention maps of each layer will be supervised by the mask,the attention module can capture meaningful information in the earlier stage.This can highly accelerate the learning process of attention modules.

# 3.4. Loss Function

During training,our overall loss function of Panoptic SegFormer can be written as:

$$
\mathcal { L } = \lambda _ { \mathrm { t h i n g s } } \mathcal { L } _ { \mathrm { t h i n g s } } + \lambda _ { \mathrm { s t u f f } } \mathcal { L } _ { \mathrm { s t u f f } } ,
$$

where $L _ { \mathrm { t h i n g s } }$ and $L _ { \mathrm { s t u f f } }$ are loss for things and stuff, separately. $\lambda _ { \mathrm { t h i n g s } }$ and $\lambda _ { \mathrm { s t u f f } }$ are hyperparameters.

Things Loss. Following common practices [1,30], we search the best bipartite matching between the prediction set and the ground truth set. Specifically, we utilize Hungarian algorithm [31] to search for the permutation with the minimum matching cost, which is the sum of the classification loss $\mathcal { L } _ { \mathrm { c l s } }$ ,detection loss $\mathcal { L } _ { \mathrm { d e t } }$ and the segmentation loss $\mathcal { L } _ { \mathrm { s e g } }$ ． The overall loss function for the thing categories is accordingly defined as follows:

$$
\mathcal { L } _ { \mathrm { t h i n g s } } = \lambda _ { \mathrm { d e t } } \mathcal { L } _ { \mathrm { d e t } } + \sum _ { i } ^ { D _ { m } } { ( \lambda _ { \mathrm { c l s } } \mathcal { L } _ { \mathrm { c l s } } ^ { i } + \lambda _ { \mathrm { s e g } } \mathcal { L } _ { \mathrm { s e g } } ^ { i } ) } ,
$$

where $\lambda _ { \mathrm { c l s } } , \lambda _ { \mathrm { s e g } }$ and $\lambda _ { \mathrm { l o c } }$ are the weights to balance three losses. $D _ { m }$ is the number of layers in the mask decoder. Cs is the classification loss that is implemented by Focal loss [27], and $\mathcal { L } _ { \mathrm { s e g } } ^ { i }$ is the segmentation lossimplemented by Dice loss [32]. $\dot { \mathcal { L } _ { \mathrm { d e t } } }$ is the loss of Deformable DETR that used to perform detection.

Stuff Loss. We use a fixed matching strategy for stuff. Thus there is a one-to-one mapping between stuff queries and stuff categories. The loss for the stuff categories is similarly defined as:

$$
\mathcal { L } _ { \mathrm { s t u f f } } = \sum _ { i } ^ { D _ { m } } { ( \lambda _ { \mathrm { c l s } } \mathcal { L } _ { \mathrm { c l s } } ^ { i } + \lambda _ { \mathrm { s e g } } \mathcal { L } _ { \mathrm { s e g } } ^ { i } ) } ,
$$

where $\mathcal { L } _ { \mathrm { c l s } } ^ { i }$ s and Cseg a are the same as those in Eq. (4).

# 3.5. Mask-Wise Merging Inference

Panoptic Segmentation requires each pixel to be assigned a category label (or void） and instance id (ignored for stuff) [6]. One challenge of panoptic segmentation is that it requires generating non-overlap results. Recent methods [1-3] directly use pixel-wise argmax to determine the attribution of each pixel,and this can solve the overlap problem naturally. Although pixel-wise argmax strategy is simple and effective,we observe that it consistently produces false-positive results due to the abnormal pixel values.

Unlike pixel-wise argmax resolves conflicts on each pixel,we propose the mask-wise merging strategy by resolving the conflicts among predicted masks. Specifically, we use the confidence scores of masks to determine the attribution of the overlap region. Inspired by previous NMS methods [28], the confidence scores take into both classifcation probability and predicted mask qualities. The confidence score of the i-th result can be formulated as:

$$
s _ { i } = p _ { i } ^ { \alpha } \times \mathrm { a v e r a g e } \big ( \mathbb { 1 } _ { \{ m _ { i } [ h , w ] > 0 . 5 \} } m _ { i } [ h , w ] \big ) ^ { \beta } ,
$$

where $p _ { i }$ is the most likely class probability of i-th result. $m _ { i } [ h , w ]$ is the mask logit at pixel $[ h , w ]$ ， $\alpha , \beta$ are used to balance the weight of classification probability and segmentation qualities.

As illustrated in Algorithm 1, mask-wise merging strategy takes $c , s$ ,and $m$ as input, denoting the predicted categories,confidence scores,and segmentation masks,respectively. It outputs a semantic mask SemMsk and an instance id mask IdMsk, to assign a category label and an instance id to each pixel. Specifically, SemMsk and IdMsk are first

# Algorithm 1: Mask-Wise Merging

<table><tr><td>#</td><td>def MaskWiseMergeing (c,s,m): category c∈RN</td></tr><tr><td>#</td><td> confidence score s ∈ RN</td></tr><tr><td>#</td><td> mask m∈ RNxHxW</td></tr><tr><td></td><td>SemMsk = np.zeros(H,W)</td></tr><tr><td></td><td>IdMsk = np.zeros(H,W)</td></tr><tr><td></td><td>order = np.argsort(-s)</td></tr><tr><td></td><td>id=0</td></tr><tr><td></td><td>for i in order:</td></tr><tr><td></td><td>mi = m[i]&gt;0.5 &amp; (SemMsk==0) mi</td></tr><tr><td></td><td>if s[i]&lt; tcnf Or m[i&gt;0.5</td></tr><tr><td></td><td>continue</td></tr><tr><td></td><td>SemMsk[mi]= c[i]</td></tr><tr><td></td><td>IdMsk[mi] = id</td></tr><tr><td></td><td></td></tr><tr><td></td><td>id += 1</td></tr></table>

initialized by zeros.Then,we sort prediction results in descending order of confidence score and fill the sorted predicted masks into SemMsk and IdMsk in order. Then we discard the results with confidence scores below $\mathrm { { t _ { c l s } } }$ and remove the overlaps with lower confidence scores. Only remained non-overlap part with a suffcient fraction $\mathrm { { t } _ { k e e p } }$ to origin mask will be kept. Finally, the category label and unique id of each mask are added to generate non-overlap panoptic format results.

# 4. Experiments

We evaluate Panoptic SegFormer on COCO [11] and ADE20K dataset [33], comparing it with several state-ofthe-art methods. We provide the main results of panoptic segmentation and instance segmentation. We also conduct detailed ablation studies to verify the effects of each module.Please refer to Appendix for implementation details.

# 4.1. Dataset

We perform experiments on COCO 2017 datasets [11] without external data. The COCO dataset contains 118K training images and 5k validation images,and it contains 80 things and 53 stuff. We further demonstrate the generality of our model on the ADE2OK dataset [33],which contains 100 things and 50 stuff.

# 4.2.Main Results

Panoptic segmentation. We conduct experiments on COCO val set and test-dev set. In Tab.1 and Tab. 2, we report our main results,comparing with other state-ofthe-art methods. Panoptic SegFormer attains $4 9 . 6 \%$ PQ on COCO val with ResNet-5O as the backbone and singlescale input, and it surpasses previous methods K-Net [4] and DETR[1] over $2 . 5 \%$ PQ and $6 . 2 \%$ PQ, respectively. Except for the remarkable performance,the training of

Table 1.Experiments on COCO val set. #P and #F indicate number of parameters (M) and number of FLOPs (G). Panoptic SegFormer (R50) achieves $4 9 . 6 \%$ PQ on COCO val, surpassing previous methods such as DETR[1] and MaskFormer [3] over $6 . 2 \%$ PQ and $3 . 1 \%$ PQ respectively. † notes that backbones are pretrained on ImageNet-22K.   

<table><tr><td>Method</td><td>Backbone</td><td>Epochs</td><td>PQ</td><td>PQth</td><td>PQst</td><td>#P</td><td>#F</td></tr><tr><td>Panoptic FPN [7]</td><td>R50</td><td>36</td><td>41.5</td><td>48.5</td><td>31.1</td><td>-</td><td>1</td></tr><tr><td>SOLOv2 [28]</td><td>R50</td><td>36</td><td>42.1</td><td>49.6</td><td>30.7</td><td>-</td><td>-</td></tr><tr><td>DETR[1]</td><td>R50</td><td>325</td><td>43.4</td><td>48.2</td><td>36.3</td><td>42.9</td><td>248</td></tr><tr><td>Panoptic FCN [21]</td><td>R50</td><td>36</td><td>43.6</td><td>49.3</td><td>35.0</td><td>37.0</td><td>244</td></tr><tr><td>K-Net [4]</td><td>R50</td><td>36</td><td>47.1</td><td>51.7</td><td>40.3</td><td>1</td><td>-</td></tr><tr><td>MaskFormer [3]</td><td>R50</td><td>300</td><td>46.5</td><td>51.0</td><td>39.8</td><td>45.0</td><td>181</td></tr><tr><td>Panoptic SegFormer</td><td>R50</td><td>12</td><td>48.0</td><td>52.3</td><td>41.5</td><td>51.0</td><td>214</td></tr><tr><td>Panoptic SegFormer</td><td>R50</td><td>24</td><td>49.6</td><td>54.4</td><td>42.4</td><td>51.0</td><td>214</td></tr><tr><td>DETR[1]</td><td>R101</td><td>325</td><td>45.1</td><td>50.5</td><td>37.0</td><td>61.8</td><td>306</td></tr><tr><td>Max-Deeplab-S [2]</td><td>Max-S [2]</td><td>54</td><td>48.4</td><td>53.0</td><td>41.5</td><td>61.9</td><td>162</td></tr><tr><td>MaskFormer [3]</td><td>R101</td><td>300</td><td>47.6</td><td>52.5</td><td>40.3</td><td>64.0</td><td>248</td></tr><tr><td>Panoptic SegFormer</td><td>R101</td><td>24</td><td>50.6</td><td>55.5</td><td>43.2</td><td>69.9</td><td>286</td></tr><tr><td>Max-Deeplab-L [2]</td><td>Max-L [2]</td><td>54</td><td>51.1</td><td>57.0</td><td>42.2</td><td>451.0</td><td>1846</td></tr><tr><td>Panoptic FCN [36]</td><td>Swin-L</td><td>36</td><td>51.8</td><td>58.6</td><td>41.6</td><td></td><td>-</td></tr><tr><td>MaskFormer [3]</td><td>Swin-L</td><td>300</td><td>52.7</td><td>58.5</td><td>44.0</td><td>212.0</td><td>792</td></tr><tr><td>K-Net [4]</td><td>Swin-L</td><td>36</td><td>54.6</td><td>60.2</td><td>46.0</td><td>208.9</td><td>-</td></tr><tr><td>Panoptic SegFormer</td><td>Swin-Lt</td><td>24</td><td>55.8</td><td>61.7</td><td>46.9</td><td>221.4</td><td>816</td></tr><tr><td>Panoptic SegFormer</td><td>PVTv2-B5†</td><td>24</td><td>55.4</td><td>61.2</td><td>46.6</td><td>104.9</td><td>349</td></tr></table>

Table 2.Experiments on COCO test-dev set.† notes that backbones are pre-trained on ImageNet-22K.   

<table><tr><td>Method</td><td>Backbone</td><td>PQ</td><td>PQth</td><td>PQst</td><td>SQ</td><td>RQ</td></tr><tr><td>Max-Deeplab-L [2]</td><td>Max-L [2]</td><td>51.3</td><td>57.2</td><td>42.4</td><td>82.5</td><td>61.3</td></tr><tr><td>Innovation [35]</td><td>ensemble</td><td>53.5</td><td>61.8</td><td>41.1</td><td>83.4</td><td>63.3</td></tr><tr><td>MaskFormer [3]</td><td>Swin-L</td><td>53.3</td><td>59.1</td><td>44.5</td><td>82.0</td><td>64.1</td></tr><tr><td>K-Net [4]</td><td>Swin-L†</td><td>55.2</td><td>61.2</td><td>46.2</td><td>82.4</td><td>66.1</td></tr><tr><td>Panoptic SegFormer</td><td>R50</td><td>50.2</td><td>55.3</td><td>42.4</td><td>81.9</td><td>60.4</td></tr><tr><td>Panoptic SegFormer</td><td>R101</td><td>50.9</td><td>56.2</td><td>43.0</td><td>82.0</td><td>61.2</td></tr><tr><td>Panoptic SegFormer</td><td>Swin-L</td><td>56.2</td><td>62.3</td><td>47.0</td><td>82.8</td><td>67.1</td></tr><tr><td>Panoptic SegFormer</td><td>PVTv2-B5†</td><td>55.8</td><td>61.9</td><td>46.5</td><td>83.0</td><td>66.5</td></tr></table>

Table 3.Panoptic segmentation results on ADE2OK val set.   

<table><tr><td>Method</td><td>Backbone</td><td>PQ</td><td>PQth</td><td>PQst</td><td>SQ</td><td>RQ</td></tr><tr><td>BGRNet[37]</td><td>R50</td><td>31.8</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Auto-Panoptic [38]</td><td>ShuffleNetV2 [39]</td><td>32.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MaskFormer [3]</td><td>R50</td><td>34.7</td><td>32.2</td><td>39.7</td><td>76.7</td><td>42.8</td></tr><tr><td>MaskFormer [3]</td><td>R101</td><td>35.7</td><td>34.5</td><td>38.0</td><td>77.4</td><td>43.8</td></tr><tr><td>Panoptic SegFormer</td><td>R50</td><td>36.4</td><td>35.3</td><td>38.6</td><td>78.0</td><td>44.9</td></tr></table>

Panoptic SegFormer is effcient. Under $1 \times$ training strategy (12 epochs), Panoptic SegFormer (R5O) achieves $4 8 . 0 \%$ PQ that outperforms MaskFormer [3] that training 3OO epochs by $1 . 5 \%$ PQ.Enhanced by vision transformer backbone Swin-L [34], Panoptic SegFormer attains a new record of $5 6 . 2 \%$ PQ on COCO test-dev without bells and whistles, surpassing MaskFormer [3] over $2 . 9 \%$ PQ. Our method even surpasses the previous competition-level method Innovation [35] over $2 . 7 ~ \%$ PQ.We also obtain comparable performance by employing PVTv2-B5 [5],while the model parameters and FLOPs are reduced significantly compared to Swin-L. Panoptic SegFormer also outperforms MaskFormer by $1 . 7 \%$ PQ on ADE20K dataset [33], see Tab.3.

Instance segmentation. Panoptic SegFormer can be converted to an instance segmentation model by just dis

Table 4. Instance segmentation on COCO t est -dev set.   

<table><tr><td>Method</td><td>Backbone</td><td>Apseg</td><td>APg</td><td>APg M</td><td>AP</td></tr><tr><td>Mask R-CNN [40]</td><td>R50</td><td>37.5</td><td>21.1</td><td>39.6</td><td>48.3</td></tr><tr><td>SOLOv2 [28]</td><td>R50</td><td>38.8</td><td>16.5</td><td>41.7</td><td>56.2</td></tr><tr><td>K-Net [4]</td><td>R50</td><td>38.6</td><td>19.1</td><td>42.0</td><td>57.7</td></tr><tr><td>SOLQ[25]</td><td>R50</td><td>39.7</td><td>21.5</td><td>42.5</td><td>53.1</td></tr><tr><td>HTC[14]</td><td>R50</td><td>39.7</td><td>22.6</td><td>42.2</td><td>50.6</td></tr><tr><td>QueryInst [13]</td><td>R50</td><td>40.6</td><td>23.4</td><td>42.5</td><td>52.8</td></tr><tr><td>Ours (w/o crop)</td><td>R50</td><td>40.4</td><td>21.1</td><td>43.8</td><td>54.7</td></tr><tr><td>Ours (w/ crop)</td><td>R50</td><td>41.7</td><td>21.9</td><td>45.3</td><td>56.3</td></tr></table>

<table><tr><td></td><td>Epochs</td><td>PQ</td><td>#Params</td><td>FLOPs</td><td>FPS</td></tr><tr><td>baseline (DETR[1])</td><td>325</td><td>43.4</td><td>42.9M</td><td>247.5G</td><td>4.9</td></tr><tr><td>+ mask-wise merging</td><td>325</td><td>44.7</td><td>42.9M</td><td>247.5G</td><td>6.1</td></tr><tr><td>++ ms deformable attention [12]</td><td>50</td><td>47.3</td><td>44.9M</td><td>618.7G</td><td>2.7</td></tr><tr><td>+++ mask decoder</td><td>24</td><td>48.5</td><td>51.0M</td><td>214.8G</td><td>7.8</td></tr><tr><td>++++ query decoupling</td><td>24</td><td>49.6</td><td>51.0M</td><td>214.2G</td><td>7.8</td></tr></table>

Table 5. We increase the panoptic segmentation performance of DETR[1](R50 [23]) from $4 3 . 4 \%$ PQ to $4 9 . 6 \%$ PQ with fewer training epochs,less computation cost,and faster inference speed.

carding stuf queries. In Tab. 4, we report our instance segmentation results on COCO test-dev set. We achieve results comparable to the current state-of-the-art methods such as QueryInst [13] and HTC [14], and $1 . 8 \mathrm { \ A P }$ higher than K-Net [4]. Using random crops during training boosts the AP by 1.3 percentage points.

# 4.3. Ablation Studies

First, we show the effect of each module in Tab.5. Compared to baseline DETR,our model achieves better performance,faster inference speed and significantly reduces the training epochs. We use Panoptic SegFormer (R5O) to perform ablation experiments by default.

Effect of Location Decoder.Location decoder assists queries to capture the location information of things. Tab. 6 shows the results with varying the num

<table><tr><td>#Layer</td><td>PQ</td><td>PQth</td><td>PQst</td></tr><tr><td>0</td><td>47.0</td><td>50.0</td><td>42.5</td></tr><tr><td>1</td><td>47.7</td><td>51.1</td><td>42.5</td></tr><tr><td>2</td><td>48.1</td><td>51.8</td><td>42.5</td></tr><tr><td>6* (box-free)</td><td>49.2</td><td>53.5</td><td>42.6</td></tr><tr><td>6</td><td>49.6</td><td>54.4</td><td>42.4</td></tr></table>

Table 6.Ablate location decoder.

ber of layers in the location decoder. With fewer location decoder layers,our model performs worse on things,and it demonstrates that learning location clues through the location decoder is beneficial to the model to handle things better. \* notes we predict mass centers rather than bounding boxes in our location decoder, and this box-free model achieves comparable results ( $4 9 . 2 \%$ PQ vs. $4 9 . 6 \%$ PQ).

Mask-wise Merging. As shows in Tab.7, we compare our mask-wise merging strategy against pixel-wise argmax strategy on various models. We use both Mask PQ and Boundary PQ[41] to make our conclusions more credible. Models with mask-wise merging strategy always performs better. DETR with mask-wise merging outperforms origin DETR by $1 . 3 \%$ PQ [1]. In addition,our mask-wise mergingis $20 \%$ less time-consuming than DETR's pixel-wise argmax since DETR uses more tricks in its code,such as merging stuff with the same category and iteratively remov

![](Images_SMFVBYXT/beb3054d83e853bc3ab693c007fc290c82eaf3acaa04c0f37753135f6477d98d.jpg)  
Figure 4.While using pixel-wise argmax,the keyboard is covered on the laptop (noted by the red circle in (e). However, the laptop has a higher classification probability than the keyboard. The pixel-wise argmax strategy fails to use this important clue.Masks logits were generated through DETR-R50 [1].

Table 7.Effect of mask-wise merging strategy. The table shows the results of models with different post-processing methods,and the backbone is ResNet-50.“(p)”refers to using pixel-wise argmax as the post-processing method.‘ $\left( \boldsymbol { \mathsf { p } } ^ { * } \right)$ ’considers both class probability and mask prediction probability in its pixel-wise argmax strategy [3]. Models with“(m)”that employ mask-wise merging always perform beter in both Mask PQ and Boundary PQ[41] than pixel-wise argmax method.   

<table><tr><td rowspan="2">Method</td><td colspan="3">Mask PQ</td><td colspan="3">Boundary PQ [41]</td></tr><tr><td>PQ</td><td>SQ</td><td>RQ</td><td>PQ</td><td>SQ</td><td>RQ</td></tr><tr><td>DETR (p)</td><td>43.4</td><td>79.3</td><td>53.8</td><td>32.8</td><td>71.0</td><td>45.2</td></tr><tr><td>DETR (m)</td><td>44.7</td><td>80.2</td><td>54.7</td><td>33.7</td><td>71.1</td><td>46.5</td></tr><tr><td>D-DETR-MS (p) D-DETR-MS (m)</td><td>46.3</td><td>80.0</td><td>56.5</td><td>37.1</td><td>72.1</td><td>50.2</td></tr><tr><td></td><td>47.3</td><td>81.1</td><td>56.8</td><td>38.0</td><td>72.3</td><td>51.0</td></tr><tr><td>MaskFormer (p)</td><td>45.6</td><td>80.2</td><td>55.8</td><td>1</td><td>-</td><td>-</td></tr><tr><td>MaskFormer (p*)</td><td>46.5</td><td>80.4</td><td>56.8</td><td>36.8</td><td>72.5</td><td>49.8</td></tr><tr><td>MaskFormer (m)</td><td>46.8</td><td>80.4</td><td>57.2</td><td>37.6</td><td>72.6</td><td>51.1</td></tr><tr><td>Panoptic SegFormer (p)</td><td>48.4</td><td>80.7</td><td>58.9</td><td>39.3</td><td>73.0</td><td>52.9</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Panoptic SegFormer (m)</td><td>49.6</td><td>81.6</td><td>59.9</td><td>40.4</td><td>73.4</td><td>54.2</td></tr></table>

<table><tr><td>Method</td><td>PQ</td><td>PQth</td><td>PQst</td><td>Apbox</td><td>APseg</td></tr><tr><td>DETR[1]</td><td>43.4</td><td>48.2</td><td>36.3</td><td>38.8</td><td>31.1</td></tr><tr><td>D-DETR-MS[12]</td><td>47.3</td><td>52.6</td><td>39.0</td><td>45.3</td><td>37.6</td></tr><tr><td>Panoptic FCN [21]</td><td>43.6</td><td>49.3</td><td>35.0</td><td>36.6</td><td>34.5</td></tr><tr><td>Ours (Joint Matching)</td><td>48.5</td><td>54.5</td><td>39.5</td><td>44.1</td><td>37.7</td></tr><tr><td>Ours (Query Decoupling)</td><td>49.6</td><td>54.4</td><td>42.4</td><td>45.6</td><td>39.5</td></tr></table>

Table 8. Effect of query decoupling strategy. PQ and AP scores of various panoptic segmentation models on COCO val2 017.

ing masks with small areas.Fig. 4 shows one typical fail case of using pixel-wise argmax.

Mask Decoder. Our proposed mask decoder converges faster since the ground truth masks guide the attention module to focus on meaningful regions. Fig.5 shows the convergence curves of several models. We only supervise the last layer of the mask decoder while not employing deep supervision.We can observe that our method achieves $4 9 . 6 \%$ PQ with training for 24 epochs,and longer training has little effect. However, D-DETR-MS needs at least 5O epochs to achieve better performance.Deep supervision is vital for our mask decoder to perform better and converge faster. Fig.6 shows the attention maps of different layers in the mask decoder, and the attention module focuses on the target car in the previous layer when using deep supervision. The attention maps are very similar to the final predicted masks,since masks are generated by attention maps with a lightweight FC head.

![](Images_SMFVBYXT/eb011235446680d7e742a856dd1b662627b10c15fbd6396f2fd0cfce888828ca.jpg)  
Figure 5.Convergence curves of Panoptic SegFormer and DDETR-MS.We train models with different training schedules. “w/o ds”refers that we do not employ deep supervision in the mask decoder. The learning rate is reduced where the curves leap.

![](Images_SMFVBYXT/030495bc1a5562c180f33758fe3737f0bbf463b919103a827dde214ca0619837.jpg)  
Figure 6.Attention maps of different layers in the mask decoder. “ds”refers to deep supervision.

Since our mask decoder can generate masks from each layer, we evaluate the performance of each layer in the mask decoder, see Tab.10. During infer

<table><tr><td>Layer</td><td>PQ</td><td>PQth</td><td>PQst</td><td>Fps</td></tr><tr><td>1st</td><td>48.8</td><td>54.3</td><td>40.5</td><td>10.6</td></tr><tr><td>2nd</td><td>49.5</td><td>54.5</td><td>42.0</td><td>9.8</td></tr><tr><td>3rd</td><td>49.6</td><td>54.5</td><td>42.3</td><td>9.3</td></tr><tr><td>Last</td><td>49.6</td><td>54.4</td><td>42.4</td><td>7.8</td></tr></table>

Table 10.Results of each layer in the mask decoder.

ence,using the first two layers of mask decoder will be on par with the whole mask decoder. It also inferences faster because the computational cost decreases. $\mathrm { P Q } ^ { \mathrm { t h } }$ is hardly affected by the number of layers, $\mathrm { P Q } ^ { \mathrm { s t } }$ performs a little poorly in the first layer. The reason is that the location decoder has made additional refinements to the thing queries.

Effect of Query Decoupling Strategy. We compare our proposed query decoupling strategy with previous DETR's matching method (described here as “joint matching") [1], as shown in Tab. 8. Following DETR, joint matching uses a set of queries to target both things and stuff and feeds all queries to both location decoder and mask decoder. For our proposed query decoupling strategy，we use thing queries to detect things through bipartite matching and use location decoder to refine them. Stuff queries are assigned through class-fixed assign strategy. For a fair comparison, both the joint matching strategy and our query decoupling strategy employ 353 queries. We can observe that our proposed strategy highly boost $\mathrm { P Q } ^ { \mathrm { s t } }$ .In addition,panoptic segmentation model can perform instance segmentation by utilizing its thing results only. However, previous panoptic segmentation methods always perform poorly on instance segmentation task even though the two tasks are closely related. Tab. 8 shows both panoptic segmentation and instance segmentation performance of various methods. Our query decoupling strategy can achieve sota performance on panoptic segmentation task while obtaining a competitive instance segmentation performance.

<table><tr><td rowspan="2">Method</td><td rowspan="2" colspan="2">Clean Mean</td><td colspan="4">Blur</td><td colspan="4">Noise</td><td colspan="4">Digital</td><td colspan="4">Weather</td></tr><tr><td>Motion Defoc Glass</td><td></td><td></td><td>Gauss</td><td></td><td></td><td></td><td>Gauss Impul Shot Speck</td><td>Bright</td><td>Contr Satur JPEG</td><td></td><td></td><td>Snow</td><td></td><td>Spatt Fog</td><td>Frost</td></tr><tr><td>Panoptic FCN (R50)</td><td>43.8</td><td>26.8</td><td>22.5</td><td>23.7</td><td>14.1</td><td>25.0</td><td>28.2</td><td>20.0</td><td>28.3</td><td>31.9</td><td>39.4</td><td>24.3</td><td>38.0</td><td>22.9</td><td>20.0</td><td></td><td>29.635.3</td><td>25.3</td></tr><tr><td>MaskFormer (R50)</td><td>47.0</td><td>29.5</td><td>24.9</td><td>28.1</td><td>16.4</td><td>29.5</td><td>31.2</td><td>24.7</td><td>30.9</td><td>34.8</td><td>42.5</td><td>27.5</td><td>41.2</td><td>22.0</td><td>20.4</td><td>31.0</td><td>38.5</td><td>27.7</td></tr><tr><td>D-DETR (R50)</td><td>47.6</td><td>30.3</td><td>25.6</td><td>28.7</td><td>16.8</td><td>29.7</td><td>32.5</td><td>24.9</td><td>31.4</td><td>35.9</td><td>43.1</td><td>28.6</td><td>41.3</td><td>24.5</td><td>21.7</td><td></td><td>31.739.7</td><td>28.7</td></tr><tr><td>Ours (R50)</td><td>50.0</td><td>32.9</td><td>26.9</td><td>30.2</td><td>17.5</td><td>31.6</td><td>35.5</td><td>27.9</td><td>35.4</td><td>38.6</td><td>45.7</td><td>31.3</td><td>43.9</td><td>29.0</td><td>24.3</td><td></td><td>35.041.9</td><td>32.3</td></tr><tr><td>MaskFormer (Swin-L)</td><td>52.9</td><td>41.7</td><td>37.3</td><td>38.0</td><td>30.4</td><td>39.3</td><td>42.3</td><td>42.5</td><td>42.8</td><td>45.3</td><td>49.7</td><td>43.9</td><td>49.4</td><td>39.7</td><td>35.2</td><td>45.2 48.8</td><td></td><td>37.9</td></tr><tr><td>Ours (Swin-L)</td><td>55.8</td><td>47.2</td><td>41.3</td><td>41.5</td><td>34.3</td><td>42.7</td><td>48.6</td><td>49.5</td><td>48.8</td><td>50.3</td><td>53.8</td><td>50.1</td><td>53.5</td><td>46.9</td><td>44.8</td><td>51.5</td><td>53.3</td><td>44.3</td></tr><tr><td>Ours (PVTv2-B5)</td><td>55.6</td><td>47.0</td><td>41.5</td><td>41.1</td><td>36.1</td><td>42.5</td><td>48.4</td><td>49.6</td><td>48.4</td><td>50.4</td><td>53.5</td><td>50.8</td><td>53.0</td><td>46.2</td><td>42.4</td><td></td><td></td><td>50.352.944.3</td></tr></table>

Table11.PanopticsegmentationresultsonCOCO-C.Toeasethe wrkloadoftheexperiment,weuseasubsetof 2O images fromthe COCO val 2 O17. The third column is the average results on 16 types of corruption data.

In short,， query decoupling strategy achieves higher $\mathrm { P Q } ^ { \mathrm { s t } }$ and $\mathbf { A P } ^ { \mathrm { s e g } }$ compared to joint matching.We analyze the experimental results of joint matching and find that if one query prefers things more, the precision of stuff results detected by it will be lower, see

![](Images_SMFVBYXT/f8ee4d1dcfa7e7a20e122c6f4a607bd38d88235fc4b383242f5fb2e803be87ac.jpg)  
Figure 7.Things-Preference vs. StuffPrecision.

Fig.7. Each point represents the Thing-Preference and Stuff-Precision corresponding to each query,and the specific definitions are in Appendix. The red line is the linear regression of these points. When using one query set to detect things and stuff together,it will cause interference within each query. Our query decoupling strategy prevents things and stuffrom interfering within the same query.

# 4.4. Robustness to Natural Corruptions

Panoptic segmentation has promising applications in many fields,such as autonomous driving.Model robustness is one of the top concerns of autonomous driving. In this experiment, we evaluate the robustness of our model to disturbed data. We follow [42] and generate COCO-C, which extends the COCO validation set to include disturbed data generated by 16 algorithms from blur, noise,digital and weather categories. We compare our model to Panoptic FCN [21], D-DETR-MS and MaskFormer [3]. The results are shown in Tab.11. We calculated the mean results of disturbed data on COCO-C.Using the same backbone,our model always performs better than others.Previous literature [43-45] found that transformer-based model has stronger robustness on image classification and semantic segmentation tasks. Our experimental results also show that the transformer-based backbone (Swin-L and PVTv2- B5) can bring better robustness to the model. However, for tasks requiring a more complex pipeline, such as panoptic segmentation,we argue that the design of the task head also plays an important role for the robustness of the model. For example, Panoptic SegFormer (Swin-L) has an average result of $4 7 . 2 \%$ PQ on COCO-C, outperforming MaskFormer (Swin-L) by $5 . 5 \%$ PQ, higher than their gap $2 . 9 \%$ PQ) on clean data. We posit it is due to our transformer-based mask decoder having stronger robustness than the convolutionbased pixel decoder of MaskFormer.

# 5. Conclusion

Limitation. This work relies on deformable attention to process multi-scale features,and the speed is a litle slow. Our model is still hard to handle features with a larger spatial shape and does not perform well for small targets.

Discussion. Recently, the segmentation field attempted to use a uniform pipeline to process various tasks, including semantic segmentation, instance segmentation, and panoptic segmentation. However, we think that complete unification is conceptually exciting but not necessarily a suitable strategy. Given the similarities and differences among the various segmentation tasks,“seek common ground while reserving differences”is a more reasonable guiding ideology. With query decoupling strategy, we can handle things and stuff in the same paradigm since they are represented as queries. In addition，we can also design customized pipelines for things or stuff. Such a flexible strategy is more suitable for various segmentation tasks.At present, taskspecific designs still bring better performance.We encourage the community to further explore the unified segmentation frameworks and expect that Panoptic SegFormer can inspire future works.

# 6. Acknowledge

This work is supported by the Natural Science Foundation of China under Grant 61672273 and Grant 61832008. Ping Luo is supported by the General Research Fund of HK No.27208720 and 17212120. Wenhai Wang and Tong Lu are corresponding authors.

References   
[1] Nicolas Carion,Francisco Massa, Gabriel Synnaeve,Nicolas Usunier, Alexander Kirillov,and Sergey Zagoruyko. End-toend object detection with transformers. In ECCV,2020.1, 2, 3,4,5,6,7   
[2] Huiyu Wang,Yukun Zhu,Hartwig Adam,Alan Yuille,and Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with mask transformers. In CVPR, 2021.1,2,3, 4,5,6   
[3] Bowen Cheng,Alexander G Schwing,and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. In NeurIPS,2021. 1,2,3,5, 6,7,8   
[4] Wenwei Zhang， Jiangmiao Pang， Kai Chen， and Chen Change Loy.K-Net:Towards unified image segmentation. In NeurIPS,2021. 1,2,3,5, 6   
[5] Wenhai Wang,Enze Xie,Xiang Li, Deng-Ping Fan,Kaitao Song,Ding Liang,Tong Lu,Ping Luo,and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv:2106.13797,2021. 1, 6   
[6] Alexander Kirillov,Kaiming He,Ross Girshick,Carsten Rother,and Piotr Dollar. Panoptic segmentation. In CVPR, 2019. 1,2,4, 5   
[7] Alexander Kirillov,Ross Girshick,Kaiming He,and Piotr Dollar. Panoptic feature pyramid networks.In CVPR, 2019. 1,2,3,6   
[8] Navaneeth Bodla,Bharat Singh，Rama Chellappa,and Larry S Davis. Soft-nms-improving object detection with one line of code. In Proceedings of the IEEE international conference on computer vision, pages 5561-5569,2017. 1   
[9] Yuwen Xiong,Renjie Liao,Hengshuang Zhao,Rui Hu,Min Bai,Ersin Yumer,and Raquel Urtasun. Upsnet:A unifed panoptic segmentation network. In CVPR,2019.1, 2   
[10] Tsung-Yi Lin,Piotr Dollar,Ross Girshick,Kaiming He, Bharath Hariharan,and Serge Belongie.Feature pyramid networks for object detection.In CVPR,2017.1   
[11] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,Piotr Dollar, and C Lawrence Zitnick.Microsoft coco: Common objects in context.In ECCV,2014. 2, 5   
[12] Xizhou Zhu, Weijie Su,Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR,2020.2,3, 4,6,7   
[13] Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Fang,Ying Shan, Bin Feng,and Wenyu Liu. Instances as queries. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6910-6919, October 2021.2,3, 6   
[14] Kai Chen,Jiangmiao Pang,Jiaqi Wang,Yu Xiong,Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In CVPR, 2019.2, 6   
[15] Ujwal Bonde,Pablo F Alcantarilla,and Stefan Leutenegger. Towards bounding-box free panoptic segmentation. In DAGM German Conference on Pattern Recognition,2020.2 [16] Qizhu Li, Xiaojuan Qi,and Philip HS Tor. Unifying training and inference for panoptic segmentation. In Proceedings of theIEEE/CVFConferenceon ComputerVision and Pattern Recognition, pages 13320-13328,2020. [17] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang,Hartwig Adam,and Liang-Chieh Chen. Panoptic-deeplab: A simple,strong,and fast baseline for bottom-up panoptic segmentation. In CVPR, 2020. 2 [18] Tien-Ju Yang,Maxwell D Collins,Yukun Zhu,Jyh-Jing Hwang,Ting Liu, Xiao Zhang,Vivienne Sze, George Papandreou,and Liang-Chieh Chen.Deeperlab:Single-shot image parser. arXiv:1902.05093,2019.2 [19] Naiyu Gao, Yanhu Shan, Yupei Wang, Xin Zhao, Yinan Yu, Ming Yang,and Kaiqi Huang. SSAP: Single-shot instance segmentation with affinity pyramid. In ICCV,2019. 2 [20] Yanwei Li, Xinze Chen, Zheng Zhu,Lingxi Xie,Guan Huang,Dalong Du,and Xingang Wang. Attention-guided unified network for panoptic segmentation. In CVPR,2019.   
2 [21] Yanwei Li, Hengshuang Zhao, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun,and Jiaya Jia. Fully convolutional networks for panoptic segmentation. In CVPR,2021. 2,3, 6,7,   
8 [22] Zhi Tian,Chunhua Shen,and Hao Chen. Conditional convolutions for instance segmentation. In ECCV,2020.2, 4 [23] Kaiming He,Xiangyu Zhang, Shaoqing Ren,and Jian Sun. Deep residual learning for image recognition. In CVPR,   
2016. 3, 6 [24] Ashish Vaswani, Noam Shazeer,Niki Parmar, Jakob Uszkoreit,Llion Jones,Aidan N Gomez, Lukasz Kaiser,and Illia Polosukhin. Attention is all you need. In NeurIPS,2017.3 [25] Bin Dong,Fangao Zeng, Tiancai Wang,Xiangyu Zhang,and Yichen Wei. Solq: Segmenting objects by learning queries. NeurIPS,2021. 3,6 [26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. NeurIPS,2015.3 [27] Tsung-Yi Lin,Priya Goyal, Ross Girshick, Kaiming He,and Piotr Dollar. Focal loss for dense object detection. In ICCV,   
2017. 3,5 [28] Xinlong Wang,Rufeng Zhang,Tao Kong,Lei Li,and Chunhua Shen. SOLOv2: Dynamic and fast instance segmentation. NeurIPS,2020. 3,4,5,6 [29] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang,and Lei Li. Solo: Segmenting objects by locations. In ECCV,   
2020.4 [30] Russell Stewart,Mykhaylo Andriluka,and Andrew Y Ng. End-to-end people detection in crowded scenes.In CVPR,   
2016.4 [31] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83-97,   
1955. 5

[45] Muzammal Naseer, Kanchana Ranasinghe,Salman Khan, Munawar Hayat,Fahad Shahbaz Khan,and Ming-Hsuan Yang． Intriguing properties of vision transformers.arXiv preprint arXiv:2105.10497,2021. 8

[32] Fausto Milletari, Nassir Navab,and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In International conference on 3D vision (3DV),2016.5   
[33] Bolei Zhou,Hang Zhao,Xavier Puig,Sanja Fidler,Adela Barriuso,and Antonio Torralba.Scene_parsing through ade2Ok dataset.In Proceedings of the IEEE conference on computer vision and patern recognition, pages 633-641, 2017.5,6   
[34] Ze Liu,Yutong Lin，Yue Cao,Han Hu,Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. ICCV,2021. 6   
[35] Chongsong Chen,Jiawei Ren，Daisheng_ Jin， Zhongang Cai, Cunjun Yu, Bairun Wang, Mingyuan Zhang,and Jinyi Wu.Joint coco and mapillary workshop at iccv 2019: Coco panoptic segmentation challenge track technical report: Panoptic htc with class-guided fusion. SHR,56(84.1):67-2. 6   
[36] Yanwei Li, Hengshuang Zhao,Xiaojuan Qi, Yukang Chen, Lu Qi,Liwei Wang, Zeming Li, Jian Sun,and Jiaya Jia. Fully convolutional networks for panoptic segmentation with point-based supervision. arXiv preprint arXiv:2108.07682, 2021. 6   
[37] Yangxin Wu, Gengwei Zhang, Yiming Gao, Xiajun Deng, Ke Gong, Xiaodan Liang,and Liang Lin. Bidirectional graph reasoning network for panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 9080-9089,2020. 6   
[38] Yangxin Wu, Gengwei Zhang, Hang Xu, Xiaodan Liang,and Liang Lin. Auto-panoptic: Cooperative multi-component architecture search for panoptic segmentation. Advances in Neural Information Processing Systems,33,2020.6   
[39] Ningning Ma, Xiangyu Zhang,Hai-Tao Zheng,and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV), pages 116-131,2018. 6   
[40] Kaiming He,Georgia Gkioxari,Piotr Dollar,and Ross Girshick.Mask R-CNN. In ICCV,2017. 6   
[41] Bowen Cheng,Ross Girshick,Piotr Dollar,Alexander C Berg,and Alexander Kirillov.Boundary iou: Improving object-centric image segmentation evaluation. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15334-15342,2021. 6,7   
[42] Christoph Kamann and Carsten Rother. Benchmarking the robustness of semantic segmentation models.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8828-8838,2020. 8   
[43] Enze Xie,Wenhai Wang, Zhiding Yu,Anima Anandkumar, Jose M Alvarez,and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS,2021. 8   
[44] Srinadh Bhojanapalli,Ayan Chakrabarti,Daniel Glasner, Daliang Li, Thomas Unterthiner,and Andreas Veit. Understanding robustness of transformers for image clasification. arXiv preprint arXiv:2103.14586,2021. 8