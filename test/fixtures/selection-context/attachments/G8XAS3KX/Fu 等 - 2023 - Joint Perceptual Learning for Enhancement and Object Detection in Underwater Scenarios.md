# Joint Perceptual Learning for Enhancement and Object Detection in Underwater Scenarios

Chenping Fu, Wanqi Yuan, Jiewen Xiao, Risheng Liu, Member, IEEE, and Xin Fan, Senior Member, IEEE

Abstract—Underwater degraded images greatly challenge existing algorithms to detect objects of interest. Recently, researchers attempt to adopt attention mechanisms or composite connections for improving the feature representation of detectors. However, this solution does not eliminate the impact of degradation on image content such as color and texture, achieving minimal improvements. Another feasible solution for underwater object detection is to develop sophisticated deep architectures in order to enhance image quality or features. Nevertheless, the visually appealing output of these enhancement modules do not necessarily generate high accuracy for deep detectors. More recently, some multi-task learning methods jointly learn underwater detection and image enhancement, accessing promising improvements. Typically, these methods invoke huge architecture and expensive computations, rendering inefficient inference. Definitely, underwater object detection and image enhancement are two interrelated tasks. Leveraging information coming from the two tasks can benefit each task. Based on these factual opinions, we propose a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks. DPNet with one shared module and two task subnets learns from the two different tasks, seeking a shared representation. The shared representation provides more structural details for image enhancement and rich content information for object detection. Finally, we derive a cooperative training strategy to optimize parameters for DPNet. Extensive experiments on realworld and synthetic underwater datasets demonstrate that our method outputs visually favoring images and higher detection accuracy.

Index Terms—Object detection, Image enhancement, Multitask learning, Underwater scenes

# I. INTRODUCTION

which leads to bluish or greenish underwater images [2]. Third is light interference. Artificial lighting is widely used for underwater photography, and this non-uniform lighting causes vignetting in captured images [5], [6]. Furthermore, the flickering affects always exist on sunshine days. This will cause the captured images with strong highlights in the shallow ocean [6]. These environmental degradations greatly impact the imaging content, which makes it difficult to recognize the object from the background [7].

In practice, one solution for underwater object detection is to retrain generic detectors [8], [9], [10], [11], [12], [13] designed on high-quality image dataset (e.g., Pascal VOC [14] and COCO [15]). However, they suffer disappointed detection performance due to their poor feature representation in adverse conditions. Researchers thus improve the feature representation capacity of detection networks through operations such as composite connection [4], [16] or attention strategy [17]. However, this strategy does not eliminate the impact of degradation on image content such as color and texture, achieving minimal improvements.

Another feasible solution for underwater detection is to develop sophisticated deep architectures to enhance image/feature quality. For example, Sea-Thru [18], USICR [19], and FUnIE-GAN [20] present enhancement algorithms to recover the color or texture of underwater images. Directly taking these enhancement networks as processing steps for improving image quality involves complicated architectures. Moreover, the visually appealing outputs of these enhancement modules do not necessarily generate high accuracy for deep detection algorithms [3], [1].

D Etecting objects of interest under underwater conditionsare extremely important for applications such as marine monitoring and aquaculture [1], [2], [3], [4]. However, underwater object detection suffers from low detection accuracy due to various environmental degradations (as shown in Fig. 1). First are haze-like effects. The water medium and sediments scatter the light causing low-contrast and haze-like phenomena in underwater photography. The motion of electronics and objects can also produce blurring during high-dimensional imaging. Second are color distortions. Wavelength absorption usually causes a color reduction in the captured image,

Recently, multi-task learning has been introduced and applied well for computer vision fields. These methods aim to leverage information coming from related tasks so that each task can gain benefit [21], [22]. Specifically, MGM [21], MultiNet [23], and UWL [24] explore multi-task learning within the setting of visual scene understanding in computer vision. There are also some multi-task works for joint learning underwater object detection and image enhancement, prompting enhancement modules to generate detection-favored and visually appealing images [3]. Typically, these methods invoke huge architectures and inefficient inference, hindering their applications to time-critical scenarios. Therefore, problem definitions and effective architecture designs are still challenges to jointly learning object detection and image enhancement in underwater scenes.

Indeed, underwater object detection and image enhancement are two complementary tasks. The shared representation learned from the two tasks can help each task. Specifically, the shared representation can provide more content information including color or texture details for underwater object detection. Simultaneously, the shared representation can provide more structural details for image enhancement. Motivated by these factual opinions, this paper proposes a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, aiming to learn the shared representation that benefits the two different tasks and thereby improves the performance of the two tasks. This formulation unrolls to a dual perception network (DPNet), composed of one shared module and two task subnets. One subnet achieves detection tasks and the other one achieves enhancement tasks. The shared module links the two subnets and provides the shared representation for them. We also derive a cooperative training strategy to learn optimal parameters for DPNet. Fig. 2 demonstrates our methodology framework. Our contributions are four-fold:

![](Images_GM4C8A5M/38f897de038687d865ac65ae8ad17d9e65bed8b3f05fa12300e6dcf2ee64a14a.jpg)  
Fig. 1: Examples of underwater environmental degradations include haze-like effects, color casts, and light interference. Images are selected from UEDB.

• We supply a general bilevel modeling perspective for jointly learning underwater object detection and image enhancement. It can accurately depict the latent correspondence between the shared representation, detection, and enhancement.   
• We devise a dual perception network for the bilevel model. DPNet “seeks the shared representation from differences” that provides rich content information for underwater object detection and more structural details for image enhancement.   
We derive a cooperative training scheme from the bilevel optimization formulation, yielding optimal network parameters for DPNet.

To prove the detection and enhancement efficiency of our approach, we evaluate the proposed model on synthetic and natural underwater datasets. Quantitative and qualitative evaluation results show that our proposed approach surpasses the accuracy of current state-of-the-art object detection and image enhancement methods.

The rest of this paper is organized as follows: Sec.II provides related works including underwater object detection and multi-task learning. Sec. III describes the proposed object detection method, which is used in underwater images. Sec.IV presents the comprehensive experimental results of our method compared with other methods. Sec.V concludes our work.

# II. RELATED WORKS

# A. Underwater Object Detection

Underwater object detection aims at determining what and where an object is in an underwater image. Compared with high-quality images, underwater images captured in complex environments confront evident quality degradation. Generic deep detectors [25], [26], [27], [28], [29], [30] designed on high-quality images thus have poor performance when retrained on underwater images.

Some researchers attempt to improve the feature representation capacity of generic deep detectors [4], [16], [17]. For example, AquaNet [4] designs two efficient components MFF and MBP using multi-scale features fusion and anti-aliasing operations. FERNet [16] proposes a composite connection backbone and anchor refinement scheme to boost the feature representation. CSAM [17] proposes a novel channel sharpening attention module to fuse high-level image information for better feature utilization. SSoB [31] proposes a mixed antialiasing block and resorts neural architecture search to build a detection backbone for underwater scenes. While promising, these methods do not remove the influence of degradation factors on detection features, achieving limited improvement.

![](Images_GM4C8A5M/73ce373fbbe8c4df30a3ac36251df2dca4f8278717ee652598f658c11868b5a6.jpg)  
Fig. 2: Workflow of our method. (a) The architecture of our dual perception network (DPNet). (b) During the inference phase, we only retain the shared module and a detection/enhancement subnet for detection/enhancement tasks.

Image enhancement algorithms can improve an underwater image to a sharp version. Typically, Sea-Thru [18] proposes a physical imaging model to recover lost colors in underwater images. FUnIE-GAN [20] presents a conditional generative adversarial network-based model for recovering global content, color, and local texture of underwater images. USICR [19] uses haze lines to solve color restoration problems in underwater images. Since these underwater enhancement algorithms are designed for visually appealing outputs, directly using them as processing steps for improving image quality for deep detectors may lead to suboptimal performance.

# B. Multi-task learning in the computer vision society

Multi-task learning simultaneously learns multiple related tasks, aiming to be beneficial to each task. Many multi-task learning models have been proposed and applied in various deep learning applications in the computer vision society. For example, MGM [21] couples a discriminative multitask network with a generative network for joint learning depth estimation, surface normal estimation, and semantic segmentation tasks. MultiNet [23] uses a unified architecture to jointly learn classification, detection, and semantic segmentation tasks. UWL [24] designs a principled approach to multitask deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task.

Some researchers also attempt to design multi-task learning models for underwater enhancement and detection tasks, aiming to prompt the two tasks [3]. Specifically, HybridDetectionGAN [3] use the detection preceptor to provide feedback information in the form of gradients to guide the enhancement model, generating visually pleasing or detection favorable images. However, this manner involves a complicated architecture and occupies a huge inference time. In our paper, unlike the methods reported in [3], our DPNet has a simple but efficient joint learning strategy and architecture. During the inference phase, we only retain the small shared model and a detection/enhancement subnet for completing detection/enhancement tasks.

# III. THE PROPOSED APPROACH

Underwater imaging processes suffer from haze-like effects, color casts, and light interference. These evident quality degradations impact image content information including color and texture, which makes the detection difficult to distinguish objects from the background [7].

Jointly learning underwater object detection and image enhancement benefits each of them, since they are two complementary works. Specifically, underwater image enhancement aims to recover the color/texture of images, which helps detectors to detect objects from harsh conditions. Underwater object detection attempts to decide what and where an object is in an image. The fundamental features of detection tasks contain rich structural information, which aids enhancement methods to make the image sharper. The shared representation between detection and enhancement thus benefits the two tasks. However, problem definitions and effective architecture designs are still challenges to this joint learning issue.

Inspired by these factual opinions, we introduce a general bilevel modeling perspective for learning a shared representation between underwater object detection and image enhancement. The shared representation provides content information for prompting detection and structural details for boosting enhancement. Then, we unroll the bilevel modeling to a dual perception network. Finally, we give a cooperative training scheme to learn optimal parameters for DPNet. Fig.2 illustrates the idea, introduced next.

# A. Problem Formulation

According to the general scheme of multi-task learning, we take a shared module to connect underwater detection and enhancement subnets. The shared module seeks the shared representation from the two subnets, providing more content information for detection through enhancement task properties and more structural details for enhancement through detection task capabilities. Therefore, its parameter optimization depends on the two subnets’ optimization. Naturally, following the truism of Stackelberg’s theory [32], [33], we formulate the joint problem of underwater object detection and image enhancement as a bilevel optimization model:

$$
\begin{array} { l } { \displaystyle \underset { \bf u } { \mathrm { m i n } } ~ \mathcal { L } ^ { v a l } ( \omega ^ { * } , \Phi ( { \boldsymbol u } ) ) } \\ { \displaystyle s . t . , ~ \omega ^ { * } = \arg \underset { \omega } { \mathrm { m i n } } \mathcal { L } ^ { t r } ( \omega , \Phi ( { \boldsymbol u } ) ) , } \end{array}
$$

where $\Phi$ denotes the shared module with learnable parameters $\mathbf { \Omega } _ { \pmb { u } , \pmb { \mathcal { L } } }$ is the loss function of the joint problem, which is defined as the sum of the detection loss $\mathcal { L } _ { d e t }$ and enhancement loss $\mathcal { L } _ { e n h } . ~ \mathcal { L } ^ { v a l }$ denotes the loss with the validation set and $\mathcal { L } ^ { t r }$ denotes the loss with the training set. $\boldsymbol { \omega } ~ = ~ ( \omega _ { d e t } , \omega _ { e n h } )$ , where $\omega _ { d e t }$ and $\omega _ { e n h }$ are parameters of the detection and enhancement subnets, respectively.

The above bilevel modeling actually builds an explicit relationship among the shared module, detection, and enhancement subnets. The shared module parameter $\textbf { \em u }$ , the upper-level parameter, is updated along with lower-level parameters. $\omega _ { d e t }$ and $\omega _ { e n h }$ , the lower-level parameters, are separately optimized by the detection loss $\mathcal { L } _ { d e t }$ and enhancement loss $\mathcal { L } _ { e n h }$ due to the two subnet independence. As is shown in Fig. 2 (a), we unroll the bilevel optimization formulation to a novel DPNet to solve the problem in Eq.(1).

# B. Dual Perception Network

Most multi-task learning methods for underwater enhancement and object detection involve a complicated architecture and occupy a huge inference time. To overcome these problems, we devise a simple but efficient joint learning architecture for the two tasks (i.e., DPNet). As is shown in Fig. 2 (a), DPNet consists of three parts: a shared module, an object detection subnet, and an image enhancement subnet. During the training phase, the shared module, placed before the two subnets, is designed for providing shared representation containing more structure and content information (e.g., color or texture) to the two subnets. The enhancement and object detection subnets are trained for image enhancement and detection, respectively. As is shown in Fig. 2 (b), we only retain the shared module and a detection/enhancement subnet for detection/enhancement tasks during the inference phase. This manner reduces architectural complexity, achieving efficient memory space and inference times. As is shown in Fig. 2, we introduce the DPLNet from its three components.

The shared module extracts the shared representation of the input image that contains important information for simultaneous benefiting underwater enhancement and detection. For one thing, the shared representation contains much useful content information including recovered color and texture, which is beneficial for detectors to detect objects from degraded environments. For another thing, the shared representation contains much structural information, which is beneficial for enhancement methods to generate sharper images.

How to design a suitable structure for the shared module plays a vital role in implementation. First, underwater missions often deploy to mobile CPUs, requiring lightweight modules. Second, the shared module should contain more low-level information including color, texture, and structure, as it is applied to provide shared representation for detection and enhancement subnets. For these considerations, we construct the shared module with few layers and small kernels. Specifically, the shared module contains three convolution layers with kernels of $3 \times 3 , 5 \times 5$ , and $3 \times 3$ , respectively. The channels of the three convolution layers are set as 32.

Underwater image enhancement subnet is responsible for improving the visualization quality of underwater images. For image enhancement tasks, downsampling and upsampling operations tend to be used together, since downsampling operations cause some problems including missing image information. This manner invokes complex architectures, and we thus design the underwater image enhancement subnet with a plain convolution without downsampling. The subnet consists of three $3 \times 3$ convolutional layers with stride 1 and padding 1. The first 2 layers are followed by a batch normalization layer and Relu activation function. The last layer is followed by a batch normalization layer and Sigmoid activation function. The sigmoid activation function is used to make the output in [0,1]. The subnet produces an image with $H \times W \times 3$ , where $H , W$ , and 3 are the height, weight, and channels of the image.

Underwater object detection subnet is responsible for classification and regression. Here, we adopt RetinaNet [34] as the detection subnet. To adapt the shared module, the channel of the first layer of RetinaNet is set to 32. There are two reasons for us to choose RetinaNet as our backbone. For one thing, RetinaNet proposes the focal loss that focuses on learning hard examples to realize a balance between classes during training. For another thing, RetinaNet employs a feature pyramid network (FPN) [35] that provides a top-down pathway and lateral connections. RetinaNet thus enables the construction of higher resolution layers from a rich semantic layer, which can significantly improve detection accuracy in harsh conditions. For these reasons, RetinaNet has the potential capacity of coping with the challenge of detecting objects in poor visibility conditions.

# C. Cooperative Training Strategy

The bilevel optimization naturally derives a cooperative training strategy to obtain optimal network parameters $\textbf { \em u }$ and $\omega$ . Let $t \in [ 1 , T ]$ be the iteration times. The loss gradients with respect to $\textbf { \em u }$ is calculated as:

# (a) Sampling images from UEDB

![](Images_GM4C8A5M/85c73520b71b2049fc8fe5fc34e34d78eaf9e2d49c4bcc93b0d413e2dd635d35.jpg)  
(b) Sampling images from Underwater-Cityscapes   
Fig. 3: Sampling images from UEDB and Underwater-Cityscapes. Top row: raw underwater images with semantic labels; Bottom row: the corresponding reference images.

$$
\begin{array} { c } { \displaystyle \frac { \partial \mathcal { L } ^ { v a l } } { \partial u } = \frac { \partial \mathcal { L } ^ { v a l } ( \omega ^ { t } , \Phi ( \boldsymbol { u } ) ) } { \partial \Phi ( \boldsymbol { u } ) } \frac { \partial \Phi ( \boldsymbol { u } ) } { \partial u } + \frac { \partial \mathcal { L } ^ { v a l } ( \omega ^ { t } , \Phi ( \boldsymbol { u } ) ) } { \partial \omega ^ { t } } \frac { \partial \omega ^ { t } } { \partial u } } \\ { \displaystyle = \frac { \partial \mathcal { L } ^ { v a l } ( \omega ^ { t } , \Phi ( \boldsymbol { u } ) ) } { \partial \Phi ( \boldsymbol { u } ) } \frac { \partial \Phi ( \boldsymbol { u } ) } { \partial u } } \\ { \displaystyle + \frac { \partial \mathcal { L } ^ { v a l } ( \omega ^ { t } , \Phi ( \boldsymbol { u } ) ) } { \partial \omega ^ { t } } \frac { \partial ( \omega ^ { t - 1 } - \frac { \partial \mathcal { L } ^ { t r } ( \omega ^ { t - 1 } , \Phi ( \boldsymbol { u } ) ) } { \partial \omega } ) } { \partial u } } \end{array}
$$

The Eq. 2 reveals that the shared module, underwater image enhancement subnet, and object detection subnet are simultaneously updated. After the training process, the shared module trained with this strategy can generate a shared representation that is conducive to enhancement and detection, since the shared module is simultaneously optimized by the two tasks. Meanwhile, the detection and enhancement subnets can achieve their tasks, respectively.

# IV. EXPERIMENTS

# A. Experimental Configurations

In this section, we introduce experimental datasets including Underwater-Cityscapes and UEDB. Then, we brief implementation details of DPNet.

Comparision dataset. Some datasets have been published for underwater object detection. Typically, URPC $2 0 1 8 \sim 2 0 2 2$ 1 are created to support underwater robot professional contests, consisting of 4 categories. UDD [4] is an underwater objectgrabbing dataset, and images are manually collected by divers on Zhangzi Island, Dalian, China. RUOD [36] is an object detection dataset for general underwater scenes, containing various environmental challenges. Some underwater image enhancement datasets also have been published. Representatively, UIEB [37] includes 890 real-world underwater images and their corresponding reference images. Blasinski et al. [38] provides an open-source underwater image simulation tool and a three-parameter formation model. Duarte et al. [39] simulates underwater image degradation using milk, chlorophyll, or green tea in a tank. However, most existing underwater image datasets only support one task and cannot support detection and enhancement at the same time.

To conduct our experiments, we compose two datasets by synthetic methods or annotation manners. The two datasets have semantic labels for detection and reference images for enhancement. Specifically, Cityscapes is a clear weather image set, which includes car, truck, motorcycle/bike, train, bus, rider, and person. As per [40], we simulate underwater scenes on Cityscapes [41] to synthesize the UnderwaterCityscapes. Therefore, Underwater-Cityscapes has the same labels as Cityscapes and takes Cityscapes as the reference images. Underwater-Cityscapes consists of a train/val set with 2975 images and a test set with 500 images. For further evaluating our DPNet on real-world underwater scenes, we also build a real-world underwater enhancement $\&$ detection benchmark called UEDB. UEDB is derived from a real-world underwater image enhancement dataset UIEB [37]. We first discard some images in UIEB according to two criteria: (1) An image without objects should be discarded. (2) An image with too many objects should be discarded due to difficulties in annotating them accurately and completely with confidence. We then annotate these retaining images to construct UEDB. UEDB includes two categories of diver and fish, containing 505 real-world underwater images and their corresponding reference images. Several sampling and the corresponding reference images from UEDB and Underwater-Cityscapes are presented in Fig. 3 (a)-(b).

Implement details We initialize the shared module and underwater image enhancement subnet with kaiming init. The underwater object detection subnet is initialized with weights pretrained on ImageNet [42] (150 epochs). We train DPLNet for 24 epochs with Adam optimizer and batch size 2. The initial learning rate is 0.002 and exponential decay. We resize all image sizes to $8 0 0 ~ \times ~ 1 3 3 3$ for UEDB and UnderwaterCityscapes as input. Our approach is implemented on PyTorch with an NVIDIA Tesla V100 GPU.

# B. Underwater Object Detection Results

We compare DPNet with popular object detection methods to evaluate the enhancement performance. There are four types of methods: general detectors that are originally designed on high-quality datasets, underwater detectors which improve the feature representation depending on the architecture strategy, enhancement+detection for cascading underwater image enhancement and detection, and multi-task based methods jointly learning underwater enhancement and detection. Here, we use common evaluation metrics AP (Average Precision) for each class and mAP (mean Average Precision) to measure the detection performance.

<table><tr><td rowspan="2">Methods</td><td colspan="3">UEDB dataset</td><td colspan="10">Underwater-Cityscapes dataset</td></tr><tr><td>Diver</td><td>Fish</td><td>mAP</td><td>Person</td><td>Rider</td><td>Car</td><td>Truck</td><td>Bus</td><td>Train</td><td>Moto</td><td></td><td>Bike</td><td>mAP</td></tr><tr><td>General detectors:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Free Anchor</td><td>26.1</td><td>25.0</td><td>25.6</td><td>33.9</td><td>29.7</td><td>55.0</td><td>22.8</td><td>28.6</td><td></td><td>26.7</td><td>21.3</td><td>27.7</td><td>30.7</td></tr><tr><td>Fovea Box</td><td>27.3</td><td>23.0</td><td>25.2</td><td>31.2</td><td>26.3</td><td>50.8</td><td>23.8</td><td>31.6</td><td></td><td>25.1</td><td>27.5</td><td>25.8</td><td>30.3</td></tr><tr><td>YOLOX</td><td>26.2</td><td>20.9</td><td>23.6</td><td>34.1</td><td>27.9</td><td>55.6</td><td>25.7</td><td>44.2</td><td></td><td>29.4</td><td>28.4</td><td>26.1</td><td>33.9</td></tr><tr><td>RetinaNet</td><td>32.4</td><td>35.6</td><td>34.0</td><td>34.4</td><td>34.8</td><td>59.2</td><td>24.0</td><td>43.1</td><td></td><td>27.3</td><td>30.9</td><td>32.3</td><td>35.8</td></tr><tr><td>Grid RCNN</td><td>26.5</td><td>28.9</td><td>27.7</td><td>36.5</td><td>30.8</td><td>56.8</td><td>26.0</td><td>40.5</td><td></td><td>23.3</td><td>31.1</td><td>31.8</td><td>34.6</td></tr><tr><td>PVTv1</td><td>34.7</td><td>28.6</td><td>31.7</td><td>34.4</td><td>30.2</td><td>54.8</td><td>26.7</td><td>36.8</td><td></td><td>22.1</td><td>27.4</td><td>30.6</td><td>32.9</td></tr><tr><td>Underwater detectors:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CSAM</td><td>35.3</td><td>35.2</td><td>35.3</td><td>36.6</td><td>30.5</td><td>53.4</td><td>38.4</td><td>40.6</td><td></td><td>31.3</td><td>30.7</td><td>28.2</td><td>36.2</td></tr><tr><td>AquaNet</td><td>33.8</td><td>34.8</td><td>34.3</td><td>34.6</td><td>28.2</td><td>54.4</td><td>25.2</td><td>31.3</td><td></td><td>28.8</td><td>30.5</td><td>34.6</td><td>33.5</td></tr><tr><td>FERNet</td><td>34.0</td><td>38.6</td><td>36.3</td><td>31.5</td><td>32.4</td><td>53.0</td><td>26.3</td><td>41.2</td><td></td><td>30.3</td><td>34.2</td><td>33.3</td><td>35.3</td></tr><tr><td>Enhancement+detection:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FUnIE-GAN+FERNet</td><td>33.2</td><td>35.5</td><td>34.4</td><td>30.0</td><td>30.8</td><td>51.4</td><td>24.2</td><td>37.0</td><td></td><td>25.4</td><td>31.8</td><td>32.7</td><td>32.9</td></tr><tr><td>FUnIE-GAN+RetinaNet</td><td>30.2</td><td>30.5</td><td>30.4</td><td>33.0</td><td>32.2</td><td>57.4</td><td>23.2</td><td>40.0</td><td></td><td>26.4</td><td>28.8</td><td>30.7</td><td>34.0</td></tr><tr><td>ERH+FERNet</td><td>34.3</td><td>35.2</td><td>34.8</td><td>35.1</td><td>31.5</td><td>50.3</td><td>23.4</td><td>39.2</td><td></td><td>27.5</td><td>32.3</td><td>29.6</td><td>33.6</td></tr><tr><td>ERH+RetinaNet</td><td>30.3</td><td>33.2</td><td>31.8</td><td>33.1</td><td>33.0</td><td>58.3</td><td>24.0</td><td>41.6</td><td></td><td>26.8</td><td>29.2</td><td>31.6</td><td>34.7</td></tr><tr><td>Multi-task learning:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HybridDGAN</td><td>40.3</td><td>39.3</td><td>39.8</td><td>38.4</td><td>37.7</td><td>58.6</td><td>35.6</td><td></td><td>43.8</td><td>30.0</td><td>27.5</td><td>28.8</td><td>37.6</td></tr><tr><td>Ours:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DPNet</td><td>44.2</td><td>43.1</td><td>43.7</td><td>42.1</td><td>40.6</td><td>60.2</td><td>36.3</td><td></td><td>44.6</td><td>32.6</td><td>30.8</td><td>31.3</td><td>39.8</td></tr></table>

TABLE I: Comparisons of the detection accuracy on UEDB and Underwater-Cityscapes datasets.

Compared with general detectors. We compare our DPNet with general detectors on UEDB and Underwater-Cityscapes datasets, summarizing the comparisons in Tab I. We retrain these general detectors on the two underwater datasets, however, their performance on detection tasks is disappointing. DPNet achieves $4 3 . 7 \%$ and $3 9 . 8 \%$ mAP on UEDB and Underwater-Cityscapes datasets respectively, surpassing these methods by a large margin. For example, DPNet surpasses RetinaNet and PVTv1 by $9 . 7 \%$ and $1 2 . 0 \%$ mAP on the UEDB dataset, respectively. DPNet outperforms RetinaNet and Grid RCNN by $4 . 0 \%$ and $5 . 2 \%$ mAP on the Underwater-Cityscapes dataset, respectively.

Compared with underwater detectors. As is shown in Tab. I, we compare our DPNet with representative underwater detection methods. CSAM [17], AquaNet [4], and FERNet [16] attempt to improve the feature representation capacity of detectors in underwater scenes through attention strategy and composite connection. They achieve limited gains since these methods do not essentially remove the impact of degradation on underwater images. DPNet achieves a promising improvement. Specifically, DPNet surpasses CSAM, AquaNet, and FERNet by $8 . 4 \%$ , $9 . 4 \%$ , and $7 . 4 \%$ mAP on UEDB, respectively. DPNet outperforms CSAM, AquaNet, and FERNet by $3 . 6 \%$ , $6 . 3 \%$ , and $4 . 5 \%$ mAP on Underwater-Cityscapes, respectively.

Compared with enhancement+detection. We also compare the results of our DPNet with some enhancement+detection methods. Here, we use underwater image enhancement networks (i.e., FUnIE-GAN [20] and ERH [43]) as image processing steps and perform detection using RetinaNet and HybridDGAN trained on enhanced images. As is shown in Tab. I, we find that an underwater enhancement network as an image processing step is not always beneficial for underwater image detection. Similar phenomena are also reported in [2], [44]. The phenomenon demonstrates that the visually appealing outputs of enhancement modules do not necessarily generate high accuracy for deep detection algorithms.

TABLE II: Full-reference comparisons of the enhancement performance on UEDB and Underwater-Cityscapes datasets.   

<table><tr><td rowspan="2">Methods</td><td colspan="3">UEDB dataset</td><td colspan="2">Underwater-Cityscapes dataset</td></tr><tr><td>SSIM↑</td><td>PSNR↑</td><td>PCQI 个</td><td>SSIM个 PSNR ↑</td><td>PCQI↑</td></tr><tr><td>UDCP</td><td>0.553</td><td>12.554</td><td>0.532</td><td>0.689 22.435</td><td>0.887</td></tr><tr><td>ERH</td><td>0.721</td><td>20.230</td><td>0.862</td><td>0.884 23.776</td><td>0.932</td></tr><tr><td>HybridDGAN</td><td>0.696</td><td>22.319</td><td>0.714</td><td>0.835 24.675</td><td>0.904</td></tr><tr><td>FUnIE-GAN</td><td>0.657</td><td>19.420</td><td>0.580</td><td>0.731 23.843</td><td>0.892</td></tr><tr><td>DPNet</td><td>0.826</td><td>23.325</td><td>0.856</td><td>0.954 26.343</td><td>0.976</td></tr></table>

<table><tr><td rowspan="2">Methods</td><td colspan="2">UEDB dataset</td><td colspan="2">Underwater-Cityscapes dataset</td></tr><tr><td>UIQM↑</td><td>NIQE↓</td><td>UIQM↑</td><td>NIQE↓</td></tr><tr><td>UDCP</td><td>1.785</td><td>4.280</td><td>2.189</td><td>5.433</td></tr><tr><td>ERH</td><td>3.024</td><td>4.054</td><td>3.122</td><td>3.675</td></tr><tr><td>HybridDGAN</td><td>2.866</td><td>4.803</td><td>3.087</td><td>3.850</td></tr><tr><td>FUnIE-GAN</td><td>2.675</td><td>5.463</td><td>2.980</td><td>4.076</td></tr><tr><td>DPNet</td><td>3.403</td><td>4.189</td><td>4.434</td><td>3.332</td></tr></table>

TABLE III: Non-reference Comparisons of the enhancement performance on UEDB and Underwater-Cityscapes datasets.

![](Images_GM4C8A5M/c5c6bc9712289c263295490c773d26a4ef93da53cdc2a28f120c78de28cc9e45.jpg)  
Fig. 4: Visualization of detection results on UEDB dataset. The examples from top to bottom are light interference, color casts, and haze-like effects, respectively.

![](Images_GM4C8A5M/bfb1439e015b08462da86a7539d8ce1018c74965048d925793592095f45ff44e.jpg)  
Fig. 5: Visualization of detection results on Underwater-Cityscapes dataset. The examples from top to bottom are color casts and haze-like effects.

<table><tr><td rowspan="2">Methods</td><td colspan="2">Underwater object detection</td><td colspan="2">Underwater image enhancement</td><td colspan="2">Underwater detection and enhacement</td></tr><tr><td>Size. (M)</td><td>FPS(f·s-1)</td><td>Size (M)</td><td>FPS(f·s-1)</td><td>Size (M)</td><td>FPS (f ·s-1)</td></tr><tr><td>HybridDGAN</td><td>148.7</td><td>2.4</td><td>148.7</td><td>2.4</td><td>148.7</td><td>2.4</td></tr><tr><td>DPNet</td><td>70.3</td><td>4.3</td><td>30.2</td><td>5.5</td><td>85.0</td><td>3.5</td></tr><tr><td>Increase</td><td>78.4↓</td><td>1.9个</td><td>118.5↓</td><td>3.1个</td><td>63.7</td><td>1.1个</td></tr></table>

TABLE IV: Efficiency comparison on DPNet and HybridDGAN.

Compared with multi-task methods. HybridDGAN [3] uses hybrid underwater image synthesis and perceptual enhancement models to jointly learn underwater detection and enhancement. As is shown in Tab. I, HybridDGAN achieves more progress than CSAM, AquaNet, and FERNet. Our DPNet further improves the accuracy performance and achieves the best performance. Typically, DPNet outperforms HybridDGAN by $3 . 9 \%$ and $2 . 2 \%$ mAP on UEDB and UnderwaterCityscapes datasets, respectively.

# C. Performance Analysis

In this part, we analyze our DPNet from image enhancement properties, efficiency performance, and environmental degradation performance.

Underwater image enhancement results. We compare DPNet with popular underwater image enhancement methods to evaluate the enhancement performance. Here, we take three common full-reference metrics, including Structural Similarity (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Patch-based Contrast Quality Index (PCQI) [45]. Higher PSNR, SSIM, and PCQI scores are closer to the reference image. Besides, for comprehensive comparison, we also employed two nonreference metrics, including Underwater Image Quality Measurement (UIQM) [46] and Natural Image Quality Evaluator (NIQE) [47]. The common underwater image quality measure UIQM consists of three attribute measures, namely, the underwater image colorfulness measure (UICM), the underwater image sharpness measure (UISM), and the underwater image contrast measure (UIConM). Higher UIQM scores mean higher image quality. NIQE is a popular non-reference image quality assessment method for evaluating the naturalness of a single image; a minor NIQE score means higher image quality.

![](Images_GM4C8A5M/08b1f55dbc4dd3020fd461538da3c78d9afe668902102fe75b22d9bffbede075.jpg)  
Fig. 6: Visualization of enhancement results on Underwater-Cityscapes dataset. The examples from top to bottom are color casts, haze-like effects, and light interference, respectively.

As is shown in Tab. II, UDCP [48] is a physical model-based method, that achieves lower metric scores than other methods. ERH [43] is the latest underwater vision reconstruction method, FUnIE-GAN [20] is the representative data-driven method, and HybridDGAN [3] jointly learns enhancement and detection. ERH, FUnIE-GAN, and HybridDGAN, especially ERH, play better enhancement on UEDB and UnderwaterCityscapes datasets. Our DPNet achieves the best results on most metrics compared with these methods, especially in the synthetic dataset Underwater-Cityscapes. These experimental results demonstrate that DPNet can complete underwater image enhancement tasks well from the perspective of fullreference comparisons.

As is shown in Tab. III, the multi-task based HybridDGAN achieves acceptance performance. ERH plays better than most methods in general. Our DPNet wins on most non-reference metrics compared with all these methods. Typically, DPNet achieves the best results on the synthetic dataset UnderwaterCityscapes. These experimental results further show that DPNet can complete underwater image enhancement tasks well from the perspective of non-reference comparisons.

Efficiency analysis. We compare the proposed methods in terms of speed and parameter size, as is shown in Tab. IV. Compared to the multi-task learning method (i.e., HybridDGAN) for underwater detection and enhancement, DPNet has much fewer parameters and higher speed. Typically, our method can achieve 4.3 FPS, which is well-qualified for real-time detection tasks. DPNet has a higher enhancement speed compared to HybridDGAN with less than one-fifth of the parameters. Performing detection and enhancement simultaneously, DPNet also has much fewer parameters and higher speed in comparison with HybridDGAN. These experimental results demonstrate that DPNet performs better than HybridDGAN in speed and memory efficiency.

Environmental degradation performance. Finally, we present some qualitative results, including underwater object detection and image enhancement tasks, to demonstrate the performance of our DPNet on various environmental degradation cases.

Fig. 4 and Fig. 5 show examples of detection visualization on UEDB and Underwater-Cityscapes datasets. For light interference and color casts, most methods fail to complete detection, there exist errors or missed detection phenomena. Instead, DPNet completes the detection correctly. For haze-like effects, some methods and our DPNet can complete the detection task very well. For example, HybridDGAN achieves correct detection on UEDB datasets. RetinaNet, CSAM, and ERH $\cdot +$ FERNet achieve correct detection on Underwater-Cityscapes datasets. The qualitative results show that DPNet does detection well in various environmental degradations, since the detection subnet can access the shared representation with more content information provided by the shared module.

![](Images_GM4C8A5M/0d8dd16628bb3a01c8c913a4756e2be16ec2d4428ca0ab4513d209726902d687.jpg)  
Fig. 7: Visualization of enhancement results on UEDB dataset. The examples from top to bottom are color casts and haze-like effects, respectively.

Fig. 6 and Fig. 7 show examples of enhancement visualization on UEDB and Underwater-Cityscapes datasets. For color casts and haze-like effects, we can see that DPNet recovers the fish, plate number, and buildings with a clearer edge in comparison with other enhancement methods. Similarly, DPNet can present a sharper edge line in light interference scenes. Typically, DPNet has more natural water lines instead of overexposure like ERH, FUnIE-GAN, or HybridDGAN in light interference scenes. The qualitative results show that DPNet does image enhancement well in various environmental degradations, since the enhancement subnet can access the shared representation with more structure details provided by the shared module.

# V. CONCLUSION

Underwater object detection and image enhancement are two interrelated tasks. Leveraging information coming from the two tasks can benefit each task. Based on these factual opinions, we propose a bilevel optimization formulation for jointly learning underwater object detection and image enhancement, and then unroll to a dual perception network (DPNet) for the two tasks. DPNet with one shared module and two task subnets seeks a shared representation. The shared representation contains more structural details from the detection optimization and rich content information from the enhancement optimization. Finally, we derive a cooperative training strategy to optimize parameters for DPNet. Extensive experiments on several underwater datasets demonstrate that our method outputs not only visually favoring images but also higher detection accuracy than the state-of-the-art approaches.

# VI. ACKNOWLEDGEMENT

This work was supported in part by the National Key R&D Program of China under Grant 2020YFB1313503; in part by the National Natural Science Foundation of China under Grants 61922019, 62027826, and 61772105; in part by the LiaoNing Revitalization Talents Program under Grant XLYC1807088; and in part by the Fundamental Research Funds for the Central Universities.

# REFERENCES

[1] C. Fu, X. Fan, J. Xiao, W. Yuan, R. Liu, and Z. Luo, “Learning heavilydegraded prior for underwater object detection,” IEEE TCSVT, pp. 1–1, 2023.   
[2] R. Liu, X. Fan, M. Zhu, M. Hou, and Z. Luo, “Real-world underwater enhancement: Challenges, benchmarks, and solutions under natural light,” IEEE TCSVT, vol. 30, no. 12, pp. 4861–4875, 2020.   
[3] L. Chen, Z. Jiang, L. Tong, Z. Liu, A. Zhao, Q. Zhang, J. Dong, and H. Zhou, “Perceptual underwater image enhancement with deep learning and physical priors,” IEEE TCSVT, vol. 31, no. 8, pp. 3078–3092, 2021.   
[4] C. Liu, Z. Wang, S. Wang, T. Tang, Y. Tao, C. Yang, H. Li, X. Liu, and X. Fan, “A new dataset, poisson gan and aquanet for underwater object grabbing,” IEEE TCSVT, vol. 32, no. 5, pp. 2831–2844, 2022.   
[5] H. Lu, Y. Li, X. Xu, J. Lin, Z. Liu, X. Li, J. Yang, and S. Serikawa, “Underwater image enhancement method using weighted guided trigonometric filtering and artificial light correction,” J. Vis. Commun. Image Represent., vol. 38, pp. 504–516, 2016.   
[6] H. Lu, Y. Li, Y. Zhang, M. Chen, S. Serikawa, and H. Kim, “Underwater optical image processing: a comprehensive review,” Mob. Networks Appl., vol. 22, no. 6, pp. 1204–1211, 2017.   
[7] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel, “Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness,” in ICLR, 2019.   
[8] Z. Cai and N. Vasconcelos, “Cascade R-CNN: delving into high quality object detection,” in CVPR, 2018, pp. 6154–6162.   
[9] X. Lu, B. Li, Y. Yue, Q. Li, and J. Yan, “Grid R-CNN,” in CVPR, 2019, pp. 7363–7372.   
[10] J. Wang, K. Chen, S. Yang, C. C. Loy, and D. Lin, “Region proposal by guided anchoring,” in CVPR, 2019, pp. 2965–2974.   
[11] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-end object detection with transformers,” in ECCV, vol. 12346, 2020, pp. 213–229.   
[12] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in ICCV, 2021, pp. 9992–10 002.   
[13] W. Wang, E. Xie, X. Li, D. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao, “Pyramid vision transformer: A versatile backbone for dense prediction without convolutions,” in ICCV, 2021, pp. 548–558.   
[14] M. Everingham, L. V. Gool, C. K. I. Williams, J. M. Winn, and A. Zisserman, “The pascal visual object classes (VOC) challenge,” Int. J. Comput. Vis., vol. 88, no. 2, pp. 303–338, 2010.   
[15] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, “Microsoft COCO: common objects in ´ context,” in ECCV, vol. 8693, 2014, pp. 740–755.   
[16] B. Fan, W. Chen, Y. Cong, and J. Tian, “Dual refinement underwater object detection network,” in ECCV, vol. 12365, 2020, pp. 275–291.   
[17] L. Jiang, Y. Wang, Q. Jia, S. Xu, Y. Liu, X. Fan, H. Li, R. Liu, X. Xue, and R. Wang, “Underwater species detection using channel sharpening attention,” in ACM MM, pages $= 4 2 5 9 { \ - } 4 2 6 7 .$ , year $= 2 0 2 I$ .   
[18] D. Akkaynak and T. Treibitz, “Sea-thru: A method for removing water from underwater images,” in CVPR, 2019, pp. 1682–1691.   
[19] D. Berman, D. Levy, S. Avidan, and T. Treibitz, “Underwater single image color restoration using haze-lines and a new quantitative dataset,” IEEE TPAMI, vol. 43, no. 8, pp. 2822–2837, 2021.   
[20] M. J. Islam, Y. Xia, and J. Sattar, “Fast underwater image enhancement for improved visual perception,” IEEE Robotics Autom. Lett., vol. 5, no. 2, pp. 3227–3234, 2020.   
[21] Z. Bao, M. Hebert, and Y.-X. Wang, “Generative modeling for multi-task visual learning,” in ICML, 2022, pp. 1537–1554.   
[22] C. Doersch and A. Zisserman, “Multi-task self-supervised visual learning,” in ICCV, 2017, pp. 2070–2079.   
[23] M. Teichmann, M. Weber, J. M. Zollner, R. Cipolla, and R. Urtasun, ¨ “Multinet: Real-time joint semantic reasoning for autonomous driving,” in IEEE Intelligent Vehicles Symposium, 2018, pp. 1013–1020.   
[24] A. Kendall, Y. Gal, and R. Cipolla, “Multi-task learning using uncertainty to weigh losses for scene geometry and semantics,” in CVPR, 2018, pp. 7482–7491.   
[25] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in CVPR, 2016, pp. 779– 788.   
[26] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg, “SSD: single shot multibox detector,” in ECCV, vol. 9905, 2016, pp. 21–37.   
[27] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Dollar, “Focal loss for ´ dense object detection,” IEEE TPAMI, vol. 42, no. 2, pp. 318–327, 2020.   
[28] C. Zhu, Y. He, and M. Savvides, “Feature selective anchor-free module for single-shot object detection,” in CVPR, 2019, pp. 840–849.   
[29] X. Zhang, F. Wan, C. Liu, R. Ji, and Q. Ye, “Freeanchor: Learning to match anchors for visual object detection,” in NIPS, 2019, pp. 147–155.   
[30] T. Kong, F. Sun, H. Liu, Y. Jiang, L. Li, and J. Shi, “Foveabox: Beyound anchor-based object detection,” IEEE TIP, vol. 29, pp. 7389–7398, 2020.   
[31] R. L. Wanqi Yuan, Chenping Fu and X. Fan, “Ssob: Searching a scene-oriented architecture for underwater object detection,” The Visual Computer, 2022.   
[32] R. Liu, J. Gao, J. Zhang, D. Meng, and Z. Lin, “Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond,” IEEE TPAMI, 2021.   
[33] P. Ochs, R. Ranftl, T. Brox, and T. Pock, “Bilevel optimization with nonsmooth lower level problems,” in SSVM, vol. 9087, 2015, pp. 654– 665.   
[34] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Dollar, “Focal loss for ´ dense object detection,” in ICCV, 2017, pp. 2999–3007.   
[35] T. Lin, P. Dollar, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie, ´ “Feature pyramid networks for object detection,” in CVPR, 2017, pp. 936–944.   
[36] C. Fu, R. Liu, X. Fan, P. Chen, H. Fu, W. Yuan, M. Zhu, and Z. Luo, “Rethinking general underwater object detection: Datasets, challenges, and solutions,” Neurocomputing, vol. 517, pp. 243–256, 2023.   
[37] C. Li, C. Guo, W. Ren, R. Cong, J. Hou, S. Kwong, and D. Tao, “An underwater image enhancement benchmark dataset and beyond,” IEEE TIP, vol. 29, pp. 4376–4389, 2020.   
[38] H. Blasinski, T. Lian, and J. E. Farrell, “Underwater image systems simulation,” Rundbrief Der Gi-fachgruppe 5.10 Informationssystemarchitekturen, 2017.   
[39] A. Duarte, F. Codevilla, J. D. O. Gaya, and S. S. C. Botelho, “A dataset to evaluate underwater image restoration methods,” in OCEANS, 2016, pp. 1–6.   
[40] C. Li, S. Anwar, and F. Porikli, “Underwater scene prior inspired deep underwater image and video enhancement,” Pattern Recognit., vol. 98, 2020.   
[41] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in CVPR, 2016, pp. 3213–3223.   
[42] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in CVPR, 2009, pp. 248–255.   
[43] H. Song, L. Chang, Z. Chen, and P. Ren, “Enhancement-registrationhomogenization (ERH): A comprehensive underwater visual reconstruction paradigm,” IEEE TPAMI, vol. 44, no. 10, pp. 6953–6967, 2022.   
[44] C. Fu, R. Liu, X. Fan, P. Chen, H. Fu, W. Yuan, M. Zhu, and Z. Luo, “Rethinking general underwater object detection: Datasets, challenges, and solutions,” Neurocomputing, vol. 517, pp. 243–256, 2023.   
[45] S. Wang, K. Ma, H. Yeganeh, Z. Wang, and W. Lin, “A patch-structure representation method for quality assessment of contrast changed images,” IEEE Signal Process. Lett., vol. 22, no. 12, pp. 2387–2390, 2015.   
[46] K. Panetta, C. Gao, and S. Agaian, “Human-visual-system-inspired underwater image quality measures,” IEEE Journal of Oceanic Engineering, vol. 41, no. 3, pp. 541–551, 2016.   
[47] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a ”completely blind” image quality analyzer,” IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209–212, 2013.   
[48] P. D. Jr., E. R. Nascimento, S. S. C. Botelho, and M. F. M. Campos, “Underwater depth estimation and image restoration based on single images,” IEEE Computer Graphics and Applications, vol. 36, no. 2, pp. 24–35, 2016.